{
 "cells": [
  {
   "source": [
    "# Personalized Medicine Kaggle Competition\n",
    "> \"This was my approach to the Personalized Healthcare Redefining Cancer Treatment Kaggle competition. The goal of the competition was to create a machine learning algorithm that can classify genetic variations that are present in cancer cells.\"\n",
    "\n",
    "- toc:true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: false\n",
    "- author: Dario Arcos-DÃ­az\n",
    "- categories: [machine_learning, classification, healthcare]\n",
    "- image: images/Kaggle_logo.png"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This notebook describes my approach to the [Kaggle competition](https://www.kaggle.com/c/msk-redefining-cancer-treatment) named in the title. This was a research competition at Kaggle in cooperation with the Memorial Sloan Kettering Cancer Center (MSKCC).\n",
    "\n",
    "The goal of the competition was to create a machine learning algorithm that can classify genetic variations that are present in cancer cells.\n",
    "\n",
    "Tumors contain cells with many different abnormal mutations in their DNA: some of these mutations are the drivers of tumor growth, whereas others are neutral and considered *passengers*. Normally, mutations are manually classified into different categories after literature review by clinicians. The dataset made available for this competition contains mutations that have been manually anotated into 9 different categories. The goal is to predict the correct category of mutations in the test set.\n",
    "\n",
    "The model and submission described here got me to the 140th place (out of 1386 teams) or top 11%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The data comes in two different kinds of files: one of them contains information about the genetic variants (*training_variants* and *stage2_test_variants.csv*) and the other contains the text (clinical evidence) that was used to manually classify the variants (*training_text* and *stage2_test_text.csv*). The training data contains a class target feature corresponding to one of the 9 categories that variants can be classified as.\n",
    "\n",
    "*Note: the \"stage2\" prefix of the test files is due to the nature of the competition. There was an initial test set that was used at the beginning of the competition and a \"stage2\" test set that was used in the final week before the deadline to make the submissions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training variants: 3321\nNumber of test variants: 986\n"
     ]
    }
   ],
   "source": [
    "train_variant = pd.read_csv(\"input/training_variants\")\n",
    "test_variant = pd.read_csv(\"input/stage2_test_variants.csv\")\n",
    "train_text = pd.read_csv(\"input/training_text\", sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n",
    "test_text = pd.read_csv(\"input/stage2_test_text.csv\", header=None, skiprows=1, names=[\"ID\", \"Text\"])\n",
    "train = pd.merge(train_variant, train_text, how='left', on='ID')\n",
    "train_y = train['Class'].values\n",
    "train_x = train.drop('Class', axis=1)\n",
    "train_size=len(train_x)\n",
    "print('Number of training variants: %d' % (train_size))\n",
    "# number of train data : 3321\n",
    "\n",
    "test_x = pd.merge(test_variant, test_text, how='left', on='ID')\n",
    "test_size=len(test_x)\n",
    "print('Number of test variants: %d' % (test_size))\n",
    "# number of test data : 5668\n",
    "\n",
    "test_index = test_x['ID'].values\n",
    "all_data = np.concatenate((train_x, test_x), axis=0)\n",
    "all_data = pd.DataFrame(all_data)\n",
    "all_data.columns = [\"ID\", \"Gene\", \"Variation\", \"Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variation</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>FAM58A</td>\n",
       "      <td>Truncating Mutations</td>\n",
       "      <td>Cyclin-dependent kinases (CDKs) regulate a var...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CBL</td>\n",
       "      <td>W802*</td>\n",
       "      <td>Abstract Background  Non-small cell lung canc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>CBL</td>\n",
       "      <td>Q249E</td>\n",
       "      <td>Abstract Background  Non-small cell lung canc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>CBL</td>\n",
       "      <td>N454D</td>\n",
       "      <td>Recent evidence has demonstrated that acquired...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>CBL</td>\n",
       "      <td>L399V</td>\n",
       "      <td>Oncogenic mutations in the monomeric Casitas B...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variation</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>FAM58A</td>\n",
       "      <td>Truncating Mutations</td>\n",
       "      <td>Cyclin-dependent kinases (CDKs) regulate a var...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CBL</td>\n",
       "      <td>W802*</td>\n",
       "      <td>Abstract Background  Non-small cell lung canc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>CBL</td>\n",
       "      <td>Q249E</td>\n",
       "      <td>Abstract Background  Non-small cell lung canc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>CBL</td>\n",
       "      <td>N454D</td>\n",
       "      <td>Recent evidence has demonstrated that acquired...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>CBL</td>\n",
       "      <td>L399V</td>\n",
       "      <td>Oncogenic mutations in the monomeric Casitas B...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data from the different train and test files is now consolidated into one single file. This is necessary for the correct vectorization of the text data and categorical data later on. We can see that the text information resembles scientific article text. We will process this consolidated file in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Preprocessing\n",
    "\n",
    "In order to be able to use this data to train a machine learning model, we need to extract the features from the dataset. This means that we have to transform the text data into vectors that can be understood by an algorithm. As I am not an expert in Natural Language Processing, I applied a modified version of [this script published on Kaggle.](https://www.kaggle.com/alyosama/doc2vec-with-keras-0-77) Afterwards we will have the data in a form that I can use to train a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    cyclindependent kinases cdks regulate variety ...\n1    abstract background nonsmall cell lung cancer ...\n2    abstract background nonsmall cell lung cancer ...\n3    recent evidence demonstrated acquired uniparen...\n4    oncogenic mutations monomeric casitas blineage...\nName: Text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre-processing script by Aly Osama https://www.kaggle.com/alyosama/doc2vec-with-keras-0-77\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim import utils\n",
    "\n",
    "def constructLabeledSentences(data):\n",
    "    sentences=[]\n",
    "    for index, row in data.iteritems():\n",
    "        sentences.append(LabeledSentence(utils.to_unicode(row).split(), ['Text' + '_%s' % str(index)]))\n",
    "    return sentences\n",
    "\n",
    "def textClean(text):\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", str(text))\n",
    "    text = text.lower().split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops]    \n",
    "    text = \" \".join(text)\n",
    "    return(text)\n",
    "    \n",
    "def cleanup(text):\n",
    "    text = textClean(text)\n",
    "    text= text.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
    "    return text\n",
    "\n",
    "allText = all_data['Text'].apply(cleanup)\n",
    "sentences = constructLabeledSentences(allText)\n",
    "allText.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing script by Aly Osama https://www.kaggle.com/alyosama/doc2vec-with-keras-0-77\n",
    "\n",
    "# PROCESS TEXT DATA\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "Text_INPUT_DIM=300\n",
    "\n",
    "text_model=None\n",
    "filename='docEmbeddings_5_clean.d2v'\n",
    "if os.path.isfile(filename):\n",
    "    text_model = Doc2Vec.load(filename)\n",
    "else:\n",
    "    text_model = Doc2Vec(min_count=1, window=5, size=Text_INPUT_DIM, sample=1e-4, negative=5, workers=4, iter=5,seed=1)\n",
    "    text_model.build_vocab(sentences)\n",
    "    text_model.train(sentences, total_examples=text_model.corpus_count, epochs=text_model.iter)\n",
    "    text_model.save(filename)\n",
    "\n",
    "text_train_arrays = np.zeros((train_size, Text_INPUT_DIM))\n",
    "text_test_arrays = np.zeros((test_size, Text_INPUT_DIM))\n",
    "\n",
    "for i in range(train_size):\n",
    "    text_train_arrays[i] = text_model.docvecs['Text_'+str(i)]\n",
    "\n",
    "j=0\n",
    "for i in range(train_size,train_size+test_size):\n",
    "    text_test_arrays[j] = text_model.docvecs['Text_'+str(i)]\n",
    "    j=j+1\n",
    "    \n",
    "print(text_train_arrays[0][:10])\n",
    "\n",
    "# PROCESS GENE DATA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "Gene_INPUT_DIM=25\n",
    "\n",
    "svd = TruncatedSVD(n_components=25, n_iter=Gene_INPUT_DIM, random_state=12)\n",
    "\n",
    "one_hot_gene = pd.get_dummies(all_data['Gene'])\n",
    "truncated_one_hot_gene = svd.fit_transform(one_hot_gene.values)\n",
    "\n",
    "one_hot_variation = pd.get_dummies(all_data['Variation'])\n",
    "truncated_one_hot_variation = svd.fit_transform(one_hot_variation.values)\n",
    "\n",
    "# ENCODE THE LABELS FROM INTEGERS TO VECTORS\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_y)\n",
    "encoded_y = np_utils.to_categorical((label_encoder.transform(train_y)))\n",
    "print(encoded_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have processed the train labels, as printed above (`encoded_y`), into vectors that contain 1 in the index of the category that the sample belongs to, and zeros in all other indexes.\n",
    "\n",
    "Moreover, the training and test sets are now stacked together to look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=np.hstack((truncated_one_hot_gene[:train_size],truncated_one_hot_variation[:train_size],text_train_arrays))\n",
    "test_set=np.hstack((truncated_one_hot_gene[train_size:],truncated_one_hot_variation[train_size:],text_test_arrays))\n",
    "\n",
    "print('Training set shape is: ', train_set.shape)  # (3321, 350)\n",
    "print('Test set shape is: ', test_set.shape)  # (986, 350)\n",
    "\n",
    "print('Training set example rows:')\n",
    "print(train_set[0][:10])\n",
    "# [ -2.46065582e-23  -5.21548048e-19  -1.95048372e-20  -2.44542833e-22\n",
    "#  -1.19176742e-22   1.61985461e-25   2.93618862e-25  -6.23860891e-27\n",
    "#   1.14583929e-28  -1.79996588e-29]\n",
    "\n",
    "print('Test set example rows:')\n",
    "print(test_set[0][:10])\n",
    "# [  9.74220189e-33  -1.31484613e-27   4.37925347e-27  -9.88109317e-29\n",
    "#    7.66365772e-27   6.58254980e-26  -3.74901712e-26  -8.97613299e-26\n",
    "#   -3.75471102e-23  -1.05563623e-21]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now ready to be fed into a machine learning model, in this case, into a neural network in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a 4-layer neural network for classification\n",
    "\n",
    "The next step is to create a neural network on TensorFlow. I am using a fully-connected neural network with 4 layers. For details on how the network is built, you can check my [TensorFlow MNIST notebook](https://github.com/dariodata/TensorFlow-MNIST/blob/master/TensorFlow-MNIST.ipynb). Wherever necessary, I will explains what adaptations were specifically necessary for this challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found it useful to add the current timestamp to the name of the files that the code will output. This helped me to uniquely identify the results from each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "dirname = 'output/'  # output directory\n",
    "filename = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I select 20% of the training data to use as a validation set and be able to quantify my variance (watch out for overfitting), as I don't want to have an algorithm that only works well with this specific training data set that was provided, but one that generalizes as well as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and validation sets\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_set, encoded_y, test_size=0.20, random_state=42)\n",
    "X_train, X_val, Y_train, Y_val = X_train.T, X_val.T, Y_train.T, Y_val.T\n",
    "\n",
    "# transpose test set\n",
    "X_test = test_set.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (350, 2656)\nX_val:  (350, 665)\nY_train:  (9, 2656)\nY_val:  (9, 665)\nX_test:  (350, 986)\n"
     ]
    }
   ],
   "source": [
    "# view data set shapes\n",
    "print('X_train: ', X_train.shape)\n",
    "print('X_val: ', X_val.shape)\n",
    "print('Y_train: ', Y_train.shape)\n",
    "print('Y_val: ', Y_val.shape)\n",
    "print('X_test: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I define the functions needed to build the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "\n",
    "    Arguments:\n",
    "    n_x -- scalar, dimensions of the input\n",
    "    n_y -- scalar, number of classes (from 0 to 8, so -> 9)\n",
    "\n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
    "    \"\"\"\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=(n_x, None), name='X')\n",
    "    Y = tf.placeholder(tf.float32, shape=(n_y, None), name='Y')\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes parameters to build a neural network with tensorflow.\n",
    "\n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W and b for every layer\n",
    "    \"\"\"\n",
    "\n",
    "    tf.set_random_seed(1)\n",
    "\n",
    "    W1 = tf.get_variable('W1', [350, X_train.shape[0]], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b1 = tf.get_variable('b1', [350, 1], initializer=tf.zeros_initializer())\n",
    "    W2 = tf.get_variable('W2', [350, 350], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b2 = tf.get_variable('b2', [350, 1], initializer=tf.zeros_initializer())\n",
    "    W3 = tf.get_variable('W3', [100, 350], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b3 = tf.get_variable('b3', [100, 1], initializer=tf.zeros_initializer())\n",
    "    W4 = tf.get_variable('W4', [9, 100], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b4 = tf.get_variable('b4', [9, 1], initializer=tf.zeros_initializer())\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3,\n",
    "                  \"W4\": W4,\n",
    "                  \"b4\": b4}\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, keep_prob1, keep_prob2):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: (LINEAR -> RELU)^3 -> LINEAR -> SOFTMAX\n",
    "\n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W\" and \"b\" for every layer\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z4 -- the output of the last LINEAR unit (logits)\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve the parameters from the dictionary \"parameters\"\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    W4 = parameters['W4']\n",
    "    b4 = parameters['b4']\n",
    "\n",
    "    Z1 = tf.matmul(W1, X) + b1  # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)  # A1 = relu(Z1)\n",
    "    A1 = tf.nn.dropout(A1, keep_prob1)  # add dropout\n",
    "    Z2 = tf.matmul(W2, A1) + b2  # Z2 = np.dot(W2, a1) + b2\n",
    "    A2 = tf.nn.relu(Z2)  # A2 = relu(Z2)\n",
    "    A2 = tf.nn.dropout(A2, keep_prob2)  # add dropout\n",
    "    Z3 = tf.matmul(W3, A2) + b3  # Z3 = np.dot(W3,Z2) + b3\n",
    "    A3 = tf.nn.relu(Z3)\n",
    "    Z4 = tf.matmul(W4, A3) + b4\n",
    "\n",
    "    return Z4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z4, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "\n",
    "    Arguments:\n",
    "    Z4 -- output of forward propagation (output of the last LINEAR unit), of shape (n_classes, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z4\n",
    "\n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "\n",
    "    # transpose to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z4)\n",
    "    labels = tf.transpose(Y)\n",
    "\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size, seed=0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector, of shape (1, number of examples)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed\n",
    "\n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[1]  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0], m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(\n",
    "        m / mini_batch_size)  # number of mini batches of size mini_batch_size in your partitioning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size: k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size: k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size: m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size: m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    W1 = tf.convert_to_tensor(parameters['W1'])\n",
    "    b1 = tf.convert_to_tensor(parameters[\"b1\"])\n",
    "    W2 = tf.convert_to_tensor(parameters[\"W2\"])\n",
    "    b2 = tf.convert_to_tensor(parameters[\"b2\"])\n",
    "    W3 = tf.convert_to_tensor(parameters[\"W3\"])\n",
    "    b3 = tf.convert_to_tensor(parameters[\"b3\"])\n",
    "    W4 = tf.convert_to_tensor(parameters[\"W4\"])\n",
    "    b4 = tf.convert_to_tensor(parameters[\"b4\"])\n",
    "\n",
    "    params = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"W3\": W3,\n",
    "              \"b3\": b3,\n",
    "              \"W4\": W4,\n",
    "              \"b4\": b4}\n",
    "\n",
    "    x = tf.placeholder(\"float\", [X_train.shape[0], None])\n",
    "    keep_prob1 = tf.placeholder(tf.float32, name='keep_prob1')\n",
    "    keep_prob2 = tf.placeholder(tf.float32, name='keep_prob2')\n",
    "\n",
    "    z4 = forward_propagation(x, params, keep_prob1, keep_prob2)\n",
    "    p = tf.nn.softmax(z4, dim=0)  # dim=0 because the classes are on that axis\n",
    "    # p = tf.argmax(z4) # this gives only the predicted class as output\n",
    "\n",
    "    sess = tf.Session()\n",
    "    prediction = sess.run(p, feed_dict={x: X, keep_prob1: 1.0, keep_prob2: 1.0})\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now I define the model function which is in fact the neural network that we will train afterwards. An important difference with respect to [my previous MNIST example](https://github.com/dariodata/TensorFlow-MNIST/blob/master/TensorFlow-MNIST.ipynb) is that I added an additional regularization term to the cost function. I used L2 regularization to penalize the weights in all four layers. The bias was not penalized as this is not necessary. The strictness of this penalty was given by a `beta` constant defined at 0.01.\n",
    "\n",
    "Why use additional regularization? Because this allowed me to decrease the variance, i.e. decrease the difference in performance of the model with the training set compared to the validation set. This produced my best submission in the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate=0.0001,\n",
    "          num_epochs=1000, minibatch_size=64, print_cost=True):\n",
    "    \"\"\"\n",
    "    Implements a four-layer tensorflow neural network: (LINEAR->RELU)^3->LINEAR->SOFTMAX.\n",
    "\n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (input size, number of training examples)\n",
    "    Y_train -- test set, of shape (output size, number of training examples)\n",
    "    X_test -- training set, of shape (input size, number of training examples)\n",
    "    Y_test -- test set, of shape (output size, number of test examples)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "\n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    ops.reset_default_graph()  # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)  # to keep consistent results\n",
    "    seed = 3  # to keep consistent results\n",
    "    (n_x, m) = X_train.shape  # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]  # n_y : output size\n",
    "    costs = []  # To keep track of the cost\n",
    "    t0 = time.time()  # to mark the start of the training\n",
    "\n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "    keep_prob1 = tf.placeholder(tf.float32, name='keep_prob1')  # probability to keep a unit during dropout\n",
    "    keep_prob2 = tf.placeholder(tf.float32, name='keep_prob2')\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters()\n",
    "\n",
    "    # Forward propagation\n",
    "    Z4 = forward_propagation(X, parameters, keep_prob1, keep_prob2)\n",
    "\n",
    "    # Cost function\n",
    "    cost = compute_cost(Z4, Y)\n",
    "    regularizers = tf.nn.l2_loss(parameters['W1']) + tf.nn.l2_loss(parameters['W2']) + tf.nn.l2_loss(parameters['W3']) \\\n",
    "                   + tf.nn.l2_loss(parameters['W4'])  # add regularization term\n",
    "    beta = 0.01  # regularization constant\n",
    "    cost = tf.reduce_mean(cost + beta * regularizers)  # cost with regularization\n",
    "\n",
    "    # Backpropagation: Define the tensorflow AdamOptimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "\n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.  # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size)  # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\"\n",
    "                _, minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y,\n",
    "                                                                           keep_prob1: 0.7, keep_prob2: 0.5})\n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print(\"Cost after epoch {}: {:f}\".format(epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print(\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z4), tf.argmax(Y))\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        train_cost = cost.eval({X: X_train, Y: Y_train, keep_prob1: 1.0, keep_prob2: 1.0})\n",
    "        test_cost = cost.eval({X: X_test, Y: Y_test, keep_prob1: 1.0, keep_prob2: 1.0})\n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train, keep_prob1: 1.0, keep_prob2: 1.0})\n",
    "        test_accuracy = accuracy.eval({X: X_test, Y: Y_test, keep_prob1: 1.0, keep_prob2: 1.0})\n",
    "\n",
    "        print('Finished training in %s s' % (time.time() - t0))\n",
    "        print(\"Train Cost:\", train_cost)\n",
    "        print(\"Test Cost:\", test_cost)\n",
    "        print(\"Train Accuracy:\", train_accuracy)\n",
    "        print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per fives)')\n",
    "        plt.title(\"Learning rate = {}, beta = {},\\n\"\n",
    "                  \"test cost = {:.6f}, test accuracy = {:.6f}\".format(learning_rate, beta, test_cost, test_accuracy))\n",
    "        global filename\n",
    "        filename = timestr + '_NN4Lstage2_lr_{}_beta_{}_cost_{:.2f}-{:.2f}_acc_{:.2f}-{:.2f}'.format(\n",
    "            learning_rate, beta, train_cost, test_cost, train_accuracy, test_accuracy)\n",
    "        plt.savefig(dirname + filename + '.png')\n",
    "\n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the model function will return the learned parameters from the network and additionally will plot the cost after each epoch. The plot is also saved as a file that includes the timestamp as well as the learning rate, beta, cost and accuracy information for this particular run.\n",
    "\n",
    "Now it's time to train the model using the train and validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 6.607861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 100: 1.389869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 200: 0.988806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 300: 0.882713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 400: 0.833693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 500: 0.811457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 600: 0.793379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 700: 0.773927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 800: 0.762247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 900: 0.767449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training in 498.4203100204468 s\nTrain Cost: 0.665462\nTest Cost: 1.74987\nTrain Accuracy: 0.979292\nTest Accuracy: 0.643609\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAElCAYAAADnZln1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b6892f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train the model and get learned parameters\n",
    "parameters = model(X_train, Y_train, X_val, Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From my validation results we can observe that the network learned nicely. However, the final cost of the training data was 0.665462, where as the validation data had a final cost of 1.74987. This is a large difference and an indication that the model is overfitting. Moreover the accuracy (defined here as the fraction of correct predictions) is very high (97.9%) for the training data and only 64.3% for the validation set. Another indication that the model is overfitting even though I have used both dropout and L2 regularization to counteract this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions\n",
    "\n",
    "We use the learned parameteres to make a prediction on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use learned parameters to make prediction on test data\n",
    "prediction = predict(X_test, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example of a prediction. As we can see below, the prediction consists of the probabilities of the entry belongin to each of the nine different categories (this was the format needed for this competition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.36503336,  0.21219006,  0.01297534,  0.14676626,  0.08375936,\n        0.09217557,  0.02737238,  0.03150512,  0.02822249], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 986)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we have to do now is create a submission .csv file to save our prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create submission file\n",
    "submission = pd.DataFrame(prediction.T)\n",
    "submission['id'] = test_index\n",
    "submission.columns = ['class1', 'class2', 'class3', 'class4', 'class5', 'class6', 'class7', 'class8', 'class9', 'id']\n",
    "submission.to_csv(dirname + filename + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results interpretation\n",
    "\n",
    "Using this neural network model, my submission to Kaggle yielded following results:\n",
    "\n",
    "- Public score (based on a portion of the test data by Kaggle to provide an indication of performance during the competition): Loss = 1.69148\n",
    "- Private score (based on a different portion of the test data by Kaggle to provide the final score at the end of the competition): Loss = 2.74500\n",
    "\n",
    "The discrepancy between these two scores further shows that overfitting is an issue in working with this data in a neural network model. My model could benefit from increasing the training data and a higher regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}