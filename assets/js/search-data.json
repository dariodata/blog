{
  
    
        "post0": {
            "title": "Graph Convolutional Networks for Fraud Detection of Bitcoin Transactions",
            "content": "tl;dr . I trained 4 different types of models to classify bitcoin transactions. For each, two versions of the feature set were used: all features (local + neighborhood-aggregated) and only local features (without neighborhood information). . The best model was a Random Forest trained with all features: its performance was impaired when the aggregated features were removed. | The best graph-based neural network model was APPNP and its performance was better when only local features were used. APPNP performed better than an MLP with comparable complexity, indicating that the graph structure information gave it an advantage. | Finally, the best GCN model required using all features and several strategies to reduce overfitting. | . The excellent performance of a Random Forest shows that it makes sense to consider simple models when faced with a new task. It also indicates that the individual node features in the Elliptic dataset are already informative enough to make good predictions. It would be interesting to explore how the model performs, when fewer samples and/or features are available for training. . A shallow GCN with 2 layers might not be a good choice for node classification of a graph as sparse as the bitcoin transaction graph. If a node has few incoming edges, a graph convolution may not have enough neighbors with features to aggregate. . An interesting solution is provided by the APPNP model, which combines message passing with the teleportation principle of personalized pagerank. The long-range (20 iterations in the best model) of the predictions propagation through the network is an aspect that deserves further attention in the future. . The main performance metrics for comparison were: . Model Features Dropout Precision Recall F1 score . GCN | all | 0.5 | 0.8051 | 0.4958 | 0.6137 | . GCN | local | 0. | 0.6667 | 0.4617 | 0.5456 | . APPNP | all | 0.2 | 0.7791 | 0.6251 | 0.6936 | . APPNP | local | 0. | 0.8158 | 0.6787 | 0.7409 | . MLP | all | 0.2 | 0.6538 | 0.6593 | 0.6565 | . MLP | local | 0. | 0.7799 | 0.6740 | 0.7231 | . RandomForest | all | | 0.9167 | 0.7211 | 0.8072 | . RandomForest | local | | 0.8749 | 0.7036 | 0.7799 | . Disclaimer: This was a hobby project done mostly nocturnally and on the weekends out of pure fascination for graph theory and neural networks. Although I made every effort to apply scientific rigor, these results constitute an initial exploration and should not be considered an exhaustive analysis. Importantly, due to the random nature of certain parameters (e.g. dropout), multiple repetitions of the experiments using different random seeds and/or different validation splits are necessary for a conclusive judgement. . Introduction . The Elliptic Data Set consists of anonymized transactions collected from the bitcoin exchange during 49 distinct time-periods. The transactions are represented as a graph containing 203769 nodes (transactions) and 234355 edges (bitcoin flow from one transaction to another). A subset of the transactions are labeled as licit or illicit. A detailed description of the dataset and an initial approach applying graph convolutional networks (GCNs) for the task of node classification has been addressed by: . M. Weber, G. Domeniconi, J. Chen, D. K. I. Weidele, C. Bellei, T. Robinson, C. E. Leiserson, &quot;Anti-Money Laundering in Bitcoin:Experimenting with Graph Convolutional Networks for Financial Forensics&quot;, KDD ’19 Workshop on Anomaly Detection in Finance, August 2019, Anchorage, AK, USA. In this notebook, I will take a closer look to how graph-based neural networks can be applied to this task and propose possible directions for future analyses. . Due to the longer training times and for reproducibility, the experiments were all run using the script in models.py and all runs and metrics were tracked on Weights&amp;Biases. All results were exported to a csv file using this script and loaded onto this notebook for visualization. . import os import random import time import dgl import networkx as nx import numpy as np import pandas as pd import torch import torch.nn as nn import torch.nn.functional as F from dgl.nn.pytorch import GraphConv from sklearn.metrics import confusion_matrix, precision_recall_fscore_support import matplotlib import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline matplotlib.rcParams[&#39;figure.dpi&#39;] = 300 # set random seeds seed = 0 random.seed(seed) np.random.seed(seed) dgl.random.seed(seed) torch.manual_seed(seed); path = os.path.realpath(&#39;.&#39;) . . # load experiment results exported from Weights&amp;Biases runs_config = pd.read_csv(path+&#39;/experiments_summary.csv&#39;) runs_metrics = pd.read_csv(path+&#39;/experiments_metrics.csv&#39;) runs = runs_metrics.merge(runs_config, left_on=&#39;name&#39;, right_on=&#39;name&#39;, suffixes=(&#39;&#39;,&#39;_&#39;)) runs.rename(columns={&#39;_step&#39;:&#39;epoch&#39;}, inplace=True) runs[&#39;nobias&#39;] = runs[&#39;nobias&#39;].astype(str) runs[&#39;dropout&#39;] = runs[&#39;dropout&#39;].astype(float) runs[&#39;k&#39;] = runs[&#39;k&#39;].astype(str) runs[&#39;alpha&#39;] = runs[&#39;alpha&#39;].astype(str) runs.query(&#39;nhidden==&quot;100&quot; and _step_&gt;=999.0&#39;, inplace=True) . . Transaction data . Three tables are initially available to download from Kaggle&#39;s dataset repository: . An edgelist: the edges between bitcoin transactions (nodes identified by transaction id) necessary to build the graph | A classes table: label for each transaction can be licit, illicit, or unknown | A features table with 167 columns Transaction id | Timestep: consecutive periods of time for which all bitcoin flows are translated to edges in a graph Edges exist only between transactions within the same timestep | 93 local features, i.e. intrinsic properties of the transactions themselves such as amount, transaction fee, etc. | 72 aggregated features with information about the immediate neighborhood of each node, e.g. sum of amounts of the neighboring transactions | . | . df_edges = pd.read_csv(path + &quot;/elliptic_bitcoin_dataset/elliptic_txs_edgelist.csv&quot;) df_classes = pd.read_csv(path + &quot;/elliptic_bitcoin_dataset/elliptic_txs_classes.csv&quot;) df_features = pd.read_csv( path + &quot;/elliptic_bitcoin_dataset/elliptic_txs_features.csv&quot;, header=None ) # rename the classes to ints that can be handled by pytorch as labels df_classes[&quot;label&quot;] = df_classes[&quot;class&quot;].replace( {&quot;unknown&quot;: -1, # unlabeled nodes &quot;2&quot;: 0, # labeled licit nodes #&quot;1&quot;: 1, # labeled illicit nodes } ).astype(int) # rename features according to data description in paper rename_dict = dict( zip( range(0, 167), [&quot;txId&quot;, &quot;time_step&quot;] + [f&quot;local_{i:02d}&quot; for i in range(1, 94)] + [f&quot;aggr_{i:02d}&quot; for i in range(1, 73)], ) ) df_features.rename(columns=rename_dict, inplace=True) . print(f&quot;Number of missing data points: {df_features.isna().sum().sum()+df_classes.isna().sum().sum()}&quot;) print(f&quot;Number of nodes (transactions): {df_features[&#39;txId&#39;].nunique()}&quot;) print(f&quot;Number of edges: {df_edges.shape[0]}&quot;) print(f&quot;Number of classes: {df_classes[&#39;class&#39;].nunique()}&quot;) print(f&quot;Timesteps range from {df_features[&#39;time_step&#39;].min()} to {df_features[&#39;time_step&#39;].max()}&quot;) . Number of missing data points: 0 Number of nodes (transactions): 203769 Number of edges: 234355 Number of classes: 3 Timesteps range from 1 to 49 . Correlation analysis . Additionally, the dataset was analyzed using the handy pandas-profiling package. The complete script for the analysis is in eda.py, which generates a detailed report including multiple correlations. The main findings from the report can be summarized as: . 29 features are highly skewed | 76 features are highly correlated to other features in the dataset (Spearman correlation coefficient $ rho &gt; 0.90$) 21 aggregated features are highly correlated to other aggregated features | 54 local features are highly correlated to other local features | time_step is highly correlated with aggr_43 ($ rho = 0.91$) | . | . Constructing the transaction graph . We now have our data prepared in table format, but we want to be able to work on the graph constructed from the data. In order to create our transaction graph, we use the networkx package. We create a directed multigraph (a directed graph that allows for multiple edges between two nodes) and add the label attribute to each transaction. . g_nx = nx.MultiDiGraph() g_nx.add_nodes_from( zip(df_classes[&quot;txId&quot;], [{&quot;label&quot;: v} for v in df_classes[&quot;label&quot;]]) ) g_nx.add_edges_from(zip(df_edges[&quot;txId1&quot;], df_edges[&quot;txId2&quot;])); . print(f&quot;Graph with {g_nx.number_of_nodes()} nodes and {g_nx.number_of_edges()} edges.&quot;) print(f&quot;Number of connected components: {len(list(nx.weakly_connected_components(g_nx)))}&quot;) . Graph with 203769 nodes and 234355 edges. Number of connected components: 49 . We can confirm that there are 49 connected components (weakly connected compoments in the case of directed graphs) was constructed for each timestep. This means that the dataset consists of 49 different subgraphs, each corresponding to one timestep. . components = list(nx.weakly_connected_components(g_nx)) g_nx_t_list = [g_nx.subgraph(components[i]) for i in range(0,len(components))] . with sns.axes_style(&#39;white&#39;): fig, ax = plt.subplots(1,2, figsize=(12,6)) for i,t in enumerate([26,48]): node_label = list(nx.get_node_attributes(g_nx_t_list[t], &#39;label&#39;).values()) mapping = {-1:&#39;grey&#39;, 0:&#39;C0&#39;, 1:&#39;C3&#39;} node_color = [mapping[l] for l in node_label] nx.draw_networkx(g_nx_t_list[t], node_size=10, node_color=node_color, with_labels=False, width=0.2, alpha=0.8, arrowsize=8, ax=ax[i]) leg = ax[0].legend([&#39;unlabeled&#39;, &#39;licit&#39;, &#39;illicit&#39;]) leg.legendHandles[0].set_color(&#39;grey&#39;) leg.legendHandles[1].set_color(&#39;C0&#39;) leg.legendHandles[2].set_color(&#39;C3&#39;) plt.tight_layout() . /anaconda3/envs/dgl/lib/python3.7/site-packages/networkx/drawing/nx_pylab.py:579: MatplotlibDeprecationWarning: The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. if not cb.iterable(width): /anaconda3/envs/dgl/lib/python3.7/site-packages/networkx/drawing/nx_pylab.py:676: MatplotlibDeprecationWarning: The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. if cb.iterable(node_size): # many node sizes . We can see that most of the transactions are not labeled and that only a minority of the labeled nodes correspond to illicit transactions. Moreover, there graph does not seem to be particularly dense. There seem to be chains of transactions one after the other. Also, many of these chains seem to concentrate one type of labeled transaction: either licit or illicit. Finally, a minority of nodes seems to have a larger number of edges, connecting with transactions in multiple chains or further away from their inmediate neighborhood. . Graph metrics . We can calculate selected graph metrics to better quantify some important structural properties of the transaction graph. . g_metrics = {} g_metrics[&#39;timestep&#39;] = np.arange(1,50) g_metrics[&#39;number_of_nodes&#39;] = [graph.number_of_nodes() for graph in g_nx_t_list] g_metrics[&#39;avg_degree&#39;] = [np.mean(list(dict(nx.degree(graph)).values())) for graph in g_nx_t_list] g_metrics[&#39;density&#39;] = [nx.density(graph) for graph in g_nx_t_list] g_metrics[&#39;avg_clustering&#39;] = [nx.average_clustering(nx.DiGraph(graph)) for graph in g_nx_t_list] g_metrics[&#39;avg_shortest_path&#39;] = [nx.average_shortest_path_length(nx.DiGraph(graph)) for graph in g_nx_t_list] . fig, ax = plt.subplots(len(g_metrics)-1,1, figsize=(10,6), sharex=True) for i,label in enumerate(list(g_metrics.keys())[1:]): ax[i].bar(g_metrics[&#39;timestep&#39;], g_metrics[label], label=label) ax[i].legend() plt.xlabel(&#39;timestep&#39;); . print(f&quot;Average density of the graphs across all timesteps: {np.mean(g_metrics[&#39;density&#39;]):.6f}&quot;) print(f&quot;Average degree of all nodes across all timesteps: {np.mean(list(dict(nx.degree(g_nx)).values())):.2f}&quot;) . Average density of the graphs across all timesteps: 0.000318 Average degree of all nodes across all timesteps: 2.30 . Given the density of the transaction graph, it seems to be rather sparse. The average density accross all timesteps lies around 0.0003177 and each node has an average of 2.30 edges. In comparison, the Cora dataset (popularly used as a benchmark of node classification algorithms) has an average degree of 3.90 and a density of 0.0014812. With only 2708 nodes, Cora is a much smaller and denser graph. . Training a Graph Convolutional Network . To build and train the GCN, I used DGL as a framework for deep learning on graphs DGL is based on pytorch and uses DGLGraph objects that can be easily created from networkx graphs. Moreover, several implementations of graph-based neural layers are available in DGL. . Graph creation . First we create the DGLGraph from a networkx Graph. We also add the label information as a tensor to the node data in the DGLGraph (from now on simply &quot;graph&quot;). Similarly, we add the node feature matrix to the graph as a tensor of shape (number of nodes, number of features) = (203769, 166). . Importantly, I tested the performance of the GCN using two options for constructing the graph: . Unidirectional edges: edges going from one transaction to the next. In a GCN, this would mean that information used by the model to classify nodes would flow in one direction (downstream) only. In this case, we use g_nx directly. | Bidirectional edges: edges are made bidirectional, which would allow information flow in a GCN to travel in both directions (downstream and upstream). In this case, we use g_nx.to_undirected().to_directed(), i.e. we first make the edges undirected, and then back again directed. By doing so, networkx makes the edges in the resulting graph bidirectional. | . It is not explicitely stated in the paper, which of these two options was used by Weber et al. However, from the performance metrics, it is likely that they used the bidirectional edges version. . g = dgl.DGLGraph() g.from_networkx(g_nx) g.ndata[&quot;label&quot;] = torch.tensor( df_classes.set_index(&quot;txId&quot;).loc[sorted(g_nx.nodes()), &quot;label&quot;].values ) g.ndata[&quot;features_matrix&quot;] = torch.tensor( df_features.set_index(&quot;txId&quot;).loc[sorted(g_nx.nodes()), :].values ) print(g) . DGLGraph(num_nodes=203769, num_edges=234355, ndata_schemes={&#39;label&#39;: Scheme(shape=(), dtype=torch.int64), &#39;features_matrix&#39;: Scheme(shape=(166,), dtype=torch.float64)} edata_schemes={}) . g_nx_bidirectional = g_nx.to_undirected().to_directed() # create bidirectional graph g_bi = dgl.DGLGraph() g_bi.from_networkx(g_nx_bidirectional) g_bi.ndata[&quot;label&quot;] = torch.tensor( df_classes.set_index(&quot;txId&quot;).loc[sorted(g_nx.nodes()), &quot;label&quot;].values ) g_bi.ndata[&quot;features_matrix&quot;] = torch.tensor( df_features.set_index(&quot;txId&quot;).loc[sorted(g_nx.nodes()), :].values ) print(g_bi) . DGLGraph(num_nodes=203769, num_edges=468710, ndata_schemes={&#39;label&#39;: Scheme(shape=(), dtype=torch.int64), &#39;features_matrix&#39;: Scheme(shape=(166,), dtype=torch.float64)} edata_schemes={}) . Graph normalizaiton . The performance of the GCN benefits from using a normalized version of the adjacency matrix. I applied a common normalization approach used in the literature. It starts by adding a self-loop to each node, which is the equivalent of adding the identity matrix to the adjacency matrix A $$ tilde{A} = A + I$$ . The rest of the normalization consists of obtaining the normalized graph Laplacian (here is a great overview explaining what the Laplacian means), which can be calculated as $$ hat{A} = D^{-1/2} tilde{A}D^{-1/2}$$ where $D$ is a matrix whose diagonal contains the degree of each node of $ tilde{A}$. . This matrix multiplication notation effectively means that the normalized Laplacian can have as a value in each $(i,j)$ position: $$ hat{A}(i,j) = begin{cases} 1 &amp; text{if $i=j$} [2ex] {-1 over { sqrt{ deg(i) deg(j)}}} &amp; text{if $i neq j$ and $(i,j) in E$}, [2ex] 0 &amp; text{otherwise} end{cases}$$ where $E$ is the set of edges of the graph. As we can see, nodes with an (in)degree of zero could be troublesome, which is why we add the self-loops. . g.add_edges(g.nodes(), g.nodes()) print(g) # add self loop to the bidirectional edges graph g_bi.add_edges(g_bi.nodes(), g_bi.nodes()) print(g_bi) . DGLGraph(num_nodes=203769, num_edges=438124, ndata_schemes={&#39;label&#39;: Scheme(shape=(), dtype=torch.int64), &#39;features_matrix&#39;: Scheme(shape=(166,), dtype=torch.float64)} edata_schemes={}) DGLGraph(num_nodes=203769, num_edges=672479, ndata_schemes={&#39;label&#39;: Scheme(shape=(), dtype=torch.int64), &#39;features_matrix&#39;: Scheme(shape=(166,), dtype=torch.float64)} edata_schemes={}) . Train/validation data splitting . Now that our graph is ready, we need to split our data into training and validation sets. We will follow the same approach taken by Weber et al., which consists in a time-based split using the initial 70% of the timesteps to train the model and the remaining 30% for validation. This temporal split makes sense in the context of possible applications of such a model. A company could continuosly use their older historical data and labels for training and then predict the node classes for more recent transactions. . features = g.ndata[&quot;features_matrix&quot;].float() labels = g.ndata[&quot;label&quot;].long() # format required for cross entropy loss in_feats = features.shape[1] n_classes = 2 # licit or illicit (unknown label is ignored) n_edges = g.number_of_edges() dataset_size = df_classes[&quot;label&quot;].notna().sum() train_ratio = 0.7 train_time_steps = round(len(np.unique(features[:, 0])) * train_ratio) shutdown_timestep = 43 train_indices = ( ((features[:, 0] &lt;= train_time_steps) &amp; (labels != -1)).nonzero().view(-1) ) val_indices = ( ((features[:, 0] &gt; train_time_steps) &amp; (labels != -1)).nonzero().view(-1) ) # timestep_indices = { # t:( # ((features[:, 0] == t) &amp; (labels != -1)).nonzero().view(-1) # ) for t in range(train_time_steps+1,50) # } print(f&quot;&quot;&quot;Number of timesteps used for training: {train_time_steps} Number of timesteps used for validation: {dataset_size-train_time_steps}&quot;&quot;&quot;) . Number of timesteps used for training: 34 Number of timesteps used for validation: 203735 . GCN model architecture . For the GCN model, I used the implementation from DGL, which is based on the original implementation used by Kipf et al. (2016). In short, the algorithm is given by the formula . $$H^{(l+1)} = sigma (D^{-1/2} tilde{A}D^{-1/2}H^{(l)}W^{(l)})$$ . where $ sigma$ is the activation function (ReLu), $D^{-1/2} tilde{A}D^{-1/2}$ is the normalized graph Laplacian, $H^{(l)}$ are the logits and $W^{(l)}$ are the learnable weights of the $l$th layer of the neural network. It is also possible to add a learnable bias to each layer. There are some very good explanations of how this algorithm works available for reference. . class GCN(nn.Module): def __init__( self, g, in_feats, n_hidden, n_classes, n_layers, activation, dropout, bias ): super(GCN, self).__init__() self.g = g self.layers = nn.ModuleList() # input layer self.layers.append( GraphConv(in_feats, n_hidden, activation=activation, bias=bias) ) # hidden layers for _ in range(n_layers - 2): self.layers.append( GraphConv(n_hidden, n_hidden, activation=activation, bias=bias) ) # output layer self.layers.append(GraphConv(n_hidden, n_classes, bias=bias)) self.dropout = nn.Dropout(p=dropout) def forward(self, features): h = features for i, layer in enumerate(self.layers): if i != 0: h = self.dropout(h) h = layer(self.g, h) return h # utility function to evaluate the model def evaluate(model, loss_fcn, features, labels, mask): &quot;&quot;&quot;Calculate the loss, accuracy, precision, recall and f1_score for the masked data&quot;&quot;&quot; model.eval() with torch.no_grad(): logits = model(features) logits = logits[mask] labels = labels[mask] loss = loss_fcn(logits, labels) _, indices = torch.max(logits, dim=1) correct = torch.sum(indices == labels) p, r, f, _ = precision_recall_fscore_support(labels, indices) return loss, correct.item() * 1.0 / len(labels), p[1], r[1], f[1] # utility function to obtain a confusion matrix def eval_confusion_matrix(model, features, labels, mask): model.eval() with torch.no_grad(): logits = model(features) logits = logits[mask] labels = labels[mask] _, indices = torch.max(logits, dim=1) print(confusion_matrix(labels, indices)) . Model training . Now we can train the model using the specifications from the paper by Weber et al.: . cross entropy loss function putting higher weight for the positive (illicit) samples: 0.7 positive vs 0.3 negative | adam optimizer with learning rate 1e-3 | no weight decay is mentioned | no bias is mentioned | no dropout | train for 1000 epochs | two Graph Convolutional layers with 100 and 2 neurons respectively | . We can now define the Graph Convolutional Network architecture using DGL. In this case, it consists of n_layers GCN layers with n_hidden hidden units per layer: . def train_eval_model(model_class, g, features, **params): #bidirectional = params[&quot;bidirectional&quot;] if &quot;bidirectional&quot; in params else None in_feats = features.shape[1] n_classes = 2 n_hidden = params[&quot;n_hidden&quot;] n_layers = params[&quot;n_layers&quot;] weight_decay = params[&quot;weight_decay&quot;] bias = params[&quot;bias&quot;] dropout = params[&quot;dropout&quot;] epochs = params[&quot;epochs&quot;] lr = params[&quot;lr&quot;] posweight = params[&quot;posweight&quot;] model = model_class(g, in_feats, n_hidden, n_classes, n_layers, F.relu, dropout, bias) # weighted cross entropy loss function loss_fcn = torch.nn.CrossEntropyLoss( weight=torch.tensor([1 - posweight, posweight]) ) # use optimizer optimizer = torch.optim.Adam( model.parameters(), lr=lr, weight_decay=weight_decay ) dur = [] metrics = {&quot;loss&quot;:{&quot;train&quot;: [], &quot;val&quot;: []}, &quot;accuracy&quot;:{&quot;train&quot;: [], &quot;val&quot;: []}, &quot;precision&quot;:{&quot;train&quot;: [], &quot;val&quot;: []}, &quot;recall&quot;:{&quot;train&quot;: [], &quot;val&quot;: []}, &quot;f1_score&quot;:{&quot;train&quot;: [], &quot;val&quot;: []}, } for epoch in range(epochs): model.train() if epoch &gt;= 3: t0 = time.time() # forward pass logits = model(features) loss = loss_fcn(logits[train_indices], labels[train_indices]) metrics[&quot;loss&quot;][&quot;train&quot;].append(loss) # backward pass optimizer.zero_grad() loss.backward() optimizer.step() # duration if epoch &gt;= 3: dur.append(time.time() - t0) # evaluate on training set _, train_acc, train_precision, train_recall, train_f1_score = evaluate( model, loss_fcn, features, labels, train_indices ) metrics[&quot;accuracy&quot;][&quot;train&quot;].append(train_acc) metrics[&quot;precision&quot;][&quot;train&quot;].append(train_precision) metrics[&quot;recall&quot;][&quot;train&quot;].append(train_recall) metrics[&quot;f1_score&quot;][&quot;train&quot;].append(train_f1_score) # evaluate on validation set val_loss, val_acc, val_precision, val_recall, val_f1_score = evaluate( model, loss_fcn, features, labels, val_indices ) metrics[&quot;loss&quot;][&quot;val&quot;].append(val_loss) metrics[&quot;accuracy&quot;][&quot;val&quot;].append(val_acc) metrics[&quot;precision&quot;][&quot;val&quot;].append(val_precision) metrics[&quot;recall&quot;][&quot;val&quot;].append(val_recall) metrics[&quot;f1_score&quot;][&quot;val&quot;].append(val_f1_score) if (epoch + 1) % 100 == 0: print( f&quot;Epoch {epoch:05d} | Time(s) {np.mean(dur):.2f} | val_loss {val_loss.item():.4f} &quot; f&quot;| Precision {val_precision:.4f} | Recall {val_recall:.4f} | Acc {val_acc:.4f} &quot; f&quot;| F1_score {val_f1_score:.4f}&quot; ) print(&quot;Confusion matrix:&quot;) eval_confusion_matrix(model, features, labels, val_indices) return model, metrics . params = { &quot;n_hidden&quot; : 100, &quot;n_layers&quot; : 2, &quot;weight_decay&quot; : 0., &quot;bias&quot; : False, &quot;dropout&quot; : 0., &quot;epochs&quot; : 1000, &quot;lr&quot; : 1e-3, &quot;posweight&quot; : 0.7, } # train on graph with unidirectional edges model, metrics = train_eval_model(GCN, g, features, **params) . Epoch 00099 | Time(s) 0.60 | val_loss 0.3580 | Precision 0.2701 | Recall 0.5125 | Acc 0.8783 | F1_score 0.3537 Epoch 00199 | Time(s) 0.60 | val_loss 0.3381 | Precision 0.3649 | Recall 0.4589 | Acc 0.9130 | F1_score 0.4065 Epoch 00299 | Time(s) 0.59 | val_loss 0.3537 | Precision 0.4101 | Recall 0.4358 | Acc 0.9226 | F1_score 0.4226 Epoch 00399 | Time(s) 0.58 | val_loss 0.4015 | Precision 0.4039 | Recall 0.4192 | Acc 0.9221 | F1_score 0.4114 Epoch 00499 | Time(s) 0.58 | val_loss 0.4556 | Precision 0.3895 | Recall 0.3989 | Acc 0.9203 | F1_score 0.3942 Epoch 00599 | Time(s) 0.58 | val_loss 0.5079 | Precision 0.3622 | Recall 0.3860 | Acc 0.9160 | F1_score 0.3737 Epoch 00699 | Time(s) 0.59 | val_loss 0.5570 | Precision 0.3533 | Recall 0.3813 | Acc 0.9145 | F1_score 0.3668 Epoch 00799 | Time(s) 0.59 | val_loss 0.6020 | Precision 0.3702 | Recall 0.3860 | Acc 0.9175 | F1_score 0.3779 Epoch 00899 | Time(s) 0.60 | val_loss 0.6472 | Precision 0.3677 | Recall 0.3915 | Acc 0.9167 | F1_score 0.3792 Epoch 00999 | Time(s) 0.60 | val_loss 0.7006 | Precision 0.3618 | Recall 0.3869 | Acc 0.9158 | F1_score 0.3739 Confusion matrix: [[14848 739] [ 664 419]] . model_bi, metrics_bi = train_eval_model(GCN, g_bi, features, **params) . Epoch 00099 | Time(s) 0.62 | val_loss 0.3113 | Precision 0.3251 | Recall 0.4765 | Acc 0.9017 | F1_score 0.3865 Epoch 00199 | Time(s) 0.62 | val_loss 0.3229 | Precision 0.4209 | Recall 0.4543 | Acc 0.9239 | F1_score 0.4369 Epoch 00299 | Time(s) 0.62 | val_loss 0.3302 | Precision 0.4776 | Recall 0.4331 | Acc 0.9324 | F1_score 0.4542 Epoch 00399 | Time(s) 0.63 | val_loss 0.3456 | Precision 0.5940 | Recall 0.4054 | Acc 0.9434 | F1_score 0.4819 Epoch 00499 | Time(s) 0.63 | val_loss 0.3675 | Precision 0.7227 | Recall 0.3850 | Acc 0.9504 | F1_score 0.5024 Epoch 00599 | Time(s) 0.63 | val_loss 0.3933 | Precision 0.7757 | Recall 0.3832 | Acc 0.9527 | F1_score 0.5130 Epoch 00699 | Time(s) 0.63 | val_loss 0.4211 | Precision 0.7784 | Recall 0.3795 | Acc 0.9527 | F1_score 0.5102 Epoch 00799 | Time(s) 0.63 | val_loss 0.4465 | Precision 0.7778 | Recall 0.3813 | Acc 0.9527 | F1_score 0.5118 Epoch 00899 | Time(s) 0.63 | val_loss 0.4769 | Precision 0.7874 | Recall 0.3795 | Acc 0.9530 | F1_score 0.5121 Epoch 00999 | Time(s) 0.63 | val_loss 0.5053 | Precision 0.7684 | Recall 0.3767 | Acc 0.9521 | F1_score 0.5056 Confusion matrix: [[15464 123] [ 675 408]] . fig, ax = plt.subplots(1,3, figsize=(18,5), sharex=True) ax[0].plot(metrics[&quot;loss&quot;][&#39;train&#39;], label=&#39;unidir. train_loss&#39;, color=&#39;C0&#39;) ax[0].plot(metrics[&quot;loss&quot;][&#39;val&#39;], label=&#39;unidir. val_loss&#39;, color=&#39;C0&#39;, ls=&#39;:&#39;) ax[1].plot(metrics[&#39;f1_score&#39;][&#39;val&#39;], label=&#39;unidir. val_f1_score&#39;, color=&#39;C0&#39;) ax[2].plot(metrics[&#39;precision&#39;][&#39;val&#39;], label=&#39;unidir. val_precision&#39;, color=&#39;C0&#39;) ax[2].plot(metrics[&#39;recall&#39;][&#39;val&#39;], label=&#39;unidir. val_recall&#39;, color=&#39;C0&#39;, ls=&#39;:&#39;) ax[0].plot(metrics_bi[&quot;loss&quot;][&#39;train&#39;], label=&#39;bidir. train_loss&#39;, color=&#39;C3&#39;) ax[0].plot(metrics_bi[&quot;loss&quot;][&#39;val&#39;], label=&#39;bidir. val_loss&#39;, color=&#39;C3&#39;, ls=&#39;:&#39;) ax[1].plot(metrics_bi[&#39;f1_score&#39;][&#39;val&#39;], label=&#39;bidir. val_f1_score&#39;, color=&#39;C3&#39;) ax[2].plot(metrics_bi[&#39;precision&#39;][&#39;val&#39;], label=&#39;bidir. val_precision&#39;, color=&#39;C3&#39;) ax[2].plot(metrics_bi[&#39;recall&#39;][&#39;val&#39;], label=&#39;bidir. val_recall&#39;, color=&#39;C3&#39;, ls=&#39;:&#39;) ax[0].legend() ax[1].legend() ax[2].legend(); . Training the GCN model with these parameters led to a poorer performance than the one reported in the paper by Weber et al. The bidirectional graph variant produced the better results. Therefore it is also probably the setting used in the paper. Even though the Bitcoin flow from one transaction to another is intuitively one-directional, this would mean that, in a GCN, the information would only flow downstream. Having a bidirectional flow of information through the edges of the graph in the GCN makes information available to each node both upstream and downstream from it. This greatly improved the performance of the GCN. . Model Edges Dropout Precision Recall F1 score . GCN (Weber et al.) | | 0. | 0.812 | 0.512 | 0.628 | . GCN | unidirectional | 0. | 0.3764 | 0.3823 | 0.3793 | . GCN | bidirectional | 0. | 0.7860 | 0.3832 | 0.5152 | . In order to figure out, whether there are additional parameters that I did not yet consider in replicating the GCN approach, I performed a series of experiments changing further training parameters and comparing the results. This, in turn, was useful in understanding in what ways the model could be modified to increase its performance. . Additional experiments with GCNs . Address overfitting by weight decay L2-regularization . One observation from the previous learning curves is that the validation loss starts to increase again after ca. 400 epochs, a clear sign of overfitting. One way to address this is to add dropout to the model. In this case, I added weight-decay L2-regularization to improve training. . I further added a learnable bias to the GCN to see if this improved its performance (it did slightly). The other parameters were left intact. . query = &#39;model==&quot;gcn&quot; and onlylocal==False and dropout==&quot;0&quot;&#39; print(f&quot;Plotting {runs.query(query)[&#39;name&#39;].nunique()} runs: {runs.query(query)[&#39;name&#39;].unique()}&quot;) g = sns.relplot(&#39;epoch&#39;, &#39;train_loss&#39;, col=&#39;weight_decay&#39;, hue=&#39;bidirectional&#39;, style=&#39;nobias&#39;, palette=sns.color_palette(&quot;Set1&quot;, 2), kind=&#39;line&#39;, data=runs.query(query)); plt.ylim(0,1); g = sns.relplot(&#39;epoch&#39;, &#39;val_loss&#39;, col=&#39;weight_decay&#39;, hue=&#39;bidirectional&#39;, style=&#39;nobias&#39;, palette=sns.color_palette(&quot;Set1&quot;, 2), kind=&#39;line&#39;, data=runs.query(query)) plt.ylim(0,1); . Plotting 8 runs: [&#39;super-sweep-14&#39; &#39;blooming-sweep-13&#39; &#39;vague-sweep-10&#39; &#39;sandy-sweep-9&#39; &#39;eternal-sweep-6&#39; &#39;resilient-sweep-5&#39; &#39;ethereal-sweep-2&#39; &#39;vibrant-sweep-1&#39;] . query = &#39;model==&quot;gcn&quot; and onlylocal==False and dropout==0&#39; print(f&quot;Plotting {runs.query(query)[&#39;name&#39;].nunique()} runs: {runs.query(query)[&#39;name&#39;].unique()}&quot;) sns.relplot(&#39;epoch&#39;, &#39;val_f1_score&#39;, col=&#39;weight_decay&#39;, hue=&#39;bidirectional&#39;, style=&#39;nobias&#39;, palette=sns.color_palette(&quot;Set1&quot;, 2), kind=&#39;line&#39;, data=runs.query(query)); . Plotting 8 runs: [&#39;super-sweep-14&#39; &#39;blooming-sweep-13&#39; &#39;vague-sweep-10&#39; &#39;sandy-sweep-9&#39; &#39;eternal-sweep-6&#39; &#39;resilient-sweep-5&#39; &#39;ethereal-sweep-2&#39; &#39;vibrant-sweep-1&#39;] . Address overfitting by adding dropout . Another way to address this is to add dropout to the model. In this case, I added dropout before the second GCN layer (meaning that the inputs to the 2nd layer will be dropped out with a certain probability. Adding dropout considerably increased the precision, meaning that the model predicts far fewer false negatives. The recall, on the other hand, remains largely unchanged. The best model was obtained with a dropout $p = 0.5$ . query = &#39;model==&quot;gcn&quot; and bidirectional==True and onlylocal==False and nobias==&quot;False&quot; and weight_decay==&quot;0.0005&quot; and (dropout==0. or dropout==0.25 or dropout==&quot;0.5&quot;)&#39; print(f&quot;Plotting {runs.query(query)[&#39;name&#39;].nunique()} runs: {runs.query(query)[&#39;name&#39;].unique()}&quot;) fig, ax = plt.subplots(1,2, figsize=(12,5)) g1 = sns.relplot(&#39;epoch&#39;, &#39;val_loss&#39;, col=&#39;bidirectional&#39;, hue=&#39;dropout&#39;, kind=&#39;line&#39;, palette=sns.color_palette(&quot;Set1&quot;, 3), data=runs.query(query), ax=ax[0]) g2 = sns.relplot(&#39;epoch&#39;, &#39;train_loss&#39;, col=&#39;bidirectional&#39;, hue=&#39;dropout&#39;, kind=&#39;line&#39;, palette=sns.color_palette(&quot;Set1&quot;, 3), data=runs.query(query), ax=ax[1]) plt.close(g1.fig) plt.close(g2.fig) plt.legend(title=&#39;dropout&#39;, labels=[&#39;0.0&#39;, &#39;0.25&#39;, &#39;0.5&#39;]) plt.setp(ax, ylim=(0,0.8)); . Plotting 3 runs: [&#39;faithful-sweep-3&#39; &#39;quiet-deluge-19&#39; &#39;sandy-sweep-9&#39;] . query = &#39;model==&quot;gcn&quot; and bidirectional==True and onlylocal==False and nobias==&quot;False&quot; and weight_decay==&quot;0.0005&quot; and (dropout==0. or dropout==0.25 or dropout==&quot;0.5&quot;)&#39; print(f&quot;Plotting {runs.query(query)[&#39;name&#39;].nunique()} runs: {runs.query(query)[&#39;name&#39;].unique()}&quot;) fig, ax = plt.subplots(1,3, figsize=(18,5)) g1 = sns.relplot(&#39;epoch&#39;, &#39;val_f1_score&#39;, col=&#39;bidirectional&#39;, hue=&#39;dropout&#39;, kind=&#39;line&#39;, palette=sns.color_palette(&quot;Set1&quot;, 3), data=runs.query(query), ax=ax[0]) g2 = sns.relplot(&#39;epoch&#39;, &#39;val_precision&#39;, col=&#39;bidirectional&#39;, hue=&#39;dropout&#39;, kind=&#39;line&#39;, palette=sns.color_palette(&quot;Set1&quot;, 3), data=runs.query(query), ax=ax[1]) g3 = sns.relplot(&#39;epoch&#39;, &#39;val_recall&#39;, col=&#39;bidirectional&#39;, hue=&#39;dropout&#39;, kind=&#39;line&#39;, palette=sns.color_palette(&quot;Set1&quot;, 3), data=runs.query(query), ax=ax[2]) plt.close(g1.fig) plt.close(g2.fig) plt.close(g3.fig) plt.legend(title=&#39;dropout&#39;, labels=[&#39;0.0&#39;, &#39;0.25&#39;, &#39;0.5&#39;]); . Plotting 3 runs: [&#39;faithful-sweep-3&#39; &#39;quiet-deluge-19&#39; &#39;sandy-sweep-9&#39;] . Training a GCN with local features only . A question that arises from the paper is: how much does the graph-based information contribute to the performance of a GCN model compared to a more traditional non-graph-based approach? Weber et al. show that the node embeddings that can be extracted from a GCN can help boost other traditional models. This makes sense intuitively: because of the networked nature of the bitcoin transactions, knowing the context or &quot;neighborhood&quot; of a transaction should add important information. . However, from the description of the Elliptic dataset we know that some of the features already contain information regarding the context of the transactions. In fact, 72 out of the 166 features are aggregated features. Therefore, I was curious to find out how would a GCN model perform with only the remaining 94 local features (including timestep) as inputs. . I modified the model to limit the set of features to only the local ones (including timestep). The input node features thus now have a shape of (94,). This way we can assess the performance of a GCN without having to manually engineer features from their neighbors. In other words, we leave the feature engineering to the neural network itself. . features_local = g_bi.ndata[&quot;features_matrix&quot;][:,0:94].float() print(f&quot;&quot;&quot;Number of features (all): {features.shape[1]}&quot;&quot;&quot;) print(f&quot;&quot;&quot;Number of features (only local): {features_local.shape[1]}&quot;&quot;&quot;) . Number of features (all): 166 Number of features (only local): 94 . #params = { # &quot;bidirectional&quot; : True, # &quot;n_hidden&quot; : 100, # &quot;n_layers&quot; : 2, # &quot;weight_decay&quot; : 5e-4, # &quot;bias&quot; : True, # &quot;dropout&quot; : 0.25, # &quot;epochs&quot; : 1000, # &quot;lr&quot; : 1e-3, # &quot;posweight&quot; : 0.7, #} # #model, metrics = train_eval_model(GCN, g_bi, features_local, **params) . query = &#39;model==&quot;gcn&quot; and bidirectional==True and weight_decay==0.0005 and nobias==&quot;False&quot; and (dropout==0 or dropout==0.25 or dropout==0.5)&#39; print(f&quot;Plotting {runs.query(query)[&#39;name&#39;].nunique()} runs: {runs.query(query)[&#39;name&#39;].unique()}&quot;) sns.relplot(&#39;epoch&#39;, &#39;val_f1_score&#39;, col=&#39;onlylocal&#39;, hue=&#39;dropout&#39;, palette=sns.color_palette(&quot;Set1&quot;, 3), kind=&#39;line&#39;, data=runs.query(query)); . Plotting 6 runs: [&#39;divine-sweep-4&#39; &#39;faithful-sweep-3&#39; &#39;zesty-microwave-76&#39; &#39;quiet-deluge-19&#39; &#39;divine-sweep-11&#39; &#39;sandy-sweep-9&#39;] . We can see that the perfomance of the GCN improved by addition of dropout when all features were considered, but not when using only local features. This makes sense, as the aggregated features are likely less essential than the local ones. . Random Forest benchmark . A sobering additional finding from the paper by Weber et al. was the out-of-the-box excellent performance of a simple Random Forest in correctly classifying the transactions as licit or illicit. I was able to replicate these results too for two different sets of features: . All features | Only local features | . from sklearn.ensemble import RandomForestClassifier # function to evaluate the model def evaluate_rfc(model, features, labels, mask): &quot;&quot;&quot;Calculate the loss, accuracy, precision, recall and f1_score for the masked data&quot;&quot;&quot; pred_rfc = model.predict(features[mask]) labels = labels[mask] p, r, f, _ = precision_recall_fscore_support(labels, pred_rfc) return p[1], r[1], f[1] # confusion matrix def eval_confusion_matrix_rfc(model, features, labels, mask): pred_rfc = model.predict(features[mask]) labels = labels[mask] print(confusion_matrix(labels, pred_rfc)) . Using all features (local + aggregated) . rfc = RandomForestClassifier(n_estimators=50, max_features=50, random_state=seed) rfc.fit(features[train_indices], labels[train_indices]) p, r, f1 = evaluate_rfc(rfc, features, labels, val_indices) print( f&quot;Precision {p:.4f} | Recall {r:.4f} | &quot; f&quot;F1 score {f1:.4f}&quot; ) print(&quot;Confusion matrix:&quot;) eval_confusion_matrix_rfc(rfc, features, labels, val_indices) . Precision 0.9167 | Recall 0.7211 | F1 score 0.8072 Confusion matrix: [[15516 71] [ 302 781]] . The best results were obtained using as input all available features (local (including timestep) + aggregated). Both precision and recall of this model were high. This is confirmed by looking at the confusion matrix. The model predicts nearly no false positives and less than 30% of the illicit transactions are falsely labeled as negatives. It is a very good performance for such a simple model. . Using local features only . rfc_local = RandomForestClassifier(n_estimators=50, max_features=50, random_state=seed) rfc_local.fit(features_local[train_indices], labels[train_indices]) p, r, f1 = evaluate_rfc(rfc_local, features_local, labels, val_indices) print( f&quot;Precision {p:.4f} | Recall {r:.4f} | &quot; f&quot;F1 score {f1:.4f}&quot; ) eval_confusion_matrix_rfc(rfc_local, features_local, labels, val_indices) . Precision 0.8749 | Recall 0.7036 | F1 score 0.7799 [[15478 109] [ 321 762]] . Removing the aggregated features from the input to the Random Forest (leaving other parameters equal) impairs its performance both in precision and recall, but not dramatically. It still is a very good model working out of the box. It makes sense that not having information about the immediate neighbors of a transaction would produce a worse-performing model. . Comparison to GCN . So what do the resuls of the Random Forest tell us about GCNs and other deep learning techniques? Should we dismiss them and focus on simpler models instead? While this analysis shows that it pays to start simple and see how well we can tackle a task using classic machine learning methods first, there still are valid reasons why one should consider (graph) neural networks too. . Deep learning on graphs is cool! | Now, seriously, . There is information contained in the connections between data points, which is not being considered by a classical machine learning approach, unless carefully crafted features are available, which requires time and specific knowledge | The features available to a different dataset may be less informative than those in the Elliptic dataset, and therefore insufficient to produce a good-enough Random Forest model | There may even be situations when no intrinsic node features are available and we still want to be able to classify transactions. This would still be possible using a GCN but not with a Random Forest | Progress in unleashing the potential of GCNs can only be obtained by researching these networks | Now let&#39;s take a look at a different kind of graph-based model that, I figured, might be a good option for the bitcoin transaction classification task. . Long-range propagation of label predictions using APPNP . Let&#39;s recapitulate. . We trained a complex GCN model to classify bitcoin transactions as licit or illicit and improved its performance by better parameters and training | We found that a Random Forest model performed better than our complex GCN, almost effortlessly | . Why is this? In order to look for the answer it pays to take a closer look at how the transaction graph is structured. Let&#39;s take for example the transactions of the last timestep. . with sns.axes_style(&#39;white&#39;): plt.figure(figsize=(8,6)) node_label = list(nx.get_node_attributes(g_nx_t_list[49-1], &#39;label&#39;).values()) mapping = {-1:&#39;grey&#39;, 0:&#39;C0&#39;, 1:&#39;C3&#39;} node_color = [mapping[l] for l in node_label] nx.draw_networkx(g_nx_t_list[49-1], node_size=10, node_color=node_color, with_labels=False, width=0.2, alpha=0.8, arrowsize=8) leg = plt.legend([&#39;unlabeled&#39;, &#39;licit&#39;, &#39;illicit&#39;]) leg.legendHandles[0].set_color(&#39;grey&#39;) leg.legendHandles[1].set_color(&#39;C0&#39;) leg.legendHandles[2].set_color(&#39;C3&#39;) plt.show() . /anaconda3/envs/dgl/lib/python3.7/site-packages/networkx/drawing/nx_pylab.py:579: MatplotlibDeprecationWarning: The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. if not cb.iterable(width): /anaconda3/envs/dgl/lib/python3.7/site-packages/networkx/drawing/nx_pylab.py:676: MatplotlibDeprecationWarning: The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. if cb.iterable(node_size): # many node sizes . The original paper describing the GCN applied it to the node classification of papers in the Cora dataset. The bitcoin transaction graph is much larger and less dense than the Cora graph. One can hipothesize that for a denser graph with a higher average degree, each node could receive more information from its neighbors that would help make a GCN make a better prediction. . Graph Nodes Density Average degree . Elliptic (bitcoin transactions) | 203769 | 0.0003177 | 2.30 | . Cora (citations) | 2708 | 0.0014812 | 3.90 | . Furthermore, if we consider a simple chain of transactions (like the ones seen in the graph visualization), a node would receive information only from the previous node and pass it on only to the following node in the chain. In such a situation, it could be that the range of neighbors that feed the classification of any given node is too short and often not sufficient for a correct prediction. If this is the case, then considering a longer-ranging neighborhood could help train a better classification model. . Approximated Personalized Propagation of Neural Predictions (APPNP) . Enter APPNP. This model was recently proposed as a way to reconcile the best of two worlds: neural message passing algorithms (in principle like the GCN), and personalized pagerank. . In PageRank, a measure of how central or important a node is calculated as a function of its connections and the importance of its neighbors. The pagerank $PR$ of a node $u$ is: . $$PR(u) = (1-d){1 over {N}} + d sum_{v in mathcal{N} (u)} {PR(v) over D_{out}(v)}$$ . where $N$ is the total number of nodes, $D_{out}$ is the outdegree, and $v$ is a node of the set of neighbors $ mathcal{N}$ of $u$. Pagerank ultimately requires each node to iteratively receive information from its neighbors until the pagerank stops changing and converges. The algorithm assigns a damping factor $d$, which can be understood by imagining a random surfer visiting nodes in the graph. The surfer would visit any given node moving along the edges of the graph until it reaches it with probability $d$. However, that surfer could also visit a node by randomly teleporting to it from elsewhere in the graph with probability $(1-d)$. . In APPNP, a multilayer perceptron takes the node features as input and outputs prediction probabilities $H^{0} = f_{MLP}(X)$. These are then propagated through graph for a $K$ number of iterations. . $$H^{t+1} = (1- alpha) left( hat{D}^{-1/2} hat{A} hat{D}^{-1/2} H^{t} right) + alpha H^{0}$$ . If you think the APPNP and the pagerank equations look similar is because they are. The teleportation probability $ alpha$ corresponds to the damping factor in pagerank. It tells us that for each iteration, the predictions for a node will depend on the normalized graph laplacian with probability $(1- alpha)$ or on the output of the MLP with probability $ alpha$. . So how does do we train the APPNP model? I modified an implementation of an APPNP layer from DGL. In the original paper by Klicpera et al. (2019) they use an architecture consisting of a 2-layer MLP with 64 hidden units each, followed by the propagation component given by the APPNP equation. In order to use a comparable architecture to the GCN model, I decided to set the number of hidden units to 100 (same as previous section). . I tested several combinations of values for $K$ and $ alpha$. However, a more exhaustive hyperparameter search is needed to find the best possible configuration. Moreover, I used two versions of the feature set: all features vs. only local features. . The best F1 score was obtained for $K = 20$ propagation iterations and a teleportation probability $ alpha = 0.2$. Interestingly, the model trained with only local features performed better than using all features. Furthermore, the addition of dropout to the network had a positive effect when using all features, but a negative effect if only local features were considered. . class APPNP(nn.Module): def __init__( self, g, in_feats, n_hidden, n_classes, n_layers, activation, feat_drop, edge_drop, alpha, k, ): super(APPNP, self).__init__() self.g = g self.layers = nn.ModuleList() # input layer self.layers.append(nn.Linear(in_feats, n_hidden)) # hidden layers for _ in range(n_layers - 2): self.layers.append(nn.Linear(n_hidden, n_hidden)) # output layer self.layers.append(nn.Linear(n_hidden, n_classes)) self.activation = activation if feat_drop: self.feat_drop = nn.Dropout(feat_drop) else: self.feat_drop = lambda x: x self.propagate = APPNPConv(k, alpha, edge_drop) self.reset_parameters() def reset_parameters(self): for layer in self.layers: layer.reset_parameters() def forward(self, features): # prediction step h = features h = self.feat_drop(h) h = self.activation(self.layers[0](h)) for layer in self.layers[1:-1]: h = self.activation(layer(h)) h = self.layers[-1](self.feat_drop(h)) # propagation step h = self.propagate(self.g, h) return h . query = &#39;model==&quot;appnp&quot; and nhidden==100 and bidirectional==True and weight_decay==0.0005 and nobias==&quot;False&quot; and (dropout==0 or dropout==0.2 or dropout==0.25)&#39; print(f&quot;Plotting {runs.query(query)[&#39;name&#39;].nunique()} runs: {runs.query(query)[&#39;name&#39;].unique()}&quot;) sns.relplot(&#39;epoch&#39;, &#39;val_f1_score&#39;, row=&#39;onlylocal&#39;, col=&#39;alpha&#39;, hue=&#39;k&#39;, style=&#39;dropout&#39;, palette=sns.color_palette(&quot;Set1&quot;, 2), kind=&#39;line&#39;, data=runs.query(query)); . Plotting 12 runs: [&#39;tough-sweep-4&#39; &#39;fluent-sweep-3&#39; &#39;happy-sweep-2&#39; &#39;eager-sweep-1&#39; &#39;vivid-sweep-2&#39; &#39;copper-sweep-1&#39; &#39;classic-sweep-10&#39; &#39;confused-sweep-9&#39; &#39;generous-sweep-6&#39; &#39;dulcet-sweep-5&#39; &#39;avid-sweep-2&#39; &#39;giddy-sweep-1&#39;] . MLP benchmark . But just how much of an effect does the incorporation of the graph structure have on the performance of the model? How much of it is simply due to the MLP that is trained on top of the propagation phase of the APPNP? In order to assess this, I trained an MLP model with the same architecture and for the two different versions of the features set. . The results show that the APPNP model has a better F1 score than the MLP after 1000 epochs, both for the only local feature set and the full feature set. With respect to precision and F1 score, the addition of dropout was beneficial when using all features and prejudicial when using the local features only. In contrast, recall was slightly improved by adding dropout to both feature variants. . class MLP(nn.Module): def __init__( self, in_feats, n_hidden, n_classes, n_layers, activation, feat_drop, ): super(MLP, self).__init__() self.layers = nn.ModuleList() # input layer self.layers.append(nn.Linear(in_feats, n_hidden)) # hidden layers for _ in range(n_layers - 2): self.layers.append(nn.Linear(n_hidden, n_hidden)) # output layer self.layers.append(nn.Linear(n_hidden, n_classes)) self.activation = activation if feat_drop: self.feat_drop = nn.Dropout(feat_drop) else: self.feat_drop = lambda x: x self.reset_parameters() def reset_parameters(self): for layer in self.layers: layer.reset_parameters() def forward(self, features): # prediction step h = features h = self.feat_drop(h) h = self.activation(self.layers[0](h)) for layer in self.layers[1:-1]: h = self.activation(layer(h)) h = self.layers[-1](self.feat_drop(h)) return h . Plotting 8 runs: [&#39;tough-sweep-4&#39; &#39;fluent-sweep-3&#39; &#39;happy-sweep-2&#39; &#39;crisp-sweep-4&#39; &#39;lucky-sweep-3&#39; &#39;summer-sweep-2&#39; &#39;eager-sweep-1&#39; &#39;sandy-sweep-1&#39;] . Plotting 8 runs: [&#39;tough-sweep-4&#39; &#39;fluent-sweep-3&#39; &#39;happy-sweep-2&#39; &#39;crisp-sweep-4&#39; &#39;lucky-sweep-3&#39; &#39;summer-sweep-2&#39; &#39;eager-sweep-1&#39; &#39;sandy-sweep-1&#39;] . Plotting 8 runs: [&#39;tough-sweep-4&#39; &#39;fluent-sweep-3&#39; &#39;happy-sweep-2&#39; &#39;crisp-sweep-4&#39; &#39;lucky-sweep-3&#39; &#39;summer-sweep-2&#39; &#39;eager-sweep-1&#39; &#39;sandy-sweep-1&#39;] . Putting it all together . Please refer to my tl;dr :) .",
            "url": "https://www.arcosdiaz.com/blog/graph%20neural%20networks/fraud%20detection/2019/12/15/btc-fraud-detection.html",
            "relUrl": "/graph%20neural%20networks/fraud%20detection/2019/12/15/btc-fraud-detection.html",
            "date": " • Dec 15, 2019"
        }
        
    
  
    
        ,"post1": {
            "title": "Fitbit activity and sleep data: a time-series analysis with Generalized Additive Models",
            "content": "The goal of this notebook is to provide an analysis of the time-series data from a user of a fitbit tracker throughout a year. I will use this data to predict an additional year of the life of the user using Generalized Additive Models. . Data source: Activity, Sleep . Packages used: . pandas, numpy, matplotlib, seaborn | Prophet | . import pandas as pd import numpy as np from fbprophet import Prophet import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline . Data cleaning (missing data and outliers) . activity = pd.read_csv(&#39;OneYearFitBitData.csv&#39;) # change commas to dots activity.iloc[:,1:] = activity.iloc[:,1:].applymap(lambda x: float(str(x).replace(&#39;,&#39;,&#39;.&#39;))) # change column names to English activity.columns = [&#39;Date&#39;, &#39;BurnedCalories&#39;, &#39;Steps&#39;, &#39;Distance&#39;, &#39;Floors&#39;, &#39;SedentaryMinutes&#39;, &#39;LightMinutes&#39;, &#39;ModerateMinutes&#39;, &#39;IntenseMinutes&#39;, &#39;IntenseActivityCalories&#39;] # import the sleep data sleep = pd.read_csv(&#39;OneYearFitBitDataSleep.csv&#39;) # check the size of the dataframes activity.shape, sleep.shape # merge dataframes data = pd.merge(activity, sleep, how=&#39;outer&#39;, on=&#39;Date&#39;) # parse date into correct format data[&#39;Date&#39;] = pd.to_datetime(data[&#39;Date&#39;], format=&#39;%d-%m-%Y&#39;) # correct units for Calories and Steps for c in [&#39;BurnedCalories&#39;, &#39;Steps&#39;, &#39;IntenseActivityCalories&#39;]: data[c] = data[c]*1000 . Once imported, we should check for any missing data: . data.isnull().sum() . Date 0 BurnedCalories 0 Steps 0 Distance 0 Floors 0 SedentaryMinutes 0 LightMinutes 0 ModerateMinutes 0 IntenseMinutes 0 IntenseActivityCalories 0 MinutesOfSleep 5 MinutesOfBeingAwake 5 NumberOfAwakings 5 LengthOfRestInMinutes 5 dtype: int64 . data.iloc[np.where(data[&#39;MinutesOfSleep&#39;].isnull())[0],:] . Date BurnedCalories Steps Distance Floors SedentaryMinutes LightMinutes ModerateMinutes IntenseMinutes IntenseActivityCalories MinutesOfSleep MinutesOfBeingAwake NumberOfAwakings LengthOfRestInMinutes . 0 2015-05-08 | 1934.0 | 905000.0 | 0.65 | 0.0 | 1.355 | 46.0 | 0.0 | 0.0 | 168000.0 | NaN | NaN | NaN | NaN | . 275 2016-02-01 | 2986.0 | 11426.0 | 8.52 | 12.0 | 911.000 | 192.0 | 48.0 | 43.0 | 1478.0 | NaN | NaN | NaN | NaN | . 276 2016-02-02 | 2974.0 | 10466.0 | 7.78 | 13.0 | 802.000 | 152.0 | 48.0 | 48.0 | 1333.0 | NaN | NaN | NaN | NaN | . 277 2016-02-03 | 3199.0 | 12866.0 | 9.63 | 11.0 | 767.000 | 271.0 | 45.0 | 28.0 | 1703.0 | NaN | NaN | NaN | NaN | . 278 2016-02-04 | 2037.0 | 2449.0 | 1.87 | 0.0 | 821.000 | 80.0 | 0.0 | 0.0 | 337000.0 | NaN | NaN | NaN | NaN | . We can see that the sleep information was missing for some dates. The activity information for those days is complete. Therefore, we should not get rid of those rows just now. . data.iloc[np.where(data[&#39;Steps&#39;]==0)[0],:] . Date BurnedCalories Steps Distance Floors SedentaryMinutes LightMinutes ModerateMinutes IntenseMinutes IntenseActivityCalories MinutesOfSleep MinutesOfBeingAwake NumberOfAwakings LengthOfRestInMinutes . 235 2015-12-23 | 1789.0 | 0.0 | 0.0 | 0.0 | 1.44 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 317 2016-03-13 | 1790.0 | 0.0 | 0.0 | 0.0 | 1.44 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 332 2016-03-28 | 1790.0 | 0.0 | 0.0 | 0.0 | 1.44 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . We can also see that the step count for some datapoints is zero. If we look at the complete rows, we can see that on those days nearly no other data was recorded. I assume that the user probably did not wear the fitness tracker on that day and we could get rid of those complete rows. . data = data.drop(np.where(data[&#39;Steps&#39;]==0)[0], axis=0) . sns.distplot(data[&#39;Steps&#39;]) plt.title(&#39;Histogram for step count&#39;) . &lt;matplotlib.text.Text at 0x10fb34cc0&gt; . Step count is probably the most accurate measure obtained from a pedometer. Looking at the distribution of this variable, however, we can see that there is a chance that we have outliers in the data, as at least one value seems to be much higher than all the rest. . data.sort_values(by=&#39;Steps&#39;, ascending=False).head() . Date BurnedCalories Steps Distance Floors SedentaryMinutes LightMinutes ModerateMinutes IntenseMinutes IntenseActivityCalories MinutesOfSleep MinutesOfBeingAwake NumberOfAwakings LengthOfRestInMinutes . 0 2015-05-08 | 1934.0 | 905000.0 | 0.65 | 0.0 | 1.355 | 46.0 | 0.0 | 0.0 | 168000.0 | NaN | NaN | NaN | NaN | . 267 2016-01-24 | 1801.0 | 39000.0 | 0.03 | 0.0 | 1.076 | 5.0 | 0.0 | 0.0 | 16000.0 | 342.0 | 48.0 | 31.0 | 390.0 | . 42 2015-06-13 | 4083.0 | 26444.0 | 19.65 | 22.0 | 549.000 | 429.0 | 56.0 | 56.0 | 2818.0 | 169.0 | 20.0 | 11.0 | 196.0 | . 363 2016-04-28 | 4030.0 | 25571.0 | 19.30 | 15.0 | 606.000 | 293.0 | 42.0 | 129.0 | 2711.0 | 374.0 | 56.0 | 34.0 | 430.0 | . 320 2016-03-16 | 3960.0 | 25385.0 | 20.45 | 17.0 | 638.000 | 254.0 | 17.0 | 124.0 | 2556.0 | 368.0 | 46.0 | 22.0 | 414.0 | . We found the outlier! It seems that the step count for the first day (our data starts on May 8th, 2015) is too high to be a correct value for the amount of steps taken by the user on that day. Maybe the device saves the vibration since its production as step count which is loaded on the first day that the user wears the tracker. We can anyway get rid of that row since the sleep data is also not available for this day. . data = data.drop(np.where(data[&#39;Steps&#39;]&gt;=100000)[0], axis=0) . Now we can look at our preprocessed data. Shape, distribution of the variables, and a look at some rows from the dataframe, are all useful things to observe: . data.shape . (369, 14) . fig, ax = plt.subplots(5,3, figsize=(8,10)) for c, a in zip(data.columns[1:], ax.flat): df = pd.DataFrame() df[&#39;ds&#39;] = data[&#39;Date&#39;] df[&#39;y&#39;] = data[c] df = df.dropna(axis=0, how=&#39;any&#39;) sns.distplot(df[&#39;y&#39;], axlabel=False, ax=a) a.set_title(c) plt.suptitle(&#39;Histograms of variables from fitbit data&#39;, y=1.02, fontsize=14); plt.tight_layout() . data.head() . Date BurnedCalories Steps Distance Floors SedentaryMinutes LightMinutes ModerateMinutes IntenseMinutes IntenseActivityCalories MinutesOfSleep MinutesOfBeingAwake NumberOfAwakings LengthOfRestInMinutes . 1 2015-05-09 | 3631.0 | 18925.0 | 14.11 | 4.0 | 611.0 | 316.0 | 61.0 | 60.0 | 2248.0 | 384.0 | 26.0 | 23.0 | 417.0 | . 2 2015-05-10 | 3204.0 | 14228.0 | 10.57 | 1.0 | 602.0 | 226.0 | 14.0 | 77.0 | 1719.0 | 454.0 | 35.0 | 21.0 | 491.0 | . 3 2015-05-11 | 2673.0 | 6756.0 | 5.02 | 8.0 | 749.0 | 190.0 | 23.0 | 4.0 | 962000.0 | 387.0 | 46.0 | 25.0 | 436.0 | . 4 2015-05-12 | 2495.0 | 5020.0 | 3.73 | 1.0 | 876.0 | 171.0 | 0.0 | 0.0 | 736000.0 | 311.0 | 31.0 | 21.0 | 350.0 | . 5 2015-05-13 | 2760.0 | 7790.0 | 5.79 | 15.0 | 726.0 | 172.0 | 34.0 | 18.0 | 1094.0 | 407.0 | 65.0 | 44.0 | 491.0 | . Predicting the step count for an additional year . In order to use the Prophet package to predict the future using a Generalized Additive Model, we need to create a dataframe with columns ds and y (we need to do this for each variable): . ds is the date stamp data giving the time component | y is the variable that we want to predict | . In our case we will use the log transform of the step count in order to decrease the effect of outliers on the model. . df = pd.DataFrame() df[&#39;ds&#39;] = data[&#39;Date&#39;] df[&#39;y&#39;] = data[&#39;Steps&#39;] # log-transform of step count df[&#39;y&#39;] = np.log(df[&#39;y&#39;]) . Now we need to specify the type of growth model that we want to use: . Linear: assumes that the variable y grows linearly in time (doesn&#39;t apply to our step count scenario, if the person sticks to their normal lifestyle) | Logistic: assumes that the variable y grows logistically in time and saturates at some point | . I will assume that the person, for whom we want to predict the step count in the following year, will not have any dramatic lifestyle changes that makes them start to walk more. Therefore, I am using logistic &#39;growth&#39; capped to a cap of the mean of the data, which in practice means that the step count&#39;s growth trend will be &#39;zero growth&#39;. . df[&#39;cap&#39;] = df[&#39;y&#39;].median() m = Prophet(growth=&#39;logistic&#39;, yearly_seasonality=True) m.fit(df) . INFO:fbprophet.forecaster:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. /Users/dario/anaconda/envs/datasci/lib/python3.6/site-packages/pystan/misc.py:399: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. elif np.issubdtype(np.asarray(v).dtype, float): . &lt;fbprophet.forecaster.Prophet at 0x115274dd8&gt; . After fitting the model, we need a new dataframe future with the additional rows for which we want to predict y. . future = m.make_future_dataframe(periods=365, freq=&#39;D&#39;) future[&#39;cap&#39;] = df[&#39;y&#39;].median() . Now we can call predict on the fitted model and obtain relevant statistics for the forecast period. We can also plot the results. . forecast = m.predict(future) forecast[[&#39;ds&#39;, &#39;yhat&#39;, &#39;yhat_lower&#39;, &#39;yhat_upper&#39;]].tail() . ds yhat yhat_lower yhat_upper . 729 2017-05-03 | 9.565727 | 9.109546 | 10.034228 | . 730 2017-05-04 | 9.547910 | 9.089434 | 10.038742 | . 731 2017-05-05 | 9.536875 | 9.084160 | 10.035884 | . 732 2017-05-06 | 9.525034 | 9.068735 | 10.008784 | . 733 2017-05-07 | 9.289238 | 8.792235 | 9.746812 | . m.plot(forecast, ylabel=&#39;log(Steps)&#39;, xlabel=&#39;Date&#39;); plt.title(&#39;1-year prediction of step count from 1 year of fitbit data&#39;); . We can see that the model did a good job in mimicking the behavior of step count during the year for which the data was available. This seems reasonable, as we do not expect the pattern to vary necessarily, if the person continues to have a similar lifestyle. . Additionally, we can plot the components from the Generalized Additive Model and see their effect on the &#39;y&#39; variable. In this case we have the general trend (remember we capped this at &#39;10&#39;), the yearly seasonality effect, and the weekly effect. . m.plot_components(forecast); plt.suptitle(&#39;GAM components for prediction of step count&#39;, y=1.02, fontsize=14); . Here we see some interesting patterns: . The general &#39;growth&#39; trend is as expected, as we assumed that there would be no growth beyond the mean of the existing data. | The yearly effect shows a trend towards higher activity during the summer months, however the variation is considerable, probably due to the fact that our dataset consisted of the data for one year only | The weekly effect shows that Sunday is a day of lower activity for this person whereas Saturday is the day where the activity is the highest. So, grocery shopping on Saturday, Netflix on Sunday? :) | . Sleep analysis . A very important part of our lives is sleep. It would be very interesting to look at the sleep habits of the user of the fitness tracker and see if we can get some insights from this data. . df = pd.DataFrame() df[&#39;ds&#39;] = data[&#39;Date&#39;] df[&#39;y&#39;] = data[&#39;MinutesOfSleep&#39;] df = df.dropna(axis=0, how=&#39;any&#39;) # drop rows where sleep time is zero, as this would mean that the person did not wear the tracker overnight and the data is missing df = df.iloc[np.where(df[&#39;y&#39;]!=0)[0],:] . sns.distplot(df[&#39;y&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1163770f0&gt; . df[&#39;cap&#39;] = df[&#39;y&#39;].median() m = Prophet(growth=&#39;logistic&#39;, yearly_seasonality=True) m.fit(df) future = m.make_future_dataframe(periods=365, freq=&#39;D&#39;) future[&#39;cap&#39;] = df[&#39;y&#39;].median() forecast = m.predict(future) m.plot(forecast); plt.title(&#39;1-year prediction of MinutesOfSleep from 1 year of fitbit data&#39;); . INFO:fbprophet.forecaster:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. /Users/dario/anaconda/envs/datasci/lib/python3.6/site-packages/pystan/misc.py:399: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. elif np.issubdtype(np.asarray(v).dtype, float): . The model again seems to predict a similar sleep behavior for the predicted year. This seems reasonable, as we do not expect the pattern to vary necessarily, if the person continues to have a similar lifestyle. . m.plot_components(forecast); plt.suptitle(&#39;GAM components for prediction of MinutesOfSleep&#39;, y=1.02, fontsize=14); . A look at the amount of sleep reveals: . A saturation trend at the median (we set this assumption) | A yearly effect shows a trend towards higher amount of sleep during the summer months, with more variation during winter | The weekly effect shows lowest sleep amount on Mondays (maybe going to bed late on Sunday and waking up early on Monday is a pattern for this user). Highest amout of sleep occurs on Saturdays (no alarm to wake up to on Saturday morning!). Interestingly, the user seems to get more sleep on Wednesdays than on Mondays or Tuesdays, which could mean that their work schedule is not constant during week-days. | . Appendix . As an exercise, I have plotted the predictions for the most interesing variables in the dataset. Enjoy! . zeros_allowed = [&#39;Floors&#39;, &#39;SedentaryMinutes&#39;, &#39;LightMinutes&#39;, &#39;ModerateMinutes&#39;, &#39;IntenseMinutes&#39;, &#39;IntenseActivityCalories&#39;, &#39;MinutesOfBeingAwake&#39;, &#39;NumberOfAwakings&#39;] fig, ax = plt.subplots(3,3, figsize=(12,6), sharex=True) predict_cols = [&#39;Steps&#39;, &#39;Floors&#39;, &#39;BurnedCalories&#39;, &#39;LightMinutes&#39;, &#39;ModerateMinutes&#39;, &#39;IntenseMinutes&#39;, &#39;MinutesOfSleep&#39;, &#39;MinutesOfBeingAwake&#39;, &#39;NumberOfAwakings&#39;] for c, a in zip(predict_cols, ax.flat): df = pd.DataFrame() df[&#39;ds&#39;] = data[&#39;Date&#39;] df[&#39;y&#39;] = data[c] df = df.dropna(axis=0, how=&#39;any&#39;) if c not in zeros_allowed: df = df.iloc[np.where(df[&#39;y&#39;]!=0)[0],:] df[&#39;cap&#39;] = df[&#39;y&#39;].median() m = Prophet(growth=&#39;logistic&#39;, yearly_seasonality=True) m.fit(df) future = m.make_future_dataframe(periods=365, freq=&#39;D&#39;) future[&#39;cap&#39;] = df[&#39;y&#39;].median() future.tail() forecast = m.predict(future) forecast[[&#39;ds&#39;, &#39;yhat&#39;, &#39;yhat_lower&#39;, &#39;yhat_upper&#39;]].tail() m.plot(forecast, xlabel=&#39;&#39;, ax=a); a.set_title(c) #m.plot_components(forecast); plt.suptitle(&#39;1-year prediction per variable from 1 year of fitbit data&#39;, y=1.02, fontsize=14); plt.tight_layout() . INFO:fbprophet.forecaster:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. /Users/dario/anaconda/envs/datasci/lib/python3.6/site-packages/pystan/misc.py:399: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. elif np.issubdtype(np.asarray(v).dtype, float): INFO:fbprophet.forecaster:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. INFO:fbprophet.forecaster:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. INFO:fbprophet.forecaster:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. INFO:fbprophet.forecaster:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. INFO:fbprophet.forecaster:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. INFO:fbprophet.forecaster:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. INFO:fbprophet.forecaster:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. INFO:fbprophet.forecaster:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. .",
            "url": "https://www.arcosdiaz.com/blog/time%20series/wearables/forecasting/2018/04/01/fitbit_prophet.html",
            "relUrl": "/time%20series/wearables/forecasting/2018/04/01/fitbit_prophet.html",
            "date": " • Apr 1, 2018"
        }
        
    
  
    
        ,"post2": {
            "title": "Personalized Medicine Kaggle Competition",
            "content": "This notebook describes my approach to the Kaggle competition named in the title. This was a research competition at Kaggle in cooperation with the Memorial Sloan Kettering Cancer Center (MSKCC). . The goal of the competition was to create a machine learning algorithm that can classify genetic variations that are present in cancer cells. . Tumors contain cells with many different abnormal mutations in their DNA: some of these mutations are the drivers of tumor growth, whereas others are neutral and considered passengers. Normally, mutations are manually classified into different categories after literature review by clinicians. The dataset made available for this competition contains mutations that have been manually anotated into 9 different categories. The goal is to predict the correct category of mutations in the test set. . The model and submission described here got me to the 140th place (out of 1386 teams) or top 11%. . Data . The data comes in two different kinds of files: one of them contains information about the genetic variants (training_variants and stage2_test_variants.csv) and the other contains the text (clinical evidence) that was used to manually classify the variants (training_text and stage2_test_text.csv). The training data contains a class target feature corresponding to one of the 9 categories that variants can be classified as. . Note: the &quot;stage2&quot; prefix of the test files is due to the nature of the competition. There was an initial test set that was used at the beginning of the competition and a &quot;stage2&quot; test set that was used in the final week before the deadline to make the submissions. . import os import re import string import pandas as pd import numpy as np . train_variant = pd.read_csv(&quot;input/training_variants&quot;) test_variant = pd.read_csv(&quot;input/stage2_test_variants.csv&quot;) train_text = pd.read_csv(&quot;input/training_text&quot;, sep=&quot; | |&quot;, engine=&#39;python&#39;, header=None, skiprows=1, names=[&quot;ID&quot;,&quot;Text&quot;]) test_text = pd.read_csv(&quot;input/stage2_test_text.csv&quot;, header=None, skiprows=1, names=[&quot;ID&quot;, &quot;Text&quot;]) train = pd.merge(train_variant, train_text, how=&#39;left&#39;, on=&#39;ID&#39;) train_y = train[&#39;Class&#39;].values train_x = train.drop(&#39;Class&#39;, axis=1) train_size=len(train_x) print(&#39;Number of training variants: %d&#39; % (train_size)) # number of train data : 3321 test_x = pd.merge(test_variant, test_text, how=&#39;left&#39;, on=&#39;ID&#39;) test_size=len(test_x) print(&#39;Number of test variants: %d&#39; % (test_size)) # number of test data : 5668 test_index = test_x[&#39;ID&#39;].values all_data = np.concatenate((train_x, test_x), axis=0) all_data = pd.DataFrame(all_data) all_data.columns = [&quot;ID&quot;, &quot;Gene&quot;, &quot;Variation&quot;, &quot;Text&quot;] . Number of training variants: 3321 Number of test variants: 986 . all_data.head() . ID Gene Variation Text . 0 0 | FAM58A | Truncating Mutations | Cyclin-dependent kinases (CDKs) regulate a var... | . 1 1 | CBL | W802* | Abstract Background Non-small cell lung canc... | . 2 2 | CBL | Q249E | Abstract Background Non-small cell lung canc... | . 3 3 | CBL | N454D | Recent evidence has demonstrated that acquired... | . 4 4 | CBL | L399V | Oncogenic mutations in the monomeric Casitas B... | . The data from the different train and test files is now consolidated into one single file. This is necessary for the correct vectorization of the text data and categorical data later on. We can see that the text information resembles scientific article text. We will process this consolidated file in the next step. . Preprocessing . In order to be able to use this data to train a machine learning model, we need to extract the features from the dataset. This means that we have to transform the text data into vectors that can be understood by an algorithm. As I am not an expert in Natural Language Processing, I applied a modified version of this script published on Kaggle. Afterwards we will have the data in a form that I can use to train a neural network. . from nltk.corpus import stopwords from gensim.models.doc2vec import LabeledSentence from gensim import utils def constructLabeledSentences(data): sentences=[] for index, row in data.iteritems(): sentences.append(LabeledSentence(utils.to_unicode(row).split(), [&#39;Text&#39; + &#39;_%s&#39; % str(index)])) return sentences def textClean(text): text = re.sub(r&quot;[^A-Za-z0-9^,!. /&#39;+-=]&quot;, &quot; &quot;, str(text)) text = text.lower().split() stops = set(stopwords.words(&quot;english&quot;)) text = [w for w in text if not w in stops] text = &quot; &quot;.join(text) return(text) def cleanup(text): text = textClean(text) text= text.translate(str.maketrans(&quot;&quot;,&quot;&quot;, string.punctuation)) return text allText = all_data[&#39;Text&#39;].apply(cleanup) sentences = constructLabeledSentences(allText) allText.head() . Using TensorFlow backend. . 0 cyclindependent kinases cdks regulate variety ... 1 abstract background nonsmall cell lung cancer ... 2 abstract background nonsmall cell lung cancer ... 3 recent evidence demonstrated acquired uniparen... 4 oncogenic mutations monomeric casitas blineage... Name: Text, dtype: object . # PROCESS TEXT DATA from gensim.models import Doc2Vec Text_INPUT_DIM=300 text_model=None filename=&#39;docEmbeddings_5_clean.d2v&#39; if os.path.isfile(filename): text_model = Doc2Vec.load(filename) else: text_model = Doc2Vec(min_count=1, window=5, size=Text_INPUT_DIM, sample=1e-4, negative=5, workers=4, iter=5,seed=1) text_model.build_vocab(sentences) text_model.train(sentences, total_examples=text_model.corpus_count, epochs=text_model.iter) text_model.save(filename) text_train_arrays = np.zeros((train_size, Text_INPUT_DIM)) text_test_arrays = np.zeros((test_size, Text_INPUT_DIM)) for i in range(train_size): text_train_arrays[i] = text_model.docvecs[&#39;Text_&#39;+str(i)] j=0 for i in range(train_size,train_size+test_size): text_test_arrays[j] = text_model.docvecs[&#39;Text_&#39;+str(i)] j=j+1 print(text_train_arrays[0][:10]) # PROCESS GENE DATA from sklearn.decomposition import TruncatedSVD Gene_INPUT_DIM=25 svd = TruncatedSVD(n_components=25, n_iter=Gene_INPUT_DIM, random_state=12) one_hot_gene = pd.get_dummies(all_data[&#39;Gene&#39;]) truncated_one_hot_gene = svd.fit_transform(one_hot_gene.values) one_hot_variation = pd.get_dummies(all_data[&#39;Variation&#39;]) truncated_one_hot_variation = svd.fit_transform(one_hot_variation.values) # ENCODE THE LABELS FROM INTEGERS TO VECTORS from keras.utils import np_utils from sklearn.preprocessing import LabelEncoder label_encoder = LabelEncoder() label_encoder.fit(train_y) encoded_y = np_utils.to_categorical((label_encoder.transform(train_y))) print(encoded_y[0]) . We have processed the train labels, as printed above (encoded_y), into vectors that contain 1 in the index of the category that the sample belongs to, and zeros in all other indexes. . Moreover, the training and test sets are now stacked together to look like this: . train_set=np.hstack((truncated_one_hot_gene[:train_size],truncated_one_hot_variation[:train_size],text_train_arrays)) test_set=np.hstack((truncated_one_hot_gene[train_size:],truncated_one_hot_variation[train_size:],text_test_arrays)) print(&#39;Training set shape is: &#39;, train_set.shape) # (3321, 350) print(&#39;Test set shape is: &#39;, test_set.shape) # (986, 350) print(&#39;Training set example rows:&#39;) print(train_set[0][:10]) # [ -2.46065582e-23 -5.21548048e-19 -1.95048372e-20 -2.44542833e-22 # -1.19176742e-22 1.61985461e-25 2.93618862e-25 -6.23860891e-27 # 1.14583929e-28 -1.79996588e-29] print(&#39;Test set example rows:&#39;) print(test_set[0][:10]) # [ 9.74220189e-33 -1.31484613e-27 4.37925347e-27 -9.88109317e-29 # 7.66365772e-27 6.58254980e-26 -3.74901712e-26 -8.97613299e-26 # -3.75471102e-23 -1.05563623e-21] . Our data is now ready to be fed into a machine learning model, in this case, into a neural network in TensorFlow. . Training a 4-layer neural network for classification . The next step is to create a neural network on TensorFlow. I am using a fully-connected neural network with 4 layers. For details on how the network is built, you can check my TensorFlow MNIST notebook. Wherever necessary, I will explains what adaptations were specifically necessary for this challenge. . import math import time import matplotlib.pyplot as plt import numpy as np import pandas as pd import tensorflow as tf from sklearn.model_selection import train_test_split from tensorflow.python.framework import ops %matplotlib inline np.random.seed(1) . I found it useful to add the current timestamp to the name of the files that the code will output. This helped me to uniquely identify the results from each run. . timestr = time.strftime(&quot;%Y%m%d-%H%M%S&quot;) dirname = &#39;output/&#39; # output directory filename = &#39;&#39; . I select 20% of the training data to use as a validation set and be able to quantify my variance (watch out for overfitting), as I don&#39;t want to have an algorithm that only works well with this specific training data set that was provided, but one that generalizes as well as possible. . X_train, X_val, Y_train, Y_val = train_test_split(train_set, encoded_y, test_size=0.20, random_state=42) X_train, X_val, Y_train, Y_val = X_train.T, X_val.T, Y_train.T, Y_val.T # transpose test set X_test = test_set.T . print(&#39;X_train: &#39;, X_train.shape) print(&#39;X_val: &#39;, X_val.shape) print(&#39;Y_train: &#39;, Y_train.shape) print(&#39;Y_val: &#39;, Y_val.shape) print(&#39;X_test: &#39;, X_test.shape) . X_train: (350, 2656) X_val: (350, 665) Y_train: (9, 2656) Y_val: (9, 665) X_test: (350, 986) . Now I define the functions needed to build the neural network. . def create_placeholders(n_x, n_y): &quot;&quot;&quot; Creates the placeholders for the tensorflow session. Arguments: n_x -- scalar, dimensions of the input n_y -- scalar, number of classes (from 0 to 8, so -&gt; 9) Returns: X -- placeholder for the data input, of shape [n_x, None] and dtype &quot;float&quot; Y -- placeholder for the input labels, of shape [n_y, None] and dtype &quot;float&quot; &quot;&quot;&quot; X = tf.placeholder(tf.float32, shape=(n_x, None), name=&#39;X&#39;) Y = tf.placeholder(tf.float32, shape=(n_y, None), name=&#39;Y&#39;) return X, Y . def initialize_parameters(): &quot;&quot;&quot; Initializes parameters to build a neural network with tensorflow. Returns: parameters -- a dictionary of tensors containing W and b for every layer &quot;&quot;&quot; tf.set_random_seed(1) W1 = tf.get_variable(&#39;W1&#39;, [350, X_train.shape[0]], initializer=tf.contrib.layers.xavier_initializer(seed=1)) b1 = tf.get_variable(&#39;b1&#39;, [350, 1], initializer=tf.zeros_initializer()) W2 = tf.get_variable(&#39;W2&#39;, [350, 350], initializer=tf.contrib.layers.xavier_initializer(seed=1)) b2 = tf.get_variable(&#39;b2&#39;, [350, 1], initializer=tf.zeros_initializer()) W3 = tf.get_variable(&#39;W3&#39;, [100, 350], initializer=tf.contrib.layers.xavier_initializer(seed=1)) b3 = tf.get_variable(&#39;b3&#39;, [100, 1], initializer=tf.zeros_initializer()) W4 = tf.get_variable(&#39;W4&#39;, [9, 100], initializer=tf.contrib.layers.xavier_initializer(seed=1)) b4 = tf.get_variable(&#39;b4&#39;, [9, 1], initializer=tf.zeros_initializer()) parameters = {&quot;W1&quot;: W1, &quot;b1&quot;: b1, &quot;W2&quot;: W2, &quot;b2&quot;: b2, &quot;W3&quot;: W3, &quot;b3&quot;: b3, &quot;W4&quot;: W4, &quot;b4&quot;: b4} return parameters . def forward_propagation(X, parameters, keep_prob1, keep_prob2): &quot;&quot;&quot; Implements the forward propagation for the model: (LINEAR -&gt; RELU)^3 -&gt; LINEAR -&gt; SOFTMAX Arguments: X -- input dataset placeholder, of shape (input size, number of examples) parameters -- python dictionary containing your parameters &quot;W&quot; and &quot;b&quot; for every layer the shapes are given in initialize_parameters Returns: Z4 -- the output of the last LINEAR unit (logits) &quot;&quot;&quot; # Retrieve the parameters from the dictionary &quot;parameters&quot; W1 = parameters[&#39;W1&#39;] b1 = parameters[&#39;b1&#39;] W2 = parameters[&#39;W2&#39;] b2 = parameters[&#39;b2&#39;] W3 = parameters[&#39;W3&#39;] b3 = parameters[&#39;b3&#39;] W4 = parameters[&#39;W4&#39;] b4 = parameters[&#39;b4&#39;] Z1 = tf.matmul(W1, X) + b1 # Z1 = np.dot(W1, X) + b1 A1 = tf.nn.relu(Z1) # A1 = relu(Z1) A1 = tf.nn.dropout(A1, keep_prob1) # add dropout Z2 = tf.matmul(W2, A1) + b2 # Z2 = np.dot(W2, a1) + b2 A2 = tf.nn.relu(Z2) # A2 = relu(Z2) A2 = tf.nn.dropout(A2, keep_prob2) # add dropout Z3 = tf.matmul(W3, A2) + b3 # Z3 = np.dot(W3,Z2) + b3 A3 = tf.nn.relu(Z3) Z4 = tf.matmul(W4, A3) + b4 return Z4 . def compute_cost(Z4, Y): &quot;&quot;&quot; Computes the cost Arguments: Z4 -- output of forward propagation (output of the last LINEAR unit), of shape (n_classes, number of examples) Y -- &quot;true&quot; labels vector placeholder, same shape as Z4 Returns: cost - Tensor of the cost function &quot;&quot;&quot; # transpose to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...) logits = tf.transpose(Z4) labels = tf.transpose(Y) cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)) return cost . def random_mini_batches(X, Y, mini_batch_size, seed=0): &quot;&quot;&quot; Creates a list of random minibatches from (X, Y) Arguments: X -- input data, of shape (input size, number of examples) Y -- true &quot;label&quot; vector, of shape (1, number of examples) mini_batch_size - size of the mini-batches, integer seed Returns: mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y) &quot;&quot;&quot; m = X.shape[1] # number of training examples mini_batches = [] np.random.seed(seed) # Step 1: Shuffle (X, Y) permutation = list(np.random.permutation(m)) shuffled_X = X[:, permutation] shuffled_Y = Y[:, permutation].reshape((Y.shape[0], m)) # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case. num_complete_minibatches = math.floor( m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitioning for k in range(0, num_complete_minibatches): mini_batch_X = shuffled_X[:, k * mini_batch_size: k * mini_batch_size + mini_batch_size] mini_batch_Y = shuffled_Y[:, k * mini_batch_size: k * mini_batch_size + mini_batch_size] mini_batch = (mini_batch_X, mini_batch_Y) mini_batches.append(mini_batch) # Handling the end case (last mini-batch &lt; mini_batch_size) if m % mini_batch_size != 0: mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size: m] mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size: m] mini_batch = (mini_batch_X, mini_batch_Y) mini_batches.append(mini_batch) return mini_batches . def predict(X, parameters): W1 = tf.convert_to_tensor(parameters[&#39;W1&#39;]) b1 = tf.convert_to_tensor(parameters[&quot;b1&quot;]) W2 = tf.convert_to_tensor(parameters[&quot;W2&quot;]) b2 = tf.convert_to_tensor(parameters[&quot;b2&quot;]) W3 = tf.convert_to_tensor(parameters[&quot;W3&quot;]) b3 = tf.convert_to_tensor(parameters[&quot;b3&quot;]) W4 = tf.convert_to_tensor(parameters[&quot;W4&quot;]) b4 = tf.convert_to_tensor(parameters[&quot;b4&quot;]) params = {&quot;W1&quot;: W1, &quot;b1&quot;: b1, &quot;W2&quot;: W2, &quot;b2&quot;: b2, &quot;W3&quot;: W3, &quot;b3&quot;: b3, &quot;W4&quot;: W4, &quot;b4&quot;: b4} x = tf.placeholder(&quot;float&quot;, [X_train.shape[0], None]) keep_prob1 = tf.placeholder(tf.float32, name=&#39;keep_prob1&#39;) keep_prob2 = tf.placeholder(tf.float32, name=&#39;keep_prob2&#39;) z4 = forward_propagation(x, params, keep_prob1, keep_prob2) p = tf.nn.softmax(z4, dim=0) # dim=0 because the classes are on that axis # p = tf.argmax(z4) # this gives only the predicted class as output sess = tf.Session() prediction = sess.run(p, feed_dict={x: X, keep_prob1: 1.0, keep_prob2: 1.0}) return prediction . And now I define the model function which is in fact the neural network that we will train afterwards. An important difference with respect to my previous MNIST example is that I added an additional regularization term to the cost function. I used L2 regularization to penalize the weights in all four layers. The bias was not penalized as this is not necessary. The strictness of this penalty was given by a beta constant defined at 0.01. . Why use additional regularization? Because this allowed me to decrease the variance, i.e. decrease the difference in performance of the model with the training set compared to the validation set. This produced my best submission in the competition. . def model(X_train, Y_train, X_test, Y_test, learning_rate=0.0001, num_epochs=1000, minibatch_size=64, print_cost=True): &quot;&quot;&quot; Implements a four-layer tensorflow neural network: (LINEAR-&gt;RELU)^3-&gt;LINEAR-&gt;SOFTMAX. Arguments: X_train -- training set, of shape (input size, number of training examples) Y_train -- test set, of shape (output size, number of training examples) X_test -- training set, of shape (input size, number of training examples) Y_test -- test set, of shape (output size, number of test examples) learning_rate -- learning rate of the optimization num_epochs -- number of epochs of the optimization loop minibatch_size -- size of a minibatch print_cost -- True to print the cost every 100 epochs Returns: parameters -- parameters learnt by the model. They can then be used to predict. &quot;&quot;&quot; ops.reset_default_graph() # to be able to rerun the model without overwriting tf variables tf.set_random_seed(1) # to keep consistent results seed = 3 # to keep consistent results (n_x, m) = X_train.shape # (n_x: input size, m : number of examples in the train set) n_y = Y_train.shape[0] # n_y : output size costs = [] # To keep track of the cost t0 = time.time() # to mark the start of the training # Create Placeholders of shape (n_x, n_y) X, Y = create_placeholders(n_x, n_y) keep_prob1 = tf.placeholder(tf.float32, name=&#39;keep_prob1&#39;) # probability to keep a unit during dropout keep_prob2 = tf.placeholder(tf.float32, name=&#39;keep_prob2&#39;) # Initialize parameters parameters = initialize_parameters() # Forward propagation Z4 = forward_propagation(X, parameters, keep_prob1, keep_prob2) # Cost function cost = compute_cost(Z4, Y) regularizers = tf.nn.l2_loss(parameters[&#39;W1&#39;]) + tf.nn.l2_loss(parameters[&#39;W2&#39;]) + tf.nn.l2_loss(parameters[&#39;W3&#39;]) + tf.nn.l2_loss(parameters[&#39;W4&#39;]) # add regularization term beta = 0.01 # regularization constant cost = tf.reduce_mean(cost + beta * regularizers) # cost with regularization # Backpropagation: Define the tensorflow AdamOptimizer. optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Initialize all the variables init = tf.global_variables_initializer() # Start the session to compute the tensorflow graph with tf.Session() as sess: # Run the initialization sess.run(init) # Do the training loop for epoch in range(num_epochs): epoch_cost = 0. # Defines a cost related to an epoch num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set seed = seed + 1 minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed) for minibatch in minibatches: # Select a minibatch (minibatch_X, minibatch_Y) = minibatch # Run the session to execute the &quot;optimizer&quot; and the &quot;cost&quot; _, minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y, keep_prob1: 0.7, keep_prob2: 0.5}) epoch_cost += minibatch_cost / num_minibatches # Print the cost every epoch if print_cost == True and epoch % 100 == 0: print(&quot;Cost after epoch {}: {:f}&quot;.format(epoch, epoch_cost)) if print_cost == True and epoch % 5 == 0: costs.append(epoch_cost) # lets save the parameters in a variable parameters = sess.run(parameters) print(&quot;Parameters have been trained!&quot;) # Calculate the correct predictions correct_prediction = tf.equal(tf.argmax(Z4), tf.argmax(Y)) # Calculate accuracy on the test set accuracy = tf.reduce_mean(tf.cast(correct_prediction, &quot;float&quot;)) train_cost = cost.eval({X: X_train, Y: Y_train, keep_prob1: 1.0, keep_prob2: 1.0}) test_cost = cost.eval({X: X_test, Y: Y_test, keep_prob1: 1.0, keep_prob2: 1.0}) train_accuracy = accuracy.eval({X: X_train, Y: Y_train, keep_prob1: 1.0, keep_prob2: 1.0}) test_accuracy = accuracy.eval({X: X_test, Y: Y_test, keep_prob1: 1.0, keep_prob2: 1.0}) print(&#39;Finished training in %s s&#39; % (time.time() - t0)) print(&quot;Train Cost:&quot;, train_cost) print(&quot;Test Cost:&quot;, test_cost) print(&quot;Train Accuracy:&quot;, train_accuracy) print(&quot;Test Accuracy:&quot;, test_accuracy) # plot the cost plt.plot(np.squeeze(costs)) plt.ylabel(&#39;cost&#39;) plt.xlabel(&#39;iterations (per fives)&#39;) plt.title(&quot;Learning rate = {}, beta = {}, n&quot; &quot;test cost = {:.6f}, test accuracy = {:.6f}&quot;.format(learning_rate, beta, test_cost, test_accuracy)) global filename filename = timestr + &#39;_NN4Lstage2_lr_{}_beta_{}_cost_{:.2f}-{:.2f}_acc_{:.2f}-{:.2f}&#39;.format( learning_rate, beta, train_cost, test_cost, train_accuracy, test_accuracy) plt.savefig(dirname + filename + &#39;.png&#39;) return parameters . Note that the model function will return the learned parameters from the network and additionally will plot the cost after each epoch. The plot is also saved as a file that includes the timestamp as well as the learning rate, beta, cost and accuracy information for this particular run. . Now it&#39;s time to train the model using the train and validation data: . parameters = model(X_train, Y_train, X_val, Y_val) . Cost after epoch 0: 6.607861 Cost after epoch 100: 1.389869 Cost after epoch 200: 0.988806 Cost after epoch 300: 0.882713 Cost after epoch 400: 0.833693 Cost after epoch 500: 0.811457 Cost after epoch 600: 0.793379 Cost after epoch 700: 0.773927 Cost after epoch 800: 0.762247 Cost after epoch 900: 0.767449 Parameters have been trained! Finished training in 498.4203100204468 s Train Cost: 0.665462 Test Cost: 1.74987 Train Accuracy: 0.979292 Test Accuracy: 0.643609 . From my validation results we can observe that the network learned nicely. However, the final cost of the training data was 0.665462, where as the validation data had a final cost of 1.74987. This is a large difference and an indication that the model is overfitting. Moreover the accuracy (defined here as the fraction of correct predictions) is very high (97.9%) for the training data and only 64.3% for the validation set. Another indication that the model is overfitting even though I have used both dropout and L2 regularization to counteract this. . Make predictions . We use the learned parameteres to make a prediction on the test data. . prediction = predict(X_test, parameters) . Let&#39;s look at an example of a prediction. As we can see below, the prediction consists of the probabilities of the entry belongin to each of the nine different categories (this was the format needed for this competition). . prediction[:,0] . array([ 0.36503336, 0.21219006, 0.01297534, 0.14676626, 0.08375936, 0.09217557, 0.02737238, 0.03150512, 0.02822249], dtype=float32) . prediction.shape . (9, 986) . All we have to do now is create a submission .csv file to save our prediction results. . submission = pd.DataFrame(prediction.T) submission[&#39;id&#39;] = test_index submission.columns = [&#39;class1&#39;, &#39;class2&#39;, &#39;class3&#39;, &#39;class4&#39;, &#39;class5&#39;, &#39;class6&#39;, &#39;class7&#39;, &#39;class8&#39;, &#39;class9&#39;, &#39;id&#39;] submission.to_csv(dirname + filename + &#39;.csv&#39;, index=False) . Results interpretation . Using this neural network model, my submission to Kaggle yielded following results: . Public score (based on a portion of the test data by Kaggle to provide an indication of performance during the competition): Loss = 1.69148 | Private score (based on a different portion of the test data by Kaggle to provide the final score at the end of the competition): Loss = 2.74500 | . The discrepancy between these two scores further shows that overfitting is an issue in working with this data in a neural network model. My model could benefit from increasing the training data and a higher regularization. .",
            "url": "https://www.arcosdiaz.com/blog/machine_learning/classification/healthcare/2017/10/07/personalized-medicine.html",
            "relUrl": "/machine_learning/classification/healthcare/2017/10/07/personalized-medicine.html",
            "date": " • Oct 7, 2017"
        }
        
    
  
    
        ,"post3": {
            "title": "Exploratory analysis of Medicare drug cost data 2011-2015",
            "content": "Health care systems world-wide are under pressure due to the high costs associated with disease. Now more than ever, particularly in developed countries, we have access to the latest advancements in medicine. This contrasts with the challenge of making those treatments available to as many patients as possible. It is imperative to find ways maximize the positive impact on the quality of life of patients, while maintaining a sustainable health care system. For this purpose I performed an analysis of Medicare data in the USA. Furthermore I used a drug-disease open database to cluster the costs by disease. I identified the most expensive diseases (mostly chronic diseases such as Diabetes) and the most expensive medicines. A drug for the treatment of HCV infections (Harvoni) stands out with the highest total costs in 2015. After this first exploration, I propose the in-depth analysis of further data to enable more targeted conclusions and recommendations to improve health care, such as linking of price databases to compare drug costs for the similar indications or the analysis of population data registers that document life style characteristics of healthy and sick individuals to identify those at risk of developing high-cost diseases. . Relevance . Health care costs amount to a considerable part of the national budgets all over the world. In 2015, $3.2 trillion were spent for health care in the USA (17.8% of its GDP). In Germany, the health care spending reached 11.3% of GDP in 2014. On the one hand, this high health care costs can be explained by the population growth, particularly the elderly proportion, requiring higher investments to secure quality of life. On the other hand, new medicines are continously being discovered enabling the treatment of diseases that were once a sentence of death. This has as a consequence that many once fatal diseases have now become chronic with a high burden on the health care costs. . But how can governments and insurers make sure that patients receive the care they need, including latest technology advances, without bankrupting the system? One first step is the identification of high-cost diseases and drugs. This insights can then be used to identify population segments at high-risk of developing a disease, who can then be the focus of prevention measures. . Governments, insurers, patient organizations, pharmaceutical and biotech companies need all to leverage their available data, if we are to improve the health of patients now and in future generations. . Methods . Data sources . Medicare Drug Spending Data 2011-2015: drug spending and utilization data. In this analysis only Medicare Part D drugs were considered (drugs patients generally administer themselves) | Therapeutic Targets Database: Drug-to-disease mapping with ICD identifiers. | . Tools . pandas for data crunching | fuzzywuzzy for fuzzy logic matching | git for version control | . Data preprocessing . First, I cleaned up and processed the drug spending data available from Medicare for the years 2011-2015. This data includes the total spending, claim number, and beneficiary number --among others-- for each drug identified by its brand and generic names. . import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns sns.set_palette(&#39;Paired&#39;) sns.set_style(&#39;whitegrid&#39;) %matplotlib inline import warnings warnings.filterwarnings(&#39;ignore&#39;) . data = pd.read_csv(&#39;data/medicare_data_disease.csv&#39;) data.head() . Unnamed: 0 Brand Name Generic Name Claim Count Total Spending Beneficiary Count Total Annual Spending Per User Unit Count Average Cost Per Unit (Weighted) Beneficiary Count No LIS Average Beneficiary Cost Share No LIS Beneficiary Count LIS Average Beneficiary Cost Share LIS Year Matched Drug Name Indication . 0 0 | 10 wash | sulfacetamide sodium | 24.0 | 1569.19 | 16.0 | 98.074375 | 5170.0 | 0.303518 | NaN | NaN | NaN | NaN | 2011 | sulfacetamide | Acne vulgaris | . 1 1 | 1st tier unifine pentips | pen needle, diabetic | 2472.0 | 57666.73 | 893.0 | 64.576405 | 293160.0 | 0.196766 | 422.0 | 42.347204 | 471.0 | 7.54586 | 2011 | NaN | NaN | . 2 2 | 1st tier unifine pentips plus | pen needle, diabetic | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 2011 | NaN | NaN | . 3 3 | 60pse-400gfn-20dm | guaifenesin/dm/pseudoephedrine | 12.0 | 350.10 | 11.0 | 31.827273 | 497.0 | 0.704427 | NaN | NaN | NaN | NaN | 2011 | pseudoephedrine | Nasal congestion | . 4 4 | 8-mop | methoxsalen | 11.0 | 9003.26 | NaN | NaN | 298.0 | 30.212282 | NaN | NaN | NaN | NaN | 2011 | methoxsalen | Cutaneous T-cell lymphoma | . I also processed the data from the Therapeutic Targets Database, which presents the indications (diseases) associated with a drug generic name. . diseases = pd.read_csv(&#39;data/drug-disease_keys.csv&#39;) diseases.head() . Unnamed: 0 TTDDRUGID LNM Indication ICD9 ICD10 . 0 0 | DAP000001 | quetiapine | Schizophrenia | 295, 710.0 | F20, M32 | . 1 1 | DAP000002 | theophylline | Chronic obstructive pulmonary disease | 490-492, 494-496 | J40-J44, J47 | . 2 2 | DAP000003 | risperidone | Schizophrenia | 295, 710.0 | F20, M32 | . 3 3 | DAP000004 | dasatinib | Chronic myelogenous leukemia | 205.1, 208.9 | C91-C95, C92.1 | . 4 4 | DAP000004 | dasatinib | Solid tumours; Multiple myeloma | 140-199, 203.0, 210-229 | C00-C75, C7A, C7B, C90.0, D10-D36, D3A | . Then, I used a fuzzy logic algorithm to match each drug generic name of the Medicare data with the closest element from the Therapeutic Targets Database. After having a list of exact matches, I assigned the first associated indication to each Medicare drug. For details on how I did this, please check my github repository. . Results . Figure 1: Most expensive drugs and indications by total spending in a 5-year interval . spending = data.groupby(&#39;Indication&#39;).sum().sort_values(by=&#39;Total Spending&#39;, ascending=False) spending.head() . Unnamed: 0 Claim Count Total Spending Beneficiary Count Total Annual Spending Per User Unit Count Average Cost Per Unit (Weighted) Beneficiary Count No LIS Average Beneficiary Cost Share No LIS Beneficiary Count LIS Average Beneficiary Cost Share LIS Year . Indication . Diabetes mellitus 1619475 | 367700954.0 | 5.360758e+10 | 73949096.0 | 562815.013217 | 2.494315e+10 | 7277.339052 | 40704222.0 | 96730.900344 | 33243845.0 | 6840.136642 | 1449360 | . Schizophrenia 589890 | 120108011.0 | 3.029475e+10 | 16787537.0 | 665302.614794 | 5.232045e+09 | 23737.078668 | 4132330.0 | 75431.535074 | 12655011.0 | 2733.240495 | 503250 | . Chronic obstructive pulmonary disease 260320 | 85571788.0 | 2.668149e+10 | 18010399.0 | 117832.051227 | 5.169355e+09 | 1525.772351 | 8891428.0 | 16115.553801 | 9118941.0 | 953.965398 | 201300 | . Pain 2076820 | 449297282.0 | 2.237135e+10 | 125509481.0 | 635933.153421 | 3.582438e+10 | 8484.565888 | 68195577.0 | 86753.860239 | 57310277.0 | 5810.508368 | 1660725 | . Hypertension 1241330 | 659834372.0 | 2.140793e+10 | 127524840.0 | 453862.885999 | 3.924869e+10 | 2875.275230 | 84758076.0 | 99376.738594 | 42766156.0 | 8392.438238 | 1338645 | . spending_drug = data.groupby(&#39;Brand Name&#39;).sum().sort_values(by=&#39;Total Spending&#39;, ascending=False) spending_drug.head() . Unnamed: 0 Claim Count Total Spending Beneficiary Count Total Annual Spending Per User Unit Count Average Cost Per Unit (Weighted) Beneficiary Count No LIS Average Beneficiary Cost Share No LIS Beneficiary Count LIS Average Beneficiary Cost Share LIS Year . Brand Name . lantus/lantus solostar 10560 | 40959410.0 | 1.419734e+10 | 7627126.0 | 9059.358978 | 7.905816e+08 | 87.058010 | 3685935.0 | 1757.877943 | 3941191.0 | 119.054297 | 10065 | . nexium 13330 | 37338541.0 | 1.129409e+10 | 6968266.0 | 8159.874433 | 1.624007e+09 | 35.029401 | 2808631.0 | 1282.120804 | 4159635.0 | 108.193170 | 10065 | . crestor 4670 | 43304032.0 | 1.084924e+10 | 8312848.0 | 6460.499385 | 1.917524e+09 | 27.902605 | 5275631.0 | 1477.330224 | 3037217.0 | 126.576936 | 10065 | . advair diskus 385 | 30806126.0 | 1.036056e+10 | 7096159.0 | 7313.316443 | 2.273054e+09 | 22.805665 | 3613170.0 | 1284.084879 | 3482989.0 | 98.586905 | 10065 | . abilify 45 | 12506518.0 | 9.434570e+09 | 1861785.0 | 25165.999223 | 3.818410e+08 | 127.685877 | 333884.0 | 2541.346071 | 1527901.0 | 101.829169 | 10065 | . n_top = 40 fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=False, figsize=(8,8)) g = sns.barplot(x=&#39;Total Spending&#39;, y=&#39;Indication&#39;, data=spending.reset_index()[:n_top], estimator=np.sum, ax=ax1, color=sns.xkcd_rgb[&#39;dodger blue&#39;]) g.set(yticklabels=[i[:27] for i in spending[:n_top].index]) g.set_xlabel(&#39;Total Spending $&#39;) g2 = sns.barplot(x=&#39;Total Spending&#39;, y=&#39;Brand Name&#39;, data=spending_drug.reset_index()[:n_top], estimator=np.sum, ax=ax2, color=&#39;lightblue&#39;) g2.set(yticklabels=[i[:20] for i in spending_drug[:n_top].index]) g2.set_xlabel(&#39;Total Spending $&#39;) #plt.title(&#39;Top 50 indications by Beneficiary Count Sum from 2011 to 2015&#39;) fig.suptitle(&#39;Top %s indications and drugs for 5-year total spending 2011-2015&#39; %n_top, size=16) plt.tight_layout() fig.subplots_adjust(top=0.94) plt.savefig(&#39;Top_%s_disease_drug.png&#39; %n_top, dpi=300, bbox_inches=&#39;tight&#39;) . Indications (left part) . A look at the total spending for the 5-year period 2011-2015 reveals that the bulk of drug spending is covered by a small set of diseases/indications (left graph). The total spending per indication decreases rapidly by going down the list of drugs. . Diabetes occupies the first place in this list with a total 5-year spending exceding $50 billion. Following in the list, we find other chronic diseases such as schizophrenia, chronic obstructive pulmonary disease, hypertension (high blood pressure), hypercholesterolemia (high cholesterol), depression, hiv infections, multiple sclerosis, peptic ulcer disease, and chronic HCV infection (hepatitis C). Interestingly, pain medications are also in the top 4 indications by total spending. . It makes sense that treatment of chronic diseases receives the highest investment in drug spending, as patients with these diseases can live long lives when medicated. . Interestingly, the first cancer reaches only the 19th place of this list (chronic myelogenous leukemia). However, it must be noted that cancer is actually a collection of different diseases with different genetics, origin, and treatment options. These different cancers were not clustered in this analysis. . Drugs (right part) . When we look at the most expensive drugs for the total 5-year spending, we find on the top of the list: Lantus (insulin), nexium (peptic ulcer), and crestor(anti cholesterol). It makes sense as these are medicines to treat chronic diseases. . However, we cannot learn much on a high level from looking at the total spending only. Therefore, a closer look is needed. . Figure 2: Drug spending is growing but at very heterogeneous rates . spend_2015_ind = data[data[&#39;Year&#39;]==2015].groupby(&#39;Indication&#39;).sum().sort_values(by=&#39;Total Spending&#39;, ascending=False) #spend_2015_drug = data[data[&#39;Year&#39;]==2015].groupby(&#39;Brand Name&#39;).sum().sort_values(by=&#39;Total Spending&#39;, # ascending=False) spend_2015_ind.head() . Unnamed: 0 Claim Count Total Spending Beneficiary Count Total Annual Spending Per User Unit Count Average Cost Per Unit (Weighted) Beneficiary Count No LIS Average Beneficiary Cost Share No LIS Beneficiary Count LIS Average Beneficiary Cost Share LIS Year . Indication . Diabetes mellitus 323895 | 80808515.0 | 1.538882e+10 | 16756712.0 | 179199.546237 | 5.861327e+09 | 2112.287818 | 9688833.0 | 24716.663299 | 7067683.0 | 1459.860044 | 290160 | . Chronic HCV infection 5421 | 272915.0 | 8.349020e+09 | 90487.0 | 182144.098903 | 7.546096e+06 | 2134.680173 | 30454.0 | 10701.886971 | 60033.0 | 156.351475 | 4030 | . Chronic obstructive pulmonary disease 52064 | 17764181.0 | 6.756824e+09 | 3803356.0 | 30714.837787 | 1.067257e+09 | 561.945076 | 1931374.0 | 4177.371552 | 1871982.0 | 232.217985 | 40300 | . Schizophrenia 117978 | 25030047.0 | 5.468897e+09 | 3493417.0 | 192134.968549 | 1.084651e+09 | 8292.122415 | 938911.0 | 19320.941152 | 2554333.0 | 573.729378 | 100750 | . Pain 415364 | 94109025.0 | 4.956161e+09 | 27047833.0 | 164077.757502 | 7.597111e+09 | 4357.883695 | 15366894.0 | 19810.346154 | 11680708.0 | 1105.913592 | 332475 | . top_10_spend = data[data[&#39;Year&#39;]==2015].sort_values(by=&#39;Total Spending&#39;, ascending=False)[[&#39;Brand Name&#39;, &#39;Total Spending&#39;, &#39;Year&#39;]][:10] top_10_spend . Brand Name Total Spending Year . 19770 harvoni | 7.030633e+09 | 2015 | . 20104 lantus/lantus solostar | 4.359504e+09 | 2015 | . 18926 crestor | 2.883122e+09 | 2015 | . 18069 advair diskus | 2.270016e+09 | 2015 | . 21640 spiriva | 2.191466e+09 | 2015 | . 19988 januvia | 2.131952e+09 | 2015 | . 21404 revlimid | 2.077425e+09 | 2015 | . 20658 nexium | 2.012921e+09 | 2015 | . 20291 lyrica | 1.766474e+09 | 2015 | . 19818 humira/humira pen | 1.662292e+09 | 2015 | . fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=False, figsize=(8,5)) g=sns.factorplot(x=&#39;Year&#39;, y=&#39;Total Spending&#39;, hue=&#39;Brand Name&#39;, palette=&#39;coolwarm&#39;, hue_order=top_10_spend[&#39;Brand Name&#39;], data=data[data[&#39;Brand Name&#39;].isin(top_10_spend[&#39;Brand Name&#39;])], ax=ax1) ax1.set_title(&#39;Annual spending for top 10 drugs&#39;) ax1.set_ylabel(&#39;Total Spending $&#39;) plt.close(g.fig) ax2.scatter(x=spend_2015_ind[&#39;Beneficiary Count&#39;][:100], y=spend_2015_ind[&#39;Total Spending&#39;][:100], s=spend_2015_ind[&#39;Claim Count&#39;][:100]/100000, #c=spend_2015_ind.reset_index()[&#39;Indication&#39;][:100]) color=sns.xkcd_rgb[&#39;dodger blue&#39;], alpha=0.75) ax2.set_title(&#39;Top 100 indications in 2015&#39;) plt.xlabel(&#39;Beneficiary Count&#39;) plt.ylabel(&#39;Total Spending $&#39;) plt.axis([0, None, 0, None]) for label, x, y in zip(spend_2015_ind.index, spend_2015_ind[&#39;Beneficiary Count&#39;][:10], spend_2015_ind[&#39;Total Spending&#39;][:10]): plt.annotate(label, xy=(x, y), color=&#39;red&#39;, alpha=0.75) fig.suptitle(&#39;Annual drug spending development and overview of highest-cost indications&#39;, size=16) plt.tight_layout() fig.subplots_adjust(top=0.85) plt.savefig(&#39;Top_bubble_disease_drug.png&#39;, dpi=300, bbox_inches=&#39;tight&#39;) . Annual spending development for top 10 drugs (left) . The drug landscape is not temporally static. Therefore, I analyzed the annual spending since 2011 for the 10 top drugs in 2015. Eight out of these ten drugs consistently received higher spending every year, a reflection of the general health care spending panorama. However, the rate of growth for each drug is dramatically different. Particularly striking is the case of the drug Harvoni, which exhibited a &gt;7-fold growth in total spending between 2014 and 2015. . Harvoni is a medicine for the treatment of hepatitis C (HCV infection) that was launched in 2014. It is the first drug with cure rates close to 100%. Harvoni practically cures a chronic disease and this is reflected in its pricing at over $90k for a 12 week treatment. . The remaining drugs in the figure are mostly used for the treatment of chronic diseases. . But how can we more extensively evaluate the burden posed by the different diseases/indications? . Top 100 indications in 2015 (right) . In order to find out more about the distribution of the most expensive indications, I plotted the drug spendings grouped by indication for the year 2015 in a scatter plot. This way, we can not only look at the total spending but also at the number of beneficiaries for a particular indication. The size of the bubbles represents the relative number of claims. . From this graph we can assess the magnitude of how the most expensive diseases affect society. Diabetes is not only the most expensive single indication by total spending but also affects a very large number of people. . The indications with the most beneficiaries are hypertension, pain and high cholesterol. They also represent some of the highest number of claims (bubble size). This indicates that the average cost associated with each claim is low, as these are generally medications with expired patents that are priced very low. . Again it is interesting to take a look at chronic HCV infection. This is the indication for the drug Harvoni. Both the number of beneficiaries and claims are extremely low compared with other diseases. However, chronic HCV infection reached the second place in the highes total drug spending in 2015. . Next steps . I have shown in this analysis that very interesting insights can be gained from analyzing a smaller set of publicly available data. It follows that a more detailed and deeper analysis could enable more targeted conclusions and recommendations for improving the health care system and the quality of life of patients suffering from disease. Access to non-public owned data would make even deeper analysis possible. . Additional analysis could include: . Clustering of diseases/indications to higher-level categories (cancer, metabolic disease, circulatory disease, nervous system disease, etc.) | Linking of price databases to compare drug costs for the same indication on a population level | Analysis of population data registers that document life style characteristics of healthy and ill individuals to identify those at risk of developing high-cost diseases (e.g. Medical Expenditure Panel Survey, Behavioral Risk Factor Surveillance System data) | . Limitations . One limitation from this analysis is that only Part D drugs were considered. A further analysis could include Part B drugs too. . Moreover it was assumed that the fuzzy logic matching was successful in most cases. A more detailed test is required to assess match success more stringently. . All conclusions are only valid for the 2011-2015 interval. No data for 2016 was analyzed. .",
            "url": "https://www.arcosdiaz.com/blog/healthcare/eda/visualization/2017/02/06/medicare-drug-cost.html",
            "relUrl": "/healthcare/eda/visualization/2017/02/06/medicare-drug-cost.html",
            "date": " • Feb 6, 2017"
        }
        
    
  
    
        ,"post4": {
            "title": "Visualizing parallel event series in Python",
            "content": "Do movie releases produce literal earthquakes? We always hear about new movie releases being a &quot;blast&quot;, some sure are. But how do two independent events correlate with each other? In this post, I will use Python to visualize two different series of events, plotting them on top of each other to gain insights from time series data. . from datetime import datetime import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns sns.set_palette(&#39;Set2&#39;) sns.set_style(&quot;whitegrid&quot;) %matplotlib inline . Getting the data . To make this example more fun, I decided to use two independent series of events for which data is readily available in the internet: . List of earthquakes around the world | List of film releases in the USA | . Clean and prepare earthquake data . We start by downloading the .csv export from the world earthquake website to the &#39;data&#39; directory and reading the file into a pandas DataFrame . df = pd.read_csv(&#39;data/earthquakes_raw.csv&#39;, sep=&#39;;&#39;) df.dropna((0,1), how=&#39;all&#39;, inplace=True) df.head() . Date Zone/Region Magnitude (Mw) Year . 0 December 09 | SOLOMON ISLANDS | 6.9 | 2016 | . 1 December 08 | CALIFORNIA | 6.5 | 2016 | . 2 December 08 | SOLOMON ISLANDS | 7.8 | 2016 | . 3 December 06 | SUMATRA | 6.5 | 2016 | . 4 November 25 | CHINA | 6.6 | 2016 | . We have to unify the date information from the Date and Year columns. Then we can save the cleaned-up earthquake date data to a file &#39;data/earthquakes.csv&#39; . df[&#39;Date&#39;] = df[&#39;Date&#39;] + &#39; &#39; + df[&#39;Year&#39;].map(str) del df[&#39;Year&#39;] df[&#39;Date&#39;] = df[&#39;Date&#39;].apply(lambda x: datetime.strptime(x, &#39;%B %d %Y&#39;)) df[&#39;Date&#39;].to_csv(&#39;data/earthquakes.csv&#39;) df.head() . Date Zone/Region Magnitude (Mw) . 0 2016-12-09 | SOLOMON ISLANDS | 6.9 | . 1 2016-12-08 | CALIFORNIA | 6.5 | . 2 2016-12-08 | SOLOMON ISLANDS | 7.8 | . 3 2016-12-06 | SUMATRA | 6.5 | . 4 2016-11-25 | CHINA | 6.6 | . Clean and prepare movie release data . The movie release data was retrieved from this website and saved to the &#39;data&#39; directory. We then read the file into a pandas DataFrame. The resulting table tells us the release date and which movies where released on a that date (up to 5 movies). . df = pd.read_csv(&#39;data/filmrelease_raw.csv&#39;, sep=&#39;;&#39;, header=None) df.dropna((0,1), how=&#39;all&#39;, inplace=True, thresh=2) df.columns = [&#39;Date&#39;, &#39;Film1&#39;, &#39;Film2&#39;, &#39;Film3&#39;, &#39;Film4&#39;, &#39;Film5&#39;] df.head(), df.tail() . ( Date Film1 Film2 0 Friday, January 9, 2015 Taken 3 NaN 4 Friday, January 16, 2015 Blackhat Paddington 14 Friday, January 23, 2015 Mortdecai Strange Magic 24 Friday, January 30, 2015 Black or White Project Almanac 34 Friday, February 6, 2015 Jupiter Ascending Seventh Son Film3 Film4 Film5 0 NaN NaN NaN 4 The Wedding Ringer NaN NaN 14 The Boy Next Door NaN NaN 24 The Loft NaN NaN 34 SpongeBob Movie: Sponge Out of Water NaN NaN , Date Film1 Film2 836 Friday, December 9 🎥 Office Christmas Party NaN 840 Friday, December 16 🎥 Collateral Beauty 🎥 La La Land 850 Wednesday, December 21 🎥 Assassin&#39;s Creed 🎥 Passengers 863 Friday, December 23 🎥 Why Him? NaN 867 Sunday, December 25 🎥 Fences NaN Film3 Film4 Film5 836 NaN NaN NaN 840 🎥 Rogue One: A Star Wars Story NaN NaN 850 🎥 Patriots Day 🎥 Sing NaN 863 NaN NaN NaN 867 NaN NaN NaN ) . Talk about raw unclean data! It seems that, at the top of the table, the date information contains the year (2015). However, upon further inspection we can see that the bottom of the table does not show us the year anymore. From the website information we find out that, after the index 716 and onwards, the missing year information is &#39;2016&#39;. So we add this data to the DataFrame and change the date format to a more readable one. . df.loc[lambda x: x.index &gt;= 716, &#39;Date&#39;] += &#39;, 2016&#39; df[&#39;Date&#39;] = df[&#39;Date&#39;].apply(lambda x: datetime.strptime(x, &#39;%A, %B %d, %Y&#39;)) df.head() . Date Film1 Film2 Film3 Film4 Film5 . 0 2015-01-09 | Taken 3 | NaN | NaN | NaN | NaN | . 4 2015-01-16 | Blackhat | Paddington | The Wedding Ringer | NaN | NaN | . 14 2015-01-23 | Mortdecai | Strange Magic | The Boy Next Door | NaN | NaN | . 24 2015-01-30 | Black or White | Project Almanac | The Loft | NaN | NaN | . 34 2015-02-06 | Jupiter Ascending | Seventh Son | SpongeBob Movie: Sponge Out of Water | NaN | NaN | . For the purpose of plotting the frequency of an event, we are not interested in what movies were released, but only in how many on a particular date. We can count the movies by replacing the names with ones and calculating the sum. . df.iloc[:,1:] = df.iloc[:,1:].replace(r&#39; w&#39;, 1.0, regex=True) df.head() . Date Film1 Film2 Film3 Film4 Film5 . 0 2015-01-09 | 1.0 | NaN | NaN | NaN | NaN | . 4 2015-01-16 | 1.0 | 1.0 | 1.0 | NaN | NaN | . 14 2015-01-23 | 1.0 | 1.0 | 1.0 | NaN | NaN | . 24 2015-01-30 | 1.0 | 1.0 | 1.0 | NaN | NaN | . 34 2015-02-06 | 1.0 | 1.0 | 1.0 | NaN | NaN | . We can finally get rid of the unnecessary columns and save the clean data to a file. Now we are ready to start plotting our event series data. . df[&#39;film_sum&#39;] =df[[&#39;Film1&#39;, &#39;Film2&#39;, &#39;Film3&#39;, &#39;Film4&#39;, &#39;Film5&#39;]].sum(axis=1) df.drop([&#39;Film1&#39;, &#39;Film2&#39;, &#39;Film3&#39;, &#39;Film4&#39;, &#39;Film5&#39;], axis=1, inplace=True) df.to_csv(&#39;data/filmrelease.csv&#39;) df.head() . Date film_sum . 0 2015-01-09 | 1.0 | . 4 2015-01-16 | 3.0 | . 14 2015-01-23 | 3.0 | . 24 2015-01-30 | 3.0 | . 34 2015-02-06 | 3.0 | . Load the data to the plotting variables . To use this script, we have to load the clean data that we saved in the previuos steps. . df1 = pd.read_csv(&#39;data/earthquakes.csv&#39;, header=None) del df1[0] df1.columns = [&#39;Date&#39;] df1[&#39;earthquake&#39;] = np.ones(len(df1)) df1.head() . Date earthquake . 0 2016-12-09 | 1.0 | . 1 2016-12-08 | 1.0 | . 2 2016-12-08 | 1.0 | . 3 2016-12-06 | 1.0 | . 4 2016-11-25 | 1.0 | . df2 = pd.read_csv(&#39;data/filmrelease.csv&#39;, header=0) del df2[&#39;Unnamed: 0&#39;] df2.columns = [&#39;Date&#39;, &#39;movie_release&#39;] df2.head() . Date movie_release . 0 2015-01-09 | 1.0 | . 1 2015-01-16 | 3.0 | . 2 2015-01-23 | 3.0 | . 3 2015-01-30 | 3.0 | . 4 2015-02-06 | 3.0 | . As end result, we want to have a single DataFrame containing the data for both event series. Moreover, we want to have a continuous time series, including those days in which none of the two events took place (no earthquakes, and no movie releases). We do this by using the concatenate and resample functions of pandas. . df = pd.concat([df1, df2], ignore_index=True) df = df.set_index(pd.DatetimeIndex(df.Date)) df = df.sort_index() df = df.resample(&#39;1d&#39;).sum().fillna(0) # to complete every day df.head(10) . earthquake movie_release . Date . 2015-01-09 0.0 | 1.0 | . 2015-01-10 0.0 | 0.0 | . 2015-01-11 0.0 | 0.0 | . 2015-01-12 0.0 | 0.0 | . 2015-01-13 0.0 | 0.0 | . 2015-01-14 0.0 | 0.0 | . 2015-01-15 0.0 | 0.0 | . 2015-01-16 0.0 | 3.0 | . 2015-01-17 0.0 | 0.0 | . 2015-01-18 0.0 | 0.0 | . Calculating and plotting a moving average . We could simply plot each event occurrence as a data point in a time series. However, this will likely yield a graph that is not very informative. Much easier to grasp is a moving average that tells us the average frequency of the events for a defined period of time in the past. We can create columns for these moving averages, which we can then easily plot. . for i in [7*4, 7*4*2]: mvav = i # moving average period, i.e. number of points to average dfi = np.convolve(df[&#39;earthquake&#39;], np.ones((mvav,))*7/mvav # factor for obtaining average , mode=&#39;full&#39;) df[&#39;earthquake moving average %sw&#39; % (int(i/7))] = dfi[:-(i-1)] dfj = np.convolve(df[&#39;movie_release&#39;], np.ones((mvav,))*7/mvav # factor for obtaining average , mode=&#39;full&#39;) df[&#39;movie_release moving average %sw&#39; % (int(i/7))] = dfj[:-(i-1)] df.head() . earthquake movie_release earthquake moving average 4w movie_release moving average 4w earthquake moving average 8w movie_release moving average 8w . Date . 2015-01-09 0.0 | 1.0 | 0.0 | 0.25 | 0.0 | 0.125 | . 2015-01-10 0.0 | 0.0 | 0.0 | 0.25 | 0.0 | 0.125 | . 2015-01-11 0.0 | 0.0 | 0.0 | 0.25 | 0.0 | 0.125 | . 2015-01-12 0.0 | 0.0 | 0.0 | 0.25 | 0.0 | 0.125 | . 2015-01-13 0.0 | 0.0 | 0.0 | 0.25 | 0.0 | 0.125 | . The shorter the period that we choose for the moving average, the noisier our graphic will get. Let&#39;s settle with a moving average that reflects the frequency during the past 8 weeks. And voilà! Now we can see how the frequency of earthquakes and movie releases changed over time. . df.loc[:,[&#39;earthquake moving average 8w&#39;, &#39;movie_release moving average 8w&#39;]]. plot(cmap=&#39;Set2&#39;, figsize=(12,4))# possible plt.xlim(df.index[0], df.index.max()+10) plt.title(&#39;Moving average of events per week&#39;) plt.ylabel(&#39;Frequency&#39;) plt.show() . Descriptive analysis of the event occurrence . What other insights can we get from this data set? Two very dissimilar series of events, one natural, and one man-made, will surely have very different properties. Let&#39;s start with a simple question: on which days of the week do both events typically happen? . # create column for day of the week df[&#39;Day&#39;] = df.index.dayofweek df[&#39;Day&#39;] = df.Day.astype(&#39;category&#39;) df.Day.cat.categories = [&#39;Mon&#39;,&#39;Tue&#39;, &#39;Wed&#39;, &#39;Thu&#39;, &#39;Fri&#39;, &#39;Sat&#39;, &#39;Sun&#39;] # create column for type df[&#39;Type&#39;] = np.where(df[&#39;earthquake&#39;]&gt;0, &#39;earthquake&#39;, np.where(df[&#39;movie_release&#39;]&gt;0, &#39;movie_release&#39;, np.nan)) df[&#39;Type&#39;] = df[&#39;Type&#39;].astype(&#39;category&#39;) df[&#39;Type&#39;] = df[&#39;Type&#39;].cat.remove_categories([&#39;nan&#39;]) # show count for each day #df[(df.Event1 == 1)&amp;(df.Day == &#39;Mon&#39;)].Day.count() # plot count data per day of the week plt.figure() plt.title(&#39;Event count per day of the week&#39;) sns.countplot(data=df, x=&#39;Day&#39;, hue=&#39;Type&#39;) sns.despine(left=True) plt.show() . We can see that nature does not respect our weekends, as earthquakes seem to be flatly distributed by day of the week. . The movie releases, on the other hand, are most frequent on Friday, followed by Wednesday and a few on Sunday (it&#39;s almost as if movie release days were chosen by someone... /s). It seems that, if you&#39;re planning to release a new movie in the US, Friday is the way to go. People are usually happy to start the weekend with a leisurely activity, so that makes sense. As of why Saturdays and Sundays are almost not used as movie release days, even though on this days people are also usually free from work, it would be interesting to find out why. Another intriguing finding is the not high but remarkable number of releases on Wednesdays. Don&#39;t people work on Thursdays? . Analysis of frequency per week . The world is a big place (or is it a small world?) and earthquakes occur all the time, even though we might not always find out. On the other hand, I would expect that movie releases occur much more frequently. So let&#39;s take a look at the data by plotting histograms for both events side by side. . plt.figure() df[&#39;earthquake moving average 8w&#39;].hist(alpha=.9) df[&#39;movie_release moving average 8w&#39;].hist(alpha=.9) plt.title(&#39;Histogram of event frequency per week&#39;) plt.show() # With seaborn sns.distplot(df[&#39;earthquake moving average 8w&#39;]), sns.distplot(df[&#39;movie_release moving average 8w&#39;]) . (&lt;matplotlib.axes._subplots.AxesSubplot at 0x11eb49198&gt;, &lt;matplotlib.axes._subplots.AxesSubplot at 0x11eb49198&gt;) . Luckily, movie releases are much more frequent per week as earthquakes. On most weeks, there are between two and three movie releases, compared to 0.5 to 1.5 earthquakes. . Final remarks . In this post, we gathered information on the occurrence of two events: earthquakes around the world, and movie releases in the US. By plotting their moving averages we could better compare when they occurred and gained some interesting insights about how they compare. All thanks to Python! .",
            "url": "https://www.arcosdiaz.com/blog/time%20series/visualization/2017/02/06/event-tracker.html",
            "relUrl": "/time%20series/visualization/2017/02/06/event-tracker.html",
            "date": " • Feb 6, 2017"
        }
        
    
  
    
        ,"post5": {
            "title": "Simulating the revenue of a product with Monte-Carlo random walks",
            "content": "Being able to see the future would be a great superpower (or so one would think). Luckily, it is already possible to model the future using Python to gain insights into a number of problems from many different areas. In marketing, being able to model how successful a new product will be, would be of great use. In this post, I will take a look at how we can model the future revenue of a product by making certain assumptions and running a Monte Carlo Markov Chain simulation. . What are Monte Carlo methods? . Wikipedia tells us that: . Monte Carlo methods (or Monte Carlo experiments) are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. Their essential idea is using randomness to solve problems that might be deterministic in principle. . In simple terms, we define a number of rules about how a system will behave based on assumptions, and then use random samplings of these conditions over and over and measure the results. We can then look at the results altogether to gain insights into our model. . Let&#39;s see this in practice! . import numpy as np from pylab import triangular, zeros, percentile from scipy.stats import binom import pandas as pd import seaborn as sns sns.set_palette(&#39;coolwarm&#39;) sns.set_style(&quot;whitegrid&quot;) import matplotlib.pyplot as plt %matplotlib inline . /Users/dario/anaconda/envs/tensorflow/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment. warnings.warn(&#39;Matplotlib is building the font cache using fc-list. This may take a moment.&#39;) /Users/dario/anaconda/envs/tensorflow/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment. warnings.warn(&#39;Matplotlib is building the font cache using fc-list. This may take a moment.&#39;) . Define the initial assumptions . What assumptions can we safely make regarding our new product? For example, what is the estimated market size that we want to work with and what is the estimated price that we can expect. We also define the num_timesteps, the number of years for which we want to make the calculations. . marketsize_min = 5000 marketsize_mode = 12000 marketsize_max = 15000 marketshare_init = triangular(.003, .005, .01) # min, mode, max # initial percentage of users that use the product price_min=500 # minimum product price price_mode=1000 # mode product price price_max=1500 # maximum product price num_timesteps=10 # number of years for the simulation num_simulations=1024 # number of simulations per year perc_selection = [5, 25, 50, 75, 95] # percentiles to visualize in plots . Define the functions to calculate market share and revenue of a product . These are the functions used to get the data points by random sampling. Each time we run each function, the variables are randomly defined from a range and a result is obtained, e.g. a market share or a revenue amount. . def calc_marketshare(marketsize, marketshare): &#39;&#39;&#39; Calculates product market share for a given year as percentage of users that use the product compared to total number of users Arguments: marketsize : total market size as number of potential users marketshare: observed/assumed percentage of users that use the product &#39;&#39;&#39; share = binom.rvs(marketsize, marketshare, size=1) / marketsize return share def calc_revenue(marketsize, marketshare): &#39;&#39;&#39; Calculates the revenue development over a number of years Arguments: marketsize: total market size as number of potential users marketshare : observed/assumed percentarge of users that use the product &#39;&#39;&#39; product_price = triangular(price_min, price_mode, price_max) volume = marketsize*marketshare revenue = product_price * volume return revenue . Additionally, in case that a distribution is not included in the standard statistical modules of Python, we can custom write them. For example, we can define functions to return logistic and sigmoid distributions. . def logist(x, loc, scale, factor): &#39;&#39;&#39; Logistic distribution Args: x : variable in x-axis, e.g. time loc : the mean of the distribution, maximum probability scale : steepness of the curve, higher -&gt; steeper factor : multiplies to obtain higher probabilities overall &#39;&#39;&#39; return factor*np.exp((loc-x)/scale)/(scale*(1+np.exp((loc-x)/scale))**2) def sigmoid(x): L, q, loc = 10, 1, 3 return L/(1+np.exp(-q*(x-loc))) . Why do we need this logistic distribution? For example, if we want to take into account the market growth in the next ten years, we could simply assume it will be 1% or 2% or 10% and keep it constant. However, we have Python on our side and can rather model this growth in a semi-random way. We assume that the market growth is more likely to be lower (between 0 and 4%) but we want to also consider the lower probability cases in which the growth could be higher, e.g. 8%. . def logist_test(x): loc, scale = 2, 2 return 4*np.exp((loc-x)/scale)/(scale*(1+np.exp((loc-x)/scale))**2) x = np.arange(0,10) plt.plot(logist_test(x)) #plt.plot(bins, logist(bins, loc, scale)*count.max()/logist(bins, loc, scale).max()) plt.show() . Data collection and simulation . Now that we have all assumptions and &quot;rules&quot; in place, let&#39;s get some data points. . First let&#39;s create some empty matrixes where we will put the data later. . u = zeros((num_simulations,), dtype=float) # temporary market size matrix as number of potential users s = zeros((num_simulations,), dtype=float) # temporary market share matrix r = zeros((num_simulations,), dtype=float) # temporary revenue matrix rev = zeros((num_timesteps, num_simulations), dtype=float) # revenue data collection by year percentiles_rev = zeros((num_timesteps,len(perc_selection)), dtype=float) # percentiles_rev data collection by year usr = zeros((num_timesteps, num_simulations), dtype=float) # users data collection by year percentiles_usr = zeros((num_timesteps,len(perc_selection)), dtype=float) # percentiles for total users sha = zeros((num_timesteps, num_simulations), dtype=float) # market share data collection by year percentiles_sha = zeros((num_timesteps,len(perc_selection)), dtype=float) # percentiles for market share . Now we can run the simulations to get our data points for the next 10 years. The results are captured in the pre-created matrices. . for t in range(0, num_timesteps): if t==0: # First year starting with initial assumptions for k in range(num_simulations): u[k] = triangular(marketsize_min,marketsize_mode,marketsize_max) # triangular distribution of current number of potential users s[k] = calc_marketshare(u[k], marketshare_init) # market share for product r[k] = calc_revenue(u[k], s[k]) # revenue # store values in first row of matrices: rev[t,:] += r usr[t,:] += u sha[t,:] = s #percentiles of the complete revenue row at time t percentiles_rev[t,:] = percentile(rev[t,:], perc_selection) percentiles_usr[t,:] = percentile(usr[t,:], perc_selection) percentiles_sha[t,:] = percentile(sha[t,:], perc_selection) else: # Following years starting with the previous year&#39;s data for k in range(num_simulations): # estimate how much the market has grown: loc = triangular(1, 2, 4) scale = triangular(1, 2, 3) factor = 3 marketgrowth = logist(t, loc, scale, factor) u[k] += u[k] * marketgrowth # apply market growth s[k] = calc_marketshare(u[k], s[k]) + logist(t, 4, 5, 1) # apply market share increase r[k] = calc_revenue(u[k], s[k]) # calculate revenue # store values in following rows of matrices rev[t,:] = rev[t-1,:] + r usr[t,:] += u sha[t,:] = s #percentiles of the complete revenue row at time t percentiles_rev[t,:] = percentile(rev[t,:], perc_selection) percentiles_usr[t,:] = percentile(usr[t,:], perc_selection) percentiles_sha[t,:] = percentile(sha[t,:], perc_selection) . Revenue simulation plots . Having captured all our data, we can now plot it to see how the variable of interest, in this case the revenue of the new product, develops in the next 10 years. . First we print the percentiles to get the numeric data: . df = pd.DataFrame(percentiles_rev, columns=[&#39;5%&#39;,&#39;25%&#39;,&#39;50%&#39;,&#39;75%&#39;,&#39;95%&#39;]) df . 5% 25% 50% 75% 95% . 0 4.409820e+04 | 6.105972e+04 | 7.709451e+04 | 9.426475e+04 | 1.180631e+05 | . 1 4.908791e+05 | 6.756409e+05 | 8.264907e+05 | 9.994591e+05 | 1.235754e+06 | . 2 1.675192e+06 | 2.313494e+06 | 2.817660e+06 | 3.264348e+06 | 4.048545e+06 | . 3 4.206653e+06 | 5.635622e+06 | 6.838129e+06 | 8.115342e+06 | 9.764499e+06 | . 4 8.549564e+06 | 1.154145e+07 | 1.378219e+07 | 1.624323e+07 | 2.006074e+07 | . 5 1.549143e+07 | 2.064907e+07 | 2.464899e+07 | 2.935327e+07 | 3.529438e+07 | . 6 2.529986e+07 | 3.358534e+07 | 3.994820e+07 | 4.737894e+07 | 5.784066e+07 | . 7 3.803804e+07 | 5.056080e+07 | 5.988783e+07 | 7.052721e+07 | 8.471665e+07 | . 8 5.257025e+07 | 7.126223e+07 | 8.484492e+07 | 9.885473e+07 | 1.187698e+08 | . 9 6.901057e+07 | 9.476733e+07 | 1.119965e+08 | 1.314466e+08 | 1.580819e+08 | . Now we can plot these percentiles of revenue in an aggregated form. . x = np.arange(0,10) df.plot(kind=&#39;line&#39;, color=&#39;black&#39;, linewidth=0.2) plt.fill_between(x,df[&#39;25%&#39;].values,df[&#39;75%&#39;].values, color=&#39;grey&#39;, alpha=0.6) plt.fill_between(x,df[&#39;5%&#39;].values,df[&#39;95%&#39;].values, color=&#39;grey&#39;, alpha=0.4) plt.title(&quot;Revenue percentiles over %s years&quot; %num_timesteps) plt.show() . We can also plot the individual &quot;random walks&quot; of the simulation just for fun. . df2=pd.DataFrame(rev) df2.plot(kind=&#39;line&#39;, legend=False, alpha=.03) plt.title(&quot;Revenue random walks over %s years&quot; %num_timesteps) plt.show() . Market share simulation plots . Similarly, let&#39;s plot our simulation results for the market share calculations . df_usr = pd.DataFrame(percentiles_usr, columns=[&#39;5%&#39;,&#39;25%&#39;,&#39;50%&#39;,&#39;75%&#39;,&#39;95%&#39;]) #print(df) # Plot the percentiles market size x = np.arange(0,10) df_usr.plot(kind=&#39;line&#39;, color=&#39;w&#39;) plt.fill_between(x,df_usr[&#39;25%&#39;].values,df_usr[&#39;75%&#39;].values, color=&#39;grey&#39;, alpha=0.6) plt.fill_between(x,df_usr[&#39;5%&#39;].values,df_usr[&#39;95%&#39;].values, color=&#39;grey&#39;, alpha=0.4) plt.title(&quot;Market size percentiles over %s years&quot; %num_timesteps) plt.show() . df2=pd.DataFrame(usr) df2.plot(kind=&#39;line&#39;, legend=False, alpha=.03) plt.title(&quot;Market size random walks over %s years&quot; %num_timesteps) plt.show() . Product revenue and market size distribution . Finally, we can visualize how the revenue is distributed in our simulation for a particular year using histograms. For example, let&#39;s plot the distribution of revenue: . ax1 = plt.subplot(111) ax1 plt.title(&quot;Product revenue, price mode %s €&quot; %price_mode) plt.hist(rev[0], bins=50, range=(0, r.max()), label=&#39;year 1&#39;) plt.hist(rev[2], bins=50, range=(0, r.max()), label=&#39;year 3&#39;) plt.hist(rev[4], bins=50, range=(0, r.max()), label=&#39;year 5&#39;)#axis([0,width,0,height]) plt.hist(rev[6], bins=50, range=(0, r.max()), label=&#39;year 7&#39;) plt.legend() plt.show() . Of course, the farther in the future our model, the wider the distribution, as our model gets more and more uncertain. . We can do the same with the market size distribution: . ax2 = plt.subplot(111) ax2 plt.title(&quot;Market size, price mode %s €&quot; %price_mode) #hist(c, bins=50, range=(0, c.max()), ) plt.hist(usr[0], bins=50, range=(0, u.max()), label=&#39;year 1&#39;) plt.hist(usr[2], bins=50, range=(0, u.max()), label=&#39;year 3&#39;) plt.hist(usr[4], bins=50, range=(0, u.max()), label=&#39;year 5&#39;) plt.hist(usr[6], bins=50, range=(0, u.max()), label=&#39;year 7&#39;) plt.show() . Final remarks . In this post, we saw how we can use Python to model a simple Monte Carlo simulation and how we can plot these results to look at forecasting from a different perspective. .",
            "url": "https://www.arcosdiaz.com/blog/forecasting/simulation/2016/10/15/product-revenue-forecast.html",
            "relUrl": "/forecasting/simulation/2016/10/15/product-revenue-forecast.html",
            "date": " • Oct 15, 2016"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi! I’m a data scientist / neuroscientist / cat lover interested in the complex dynamics of the world we live in and how we can use data and science to solve humanity’s challenges. One idea at a time. . You can find out more about my professional background on LinkedIn . .",
          "url": "https://www.arcosdiaz.com/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://www.arcosdiaz.com/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}