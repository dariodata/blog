<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Personalized Medicine Kaggle Competition | Dario Arcos-Díaz, PhD</title><meta name=keywords content="machine_learning,classification,healthcare"><meta name=description content="This was my approach to the Personalized Healthcare Redefining Cancer Treatment Kaggle competition. The goal of the competition was to create a machine learning algorithm that can classify genetic variations that are present in cancer cells."><meta name=author content="Dario Arcos-Díaz"><link rel=canonical href=https://www.arcosdiaz.com/blog/posts/2017-10-07-personalized-medicine/><link crossorigin=anonymous href=/blog/assets/css/stylesheet.da3211e5ef867bf2b75fd5a6515cfed7195c011e8ab735694e203810a827097b.css integrity="sha256-2jIR5e+Ge/K3X9WmUVz+1xlcAR6KtzVpTiA4EKgnCXs=" rel="preload stylesheet" as=style><link rel=icon href=https://www.arcosdiaz.com/blog/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.arcosdiaz.com/blog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.arcosdiaz.com/blog/favicon-32x32.png><link rel=apple-touch-icon href=https://www.arcosdiaz.com/blog/apple-touch-icon.png><link rel=mask-icon href=https://www.arcosdiaz.com/blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://www.arcosdiaz.com/blog/posts/2017-10-07-personalized-medicine/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-2Q04SCXNNC"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-2Q04SCXNNC")}</script><meta property="og:url" content="https://www.arcosdiaz.com/blog/posts/2017-10-07-personalized-medicine/"><meta property="og:site_name" content="Dario Arcos-Díaz, PhD"><meta property="og:title" content="Personalized Medicine Kaggle Competition"><meta property="og:description" content="This was my approach to the Personalized Healthcare Redefining Cancer Treatment Kaggle competition. The goal of the competition was to create a machine learning algorithm that can classify genetic variations that are present in cancer cells."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2017-10-07T00:00:00+00:00"><meta property="article:modified_time" content="2017-10-07T00:00:00+00:00"><meta property="article:tag" content="Machine_learning"><meta property="article:tag" content="Classification"><meta property="article:tag" content="Healthcare"><meta property="og:image" content="https://www.arcosdiaz.com/images/Kaggle_logo.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://www.arcosdiaz.com/images/Kaggle_logo.png"><meta name=twitter:title content="Personalized Medicine Kaggle Competition"><meta name=twitter:description content="This was my approach to the Personalized Healthcare Redefining Cancer Treatment Kaggle competition. The goal of the competition was to create a machine learning algorithm that can classify genetic variations that are present in cancer cells."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://www.arcosdiaz.com/blog/posts/"},{"@type":"ListItem","position":2,"name":"Personalized Medicine Kaggle Competition","item":"https://www.arcosdiaz.com/blog/posts/2017-10-07-personalized-medicine/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Personalized Medicine Kaggle Competition","name":"Personalized Medicine Kaggle Competition","description":"This was my approach to the Personalized Healthcare Redefining Cancer Treatment Kaggle competition. The goal of the competition was to create a machine learning algorithm that can classify genetic variations that are present in cancer cells.","keywords":["machine_learning","classification","healthcare"],"articleBody":"This notebook describes my approach to the Kaggle competition named in the title. This was a research competition at Kaggle in cooperation with the Memorial Sloan Kettering Cancer Center (MSKCC).\nThe goal of the competition was to create a machine learning algorithm that can classify genetic variations that are present in cancer cells.\nTumors contain cells with many different abnormal mutations in their DNA: some of these mutations are the drivers of tumor growth, whereas others are neutral and considered passengers. Normally, mutations are manually classified into different categories after literature review by clinicians. The dataset made available for this competition contains mutations that have been manually anotated into 9 different categories. The goal is to predict the correct category of mutations in the test set.\nThe model and submission described here got me to the 140th place (out of 1386 teams) or top 11%.\nData The data comes in two different kinds of files: one of them contains information about the genetic variants (training_variants and stage2_test_variants.csv) and the other contains the text (clinical evidence) that was used to manually classify the variants (training_text and stage2_test_text.csv). The training data contains a class target feature corresponding to one of the 9 categories that variants can be classified as.\nNote: the “stage2” prefix of the test files is due to the nature of the competition. There was an initial test set that was used at the beginning of the competition and a “stage2” test set that was used in the final week before the deadline to make the submissions.\nimport os import re import string import pandas as pd import numpy as np train_variant = pd.read_csv(\"input/training_variants\") test_variant = pd.read_csv(\"input/stage2_test_variants.csv\") train_text = pd.read_csv(\"input/training_text\", sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"]) test_text = pd.read_csv(\"input/stage2_test_text.csv\", header=None, skiprows=1, names=[\"ID\", \"Text\"]) train = pd.merge(train_variant, train_text, how='left', on='ID') train_y = train['Class'].values train_x = train.drop('Class', axis=1) train_size=len(train_x) print('Number of training variants: %d' % (train_size)) # number of train data : 3321 test_x = pd.merge(test_variant, test_text, how='left', on='ID') test_size=len(test_x) print('Number of test variants: %d' % (test_size)) # number of test data : 5668 test_index = test_x['ID'].values all_data = np.concatenate((train_x, test_x), axis=0) all_data = pd.DataFrame(all_data) all_data.columns = [\"ID\", \"Gene\", \"Variation\", \"Text\"] Number of training variants: 3321 Number of test variants: 986 all_data.head() .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } The data from the different train and test files is now consolidated into one single file. This is necessary for the correct vectorization of the text data and categorical data later on. We can see that the text information resembles scientific article text. We will process this consolidated file in the next step.\nPreprocessing In order to be able to use this data to train a machine learning model, we need to extract the features from the dataset. This means that we have to transform the text data into vectors that can be understood by an algorithm. As I am not an expert in Natural Language Processing, I applied a modified version of this script published on Kaggle. Afterwards we will have the data in a form that I can use to train a neural network.\n# Pre-processing script by Aly Osama https://www.kaggle.com/alyosama/doc2vec-with-keras-0-77 from nltk.corpus import stopwords from gensim.models.doc2vec import LabeledSentence from gensim import utils def constructLabeledSentences(data): sentences=[] for index, row in data.iteritems(): sentences.append(LabeledSentence(utils.to_unicode(row).split(), ['Text' + '_%s' % str(index)])) return sentences def textClean(text): text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", str(text)) text = text.lower().split() stops = set(stopwords.words(\"english\")) text = [w for w in text if not w in stops] text = \" \".join(text) return(text) def cleanup(text): text = textClean(text) text= text.translate(str.maketrans(\"\",\"\", string.punctuation)) return text allText = all_data['Text'].apply(cleanup) sentences = constructLabeledSentences(allText) allText.head() Using TensorFlow backend. 0 cyclindependent kinases cdks regulate variety ... 1 abstract background nonsmall cell lung cancer ... 2 abstract background nonsmall cell lung cancer ... 3 recent evidence demonstrated acquired uniparen... 4 oncogenic mutations monomeric casitas blineage... Name: Text, dtype: object # Pre-processing script by Aly Osama https://www.kaggle.com/alyosama/doc2vec-with-keras-0-77 # PROCESS TEXT DATA from gensim.models import Doc2Vec Text_INPUT_DIM=300 text_model=None filename='docEmbeddings_5_clean.d2v' if os.path.isfile(filename): text_model = Doc2Vec.load(filename) else: text_model = Doc2Vec(min_count=1, window=5, size=Text_INPUT_DIM, sample=1e-4, negative=5, workers=4, iter=5,seed=1) text_model.build_vocab(sentences) text_model.train(sentences, total_examples=text_model.corpus_count, epochs=text_model.iter) text_model.save(filename) text_train_arrays = np.zeros((train_size, Text_INPUT_DIM)) text_test_arrays = np.zeros((test_size, Text_INPUT_DIM)) for i in range(train_size): text_train_arrays[i] = text_model.docvecs['Text_'+str(i)] j=0 for i in range(train_size,train_size+test_size): text_test_arrays[j] = text_model.docvecs['Text_'+str(i)] j=j+1 print(text_train_arrays[0][:10]) # PROCESS GENE DATA from sklearn.decomposition import TruncatedSVD Gene_INPUT_DIM=25 svd = TruncatedSVD(n_components=25, n_iter=Gene_INPUT_DIM, random_state=12) one_hot_gene = pd.get_dummies(all_data['Gene']) truncated_one_hot_gene = svd.fit_transform(one_hot_gene.values) one_hot_variation = pd.get_dummies(all_data['Variation']) truncated_one_hot_variation = svd.fit_transform(one_hot_variation.values) # ENCODE THE LABELS FROM INTEGERS TO VECTORS from keras.utils import np_utils from sklearn.preprocessing import LabelEncoder label_encoder = LabelEncoder() label_encoder.fit(train_y) encoded_y = np_utils.to_categorical((label_encoder.transform(train_y))) print(encoded_y[0]) We have processed the train labels, as printed above (encoded_y), into vectors that contain 1 in the index of the category that the sample belongs to, and zeros in all other indexes.\nMoreover, the training and test sets are now stacked together to look like this:\ntrain_set=np.hstack((truncated_one_hot_gene[:train_size],truncated_one_hot_variation[:train_size],text_train_arrays)) test_set=np.hstack((truncated_one_hot_gene[train_size:],truncated_one_hot_variation[train_size:],text_test_arrays)) print('Training set shape is: ', train_set.shape) # (3321, 350) print('Test set shape is: ', test_set.shape) # (986, 350) print('Training set example rows:') print(train_set[0][:10]) # [ -2.46065582e-23 -5.21548048e-19 -1.95048372e-20 -2.44542833e-22 # -1.19176742e-22 1.61985461e-25 2.93618862e-25 -6.23860891e-27 # 1.14583929e-28 -1.79996588e-29] print('Test set example rows:') print(test_set[0][:10]) # [ 9.74220189e-33 -1.31484613e-27 4.37925347e-27 -9.88109317e-29 # 7.66365772e-27 6.58254980e-26 -3.74901712e-26 -8.97613299e-26 # -3.75471102e-23 -1.05563623e-21] Our data is now ready to be fed into a machine learning model, in this case, into a neural network in TensorFlow.\nTraining a 4-layer neural network for classification The next step is to create a neural network on TensorFlow. I am using a fully-connected neural network with 4 layers. For details on how the network is built, you can check my TensorFlow MNIST notebook. Wherever necessary, I will explains what adaptations were specifically necessary for this challenge.\nimport math import time import matplotlib.pyplot as plt import numpy as np import pandas as pd import tensorflow as tf from sklearn.model_selection import train_test_split from tensorflow.python.framework import ops %matplotlib inline np.random.seed(1) I found it useful to add the current timestamp to the name of the files that the code will output. This helped me to uniquely identify the results from each run.\ntimestr = time.strftime(\"%Y%m%d-%H%M%S\") dirname = 'output/' # output directory filename = '' I select 20% of the training data to use as a validation set and be able to quantify my variance (watch out for overfitting), as I don’t want to have an algorithm that only works well with this specific training data set that was provided, but one that generalizes as well as possible.\n# split data into training and validation sets X_train, X_val, Y_train, Y_val = train_test_split(train_set, encoded_y, test_size=0.20, random_state=42) X_train, X_val, Y_train, Y_val = X_train.T, X_val.T, Y_train.T, Y_val.T # transpose test set X_test = test_set.T # view data set shapes print('X_train: ', X_train.shape) print('X_val: ', X_val.shape) print('Y_train: ', Y_train.shape) print('Y_val: ', Y_val.shape) print('X_test: ', X_test.shape) X_train: (350, 2656) X_val: (350, 665) Y_train: (9, 2656) Y_val: (9, 665) X_test: (350, 986) Now I define the functions needed to build the neural network.\ndef create_placeholders(n_x, n_y): \"\"\" Creates the placeholders for the tensorflow session. Arguments: n_x -- scalar, dimensions of the input n_y -- scalar, number of classes (from 0 to 8, so -\u003e 9) Returns: X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\" Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\" \"\"\" X = tf.placeholder(tf.float32, shape=(n_x, None), name='X') Y = tf.placeholder(tf.float32, shape=(n_y, None), name='Y') return X, Y def initialize_parameters(): \"\"\" Initializes parameters to build a neural network with tensorflow. Returns: parameters -- a dictionary of tensors containing W and b for every layer \"\"\" tf.set_random_seed(1) W1 = tf.get_variable('W1', [350, X_train.shape[0]], initializer=tf.contrib.layers.xavier_initializer(seed=1)) b1 = tf.get_variable('b1', [350, 1], initializer=tf.zeros_initializer()) W2 = tf.get_variable('W2', [350, 350], initializer=tf.contrib.layers.xavier_initializer(seed=1)) b2 = tf.get_variable('b2', [350, 1], initializer=tf.zeros_initializer()) W3 = tf.get_variable('W3', [100, 350], initializer=tf.contrib.layers.xavier_initializer(seed=1)) b3 = tf.get_variable('b3', [100, 1], initializer=tf.zeros_initializer()) W4 = tf.get_variable('W4', [9, 100], initializer=tf.contrib.layers.xavier_initializer(seed=1)) b4 = tf.get_variable('b4', [9, 1], initializer=tf.zeros_initializer()) parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2, \"W3\": W3, \"b3\": b3, \"W4\": W4, \"b4\": b4} return parameters def forward_propagation(X, parameters, keep_prob1, keep_prob2): \"\"\" Implements the forward propagation for the model: (LINEAR -\u003e RELU)^3 -\u003e LINEAR -\u003e SOFTMAX Arguments: X -- input dataset placeholder, of shape (input size, number of examples) parameters -- python dictionary containing your parameters \"W\" and \"b\" for every layer the shapes are given in initialize_parameters Returns: Z4 -- the output of the last LINEAR unit (logits) \"\"\" # Retrieve the parameters from the dictionary \"parameters\" W1 = parameters['W1'] b1 = parameters['b1'] W2 = parameters['W2'] b2 = parameters['b2'] W3 = parameters['W3'] b3 = parameters['b3'] W4 = parameters['W4'] b4 = parameters['b4'] Z1 = tf.matmul(W1, X) + b1 # Z1 = np.dot(W1, X) + b1 A1 = tf.nn.relu(Z1) # A1 = relu(Z1) A1 = tf.nn.dropout(A1, keep_prob1) # add dropout Z2 = tf.matmul(W2, A1) + b2 # Z2 = np.dot(W2, a1) + b2 A2 = tf.nn.relu(Z2) # A2 = relu(Z2) A2 = tf.nn.dropout(A2, keep_prob2) # add dropout Z3 = tf.matmul(W3, A2) + b3 # Z3 = np.dot(W3,Z2) + b3 A3 = tf.nn.relu(Z3) Z4 = tf.matmul(W4, A3) + b4 return Z4 def compute_cost(Z4, Y): \"\"\" Computes the cost Arguments: Z4 -- output of forward propagation (output of the last LINEAR unit), of shape (n_classes, number of examples) Y -- \"true\" labels vector placeholder, same shape as Z4 Returns: cost - Tensor of the cost function \"\"\" # transpose to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...) logits = tf.transpose(Z4) labels = tf.transpose(Y) cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)) return cost def random_mini_batches(X, Y, mini_batch_size, seed=0): \"\"\" Creates a list of random minibatches from (X, Y) Arguments: X -- input data, of shape (input size, number of examples) Y -- true \"label\" vector, of shape (1, number of examples) mini_batch_size - size of the mini-batches, integer seed Returns: mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y) \"\"\" m = X.shape[1] # number of training examples mini_batches = [] np.random.seed(seed) # Step 1: Shuffle (X, Y) permutation = list(np.random.permutation(m)) shuffled_X = X[:, permutation] shuffled_Y = Y[:, permutation].reshape((Y.shape[0], m)) # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case. num_complete_minibatches = math.floor( m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitioning for k in range(0, num_complete_minibatches): mini_batch_X = shuffled_X[:, k * mini_batch_size: k * mini_batch_size + mini_batch_size] mini_batch_Y = shuffled_Y[:, k * mini_batch_size: k * mini_batch_size + mini_batch_size] mini_batch = (mini_batch_X, mini_batch_Y) mini_batches.append(mini_batch) # Handling the end case (last mini-batch \u003c mini_batch_size) if m % mini_batch_size != 0: mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size: m] mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size: m] mini_batch = (mini_batch_X, mini_batch_Y) mini_batches.append(mini_batch) return mini_batches def predict(X, parameters): W1 = tf.convert_to_tensor(parameters['W1']) b1 = tf.convert_to_tensor(parameters[\"b1\"]) W2 = tf.convert_to_tensor(parameters[\"W2\"]) b2 = tf.convert_to_tensor(parameters[\"b2\"]) W3 = tf.convert_to_tensor(parameters[\"W3\"]) b3 = tf.convert_to_tensor(parameters[\"b3\"]) W4 = tf.convert_to_tensor(parameters[\"W4\"]) b4 = tf.convert_to_tensor(parameters[\"b4\"]) params = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2, \"W3\": W3, \"b3\": b3, \"W4\": W4, \"b4\": b4} x = tf.placeholder(\"float\", [X_train.shape[0], None]) keep_prob1 = tf.placeholder(tf.float32, name='keep_prob1') keep_prob2 = tf.placeholder(tf.float32, name='keep_prob2') z4 = forward_propagation(x, params, keep_prob1, keep_prob2) p = tf.nn.softmax(z4, dim=0) # dim=0 because the classes are on that axis # p = tf.argmax(z4) # this gives only the predicted class as output sess = tf.Session() prediction = sess.run(p, feed_dict={x: X, keep_prob1: 1.0, keep_prob2: 1.0}) return prediction And now I define the model function which is in fact the neural network that we will train afterwards. An important difference with respect to my previous MNIST example is that I added an additional regularization term to the cost function. I used L2 regularization to penalize the weights in all four layers. The bias was not penalized as this is not necessary. The strictness of this penalty was given by a beta constant defined at 0.01.\nWhy use additional regularization? Because this allowed me to decrease the variance, i.e. decrease the difference in performance of the model with the training set compared to the validation set. This produced my best submission in the competition.\ndef model(X_train, Y_train, X_test, Y_test, learning_rate=0.0001, num_epochs=1000, minibatch_size=64, print_cost=True): \"\"\" Implements a four-layer tensorflow neural network: (LINEAR-\u003eRELU)^3-\u003eLINEAR-\u003eSOFTMAX. Arguments: X_train -- training set, of shape (input size, number of training examples) Y_train -- test set, of shape (output size, number of training examples) X_test -- training set, of shape (input size, number of training examples) Y_test -- test set, of shape (output size, number of test examples) learning_rate -- learning rate of the optimization num_epochs -- number of epochs of the optimization loop minibatch_size -- size of a minibatch print_cost -- True to print the cost every 100 epochs Returns: parameters -- parameters learnt by the model. They can then be used to predict. \"\"\" ops.reset_default_graph() # to be able to rerun the model without overwriting tf variables tf.set_random_seed(1) # to keep consistent results seed = 3 # to keep consistent results (n_x, m) = X_train.shape # (n_x: input size, m : number of examples in the train set) n_y = Y_train.shape[0] # n_y : output size costs = [] # To keep track of the cost t0 = time.time() # to mark the start of the training # Create Placeholders of shape (n_x, n_y) X, Y = create_placeholders(n_x, n_y) keep_prob1 = tf.placeholder(tf.float32, name='keep_prob1') # probability to keep a unit during dropout keep_prob2 = tf.placeholder(tf.float32, name='keep_prob2') # Initialize parameters parameters = initialize_parameters() # Forward propagation Z4 = forward_propagation(X, parameters, keep_prob1, keep_prob2) # Cost function cost = compute_cost(Z4, Y) regularizers = tf.nn.l2_loss(parameters['W1']) + tf.nn.l2_loss(parameters['W2']) + tf.nn.l2_loss(parameters['W3']) \\ + tf.nn.l2_loss(parameters['W4']) # add regularization term beta = 0.01 # regularization constant cost = tf.reduce_mean(cost + beta * regularizers) # cost with regularization # Backpropagation: Define the tensorflow AdamOptimizer. optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Initialize all the variables init = tf.global_variables_initializer() # Start the session to compute the tensorflow graph with tf.Session() as sess: # Run the initialization sess.run(init) # Do the training loop for epoch in range(num_epochs): epoch_cost = 0. # Defines a cost related to an epoch num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set seed = seed + 1 minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed) for minibatch in minibatches: # Select a minibatch (minibatch_X, minibatch_Y) = minibatch # Run the session to execute the \"optimizer\" and the \"cost\" _, minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y, keep_prob1: 0.7, keep_prob2: 0.5}) epoch_cost += minibatch_cost / num_minibatches # Print the cost every epoch if print_cost == True and epoch % 100 == 0: print(\"Cost after epoch {}: {:f}\".format(epoch, epoch_cost)) if print_cost == True and epoch % 5 == 0: costs.append(epoch_cost) # lets save the parameters in a variable parameters = sess.run(parameters) print(\"Parameters have been trained!\") # Calculate the correct predictions correct_prediction = tf.equal(tf.argmax(Z4), tf.argmax(Y)) # Calculate accuracy on the test set accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) train_cost = cost.eval({X: X_train, Y: Y_train, keep_prob1: 1.0, keep_prob2: 1.0}) test_cost = cost.eval({X: X_test, Y: Y_test, keep_prob1: 1.0, keep_prob2: 1.0}) train_accuracy = accuracy.eval({X: X_train, Y: Y_train, keep_prob1: 1.0, keep_prob2: 1.0}) test_accuracy = accuracy.eval({X: X_test, Y: Y_test, keep_prob1: 1.0, keep_prob2: 1.0}) print('Finished training in %s s' % (time.time() - t0)) print(\"Train Cost:\", train_cost) print(\"Test Cost:\", test_cost) print(\"Train Accuracy:\", train_accuracy) print(\"Test Accuracy:\", test_accuracy) # plot the cost plt.plot(np.squeeze(costs)) plt.ylabel('cost') plt.xlabel('iterations (per fives)') plt.title(\"Learning rate = {}, beta = {},\\n\" \"test cost = {:.6f}, test accuracy = {:.6f}\".format(learning_rate, beta, test_cost, test_accuracy)) global filename filename = timestr + '_NN4Lstage2_lr_{}_beta_{}_cost_{:.2f}-{:.2f}_acc_{:.2f}-{:.2f}'.format( learning_rate, beta, train_cost, test_cost, train_accuracy, test_accuracy) plt.savefig(dirname + filename + '.png') return parameters Note that the model function will return the learned parameters from the network and additionally will plot the cost after each epoch. The plot is also saved as a file that includes the timestamp as well as the learning rate, beta, cost and accuracy information for this particular run.\nNow it’s time to train the model using the train and validation data:\n# train the model and get learned parameters parameters = model(X_train, Y_train, X_val, Y_val) Cost after epoch 0: 6.607861 Cost after epoch 100: 1.389869 Cost after epoch 200: 0.988806 Cost after epoch 300: 0.882713 Cost after epoch 400: 0.833693 Cost after epoch 500: 0.811457 Cost after epoch 600: 0.793379 Cost after epoch 700: 0.773927 Cost after epoch 800: 0.762247 Cost after epoch 900: 0.767449 Parameters have been trained! Finished training in 498.4203100204468 s Train Cost: 0.665462 Test Cost: 1.74987 Train Accuracy: 0.979292 Test Accuracy: 0.643609 From my validation results we can observe that the network learned nicely. However, the final cost of the training data was 0.665462, where as the validation data had a final cost of 1.74987. This is a large difference and an indication that the model is overfitting. Moreover the accuracy (defined here as the fraction of correct predictions) is very high (97.9%) for the training data and only 64.3% for the validation set. Another indication that the model is overfitting even though I have used both dropout and L2 regularization to counteract this.\nMake predictions We use the learned parameteres to make a prediction on the test data.\n# use learned parameters to make prediction on test data prediction = predict(X_test, parameters) Let’s look at an example of a prediction. As we can see below, the prediction consists of the probabilities of the entry belongin to each of the nine different categories (this was the format needed for this competition).\nprediction[:,0] array([ 0.36503336, 0.21219006, 0.01297534, 0.14676626, 0.08375936, 0.09217557, 0.02737238, 0.03150512, 0.02822249], dtype=float32) prediction.shape (9, 986) All we have to do now is create a submission .csv file to save our prediction results.\n# create submission file submission = pd.DataFrame(prediction.T) submission['id'] = test_index submission.columns = ['class1', 'class2', 'class3', 'class4', 'class5', 'class6', 'class7', 'class8', 'class9', 'id'] submission.to_csv(dirname + filename + '.csv', index=False) Results interpretation Using this neural network model, my submission to Kaggle yielded following results:\nPublic score (based on a portion of the test data by Kaggle to provide an indication of performance during the competition): Loss = 1.69148 Private score (based on a different portion of the test data by Kaggle to provide the final score at the end of the competition): Loss = 2.74500 The discrepancy between these two scores further shows that overfitting is an issue in working with this data in a neural network model. My model could benefit from increasing the training data and a higher regularization.\n","wordCount":"2991","inLanguage":"en","image":"https://www.arcosdiaz.com/images/Kaggle_logo.png","datePublished":"2017-10-07T00:00:00Z","dateModified":"2017-10-07T00:00:00Z","author":{"@type":"Person","name":"Dario Arcos-Díaz"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.arcosdiaz.com/blog/posts/2017-10-07-personalized-medicine/"},"publisher":{"@type":"Organization","name":"Dario Arcos-Díaz, PhD","logo":{"@type":"ImageObject","url":"https://www.arcosdiaz.com/blog/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://www.arcosdiaz.com/blog/ accesskey=h title="Dario Arcos-Díaz, PhD (Alt + H)">Dario Arcos-Díaz, PhD</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://www.arcosdiaz.com/blog/about/ title=About><span>About</span></a></li><li><a href=https://www.arcosdiaz.com/blog/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://www.arcosdiaz.com/blog/>Home</a>&nbsp;»&nbsp;<a href=https://www.arcosdiaz.com/blog/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Personalized Medicine Kaggle Competition</h1><div class=post-description>This was my approach to the Personalized Healthcare Redefining Cancer Treatment Kaggle competition. The goal of the competition was to create a machine learning algorithm that can classify genetic variations that are present in cancer cells.</div><div class=post-meta><span title='2017-10-07 00:00:00 +0000 UTC'>October 7, 2017</span>&nbsp;·&nbsp;<span>15 min</span>&nbsp;·&nbsp;<span>Dario Arcos-Díaz</span></div></header><figure class=entry-cover><img loading=eager src=https://www.arcosdiaz.com/images/Kaggle_logo.png alt></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#data aria-label=Data>Data</a></li><li><a href=#preprocessing aria-label=Preprocessing>Preprocessing</a></li><li><a href=#training-a-4-layer-neural-network-for-classification aria-label="Training a 4-layer neural network for classification">Training a 4-layer neural network for classification</a></li><li><a href=#make-predictions aria-label="Make predictions">Make predictions</a></li><li><a href=#results-interpretation aria-label="Results interpretation">Results interpretation</a></li></ul></div></details></div><div class=post-content><p>This notebook describes my approach to the <a href=https://www.kaggle.com/c/msk-redefining-cancer-treatment>Kaggle competition</a> named in the title. This was a research competition at Kaggle in cooperation with the Memorial Sloan Kettering Cancer Center (MSKCC).</p><p>The goal of the competition was to create a machine learning algorithm that can classify genetic variations that are present in cancer cells.</p><p>Tumors contain cells with many different abnormal mutations in their DNA: some of these mutations are the drivers of tumor growth, whereas others are neutral and considered <em>passengers</em>. Normally, mutations are manually classified into different categories after literature review by clinicians. The dataset made available for this competition contains mutations that have been manually anotated into 9 different categories. The goal is to predict the correct category of mutations in the test set.</p><p>The model and submission described here got me to the 140th place (out of 1386 teams) or top 11%.</p><h2 id=data>Data<a hidden class=anchor aria-hidden=true href=#data>#</a></h2><p>The data comes in two different kinds of files: one of them contains information about the genetic variants (<em>training_variants</em> and <em>stage2_test_variants.csv</em>) and the other contains the text (clinical evidence) that was used to manually classify the variants (<em>training_text</em> and <em>stage2_test_text.csv</em>). The training data contains a class target feature corresponding to one of the 9 categories that variants can be classified as.</p><p><em>Note: the &ldquo;stage2&rdquo; prefix of the test files is due to the nature of the competition. There was an initial test set that was used at the beginning of the competition and a &ldquo;stage2&rdquo; test set that was used in the final week before the deadline to make the submissions.</em></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> re
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> string
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span>train_variant <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(<span style=color:#e6db74>&#34;input/training_variants&#34;</span>)
</span></span><span style=display:flex><span>test_variant <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(<span style=color:#e6db74>&#34;input/stage2_test_variants.csv&#34;</span>)
</span></span><span style=display:flex><span>train_text <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(<span style=color:#e6db74>&#34;input/training_text&#34;</span>, sep<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;\|\|&#34;</span>, engine<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;python&#39;</span>, header<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, skiprows<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, names<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;ID&#34;</span>,<span style=color:#e6db74>&#34;Text&#34;</span>])
</span></span><span style=display:flex><span>test_text <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(<span style=color:#e6db74>&#34;input/stage2_test_text.csv&#34;</span>, header<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, skiprows<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, names<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;ID&#34;</span>, <span style=color:#e6db74>&#34;Text&#34;</span>])
</span></span><span style=display:flex><span>train <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>merge(train_variant, train_text, how<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;left&#39;</span>, on<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;ID&#39;</span>)
</span></span><span style=display:flex><span>train_y <span style=color:#f92672>=</span> train[<span style=color:#e6db74>&#39;Class&#39;</span>]<span style=color:#f92672>.</span>values
</span></span><span style=display:flex><span>train_x <span style=color:#f92672>=</span> train<span style=color:#f92672>.</span>drop(<span style=color:#e6db74>&#39;Class&#39;</span>, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>train_size<span style=color:#f92672>=</span>len(train_x)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;Number of training variants: </span><span style=color:#e6db74>%d</span><span style=color:#e6db74>&#39;</span> <span style=color:#f92672>%</span> (train_size))
</span></span><span style=display:flex><span><span style=color:#75715e># number of train data : 3321</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>test_x <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>merge(test_variant, test_text, how<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;left&#39;</span>, on<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;ID&#39;</span>)
</span></span><span style=display:flex><span>test_size<span style=color:#f92672>=</span>len(test_x)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;Number of test variants: </span><span style=color:#e6db74>%d</span><span style=color:#e6db74>&#39;</span> <span style=color:#f92672>%</span> (test_size))
</span></span><span style=display:flex><span><span style=color:#75715e># number of test data : 5668</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>test_index <span style=color:#f92672>=</span> test_x[<span style=color:#e6db74>&#39;ID&#39;</span>]<span style=color:#f92672>.</span>values
</span></span><span style=display:flex><span>all_data <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>concatenate((train_x, test_x), axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>all_data <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame(all_data)
</span></span><span style=display:flex><span>all_data<span style=color:#f92672>.</span>columns <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;ID&#34;</span>, <span style=color:#e6db74>&#34;Gene&#34;</span>, <span style=color:#e6db74>&#34;Variation&#34;</span>, <span style=color:#e6db74>&#34;Text&#34;</span>]
</span></span></code></pre></div><pre><code>Number of training variants: 3321
Number of test variants: 986
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span>all_data<span style=color:#f92672>.</span>head()
</span></span></code></pre></div><pre><code>.dataframe thead th {
    text-align: left;
}

.dataframe tbody tr th {
    vertical-align: top;
}
</code></pre><p></p><p>The data from the different train and test files is now consolidated into one single file. This is necessary for the correct vectorization of the text data and categorical data later on. We can see that the text information resembles scientific article text. We will process this consolidated file in the next step.</p><h2 id=preprocessing>Preprocessing<a hidden class=anchor aria-hidden=true href=#preprocessing>#</a></h2><p>In order to be able to use this data to train a machine learning model, we need to extract the features from the dataset. This means that we have to transform the text data into vectors that can be understood by an algorithm. As I am not an expert in Natural Language Processing, I applied a modified version of <a href=https://www.kaggle.com/alyosama/doc2vec-with-keras-0-77>this script published on Kaggle.</a> Afterwards we will have the data in a form that I can use to train a neural network.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Pre-processing script by Aly Osama https://www.kaggle.com/alyosama/doc2vec-with-keras-0-77</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> nltk.corpus <span style=color:#f92672>import</span> stopwords
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> gensim.models.doc2vec <span style=color:#f92672>import</span> LabeledSentence
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> gensim <span style=color:#f92672>import</span> utils
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>constructLabeledSentences</span>(data):
</span></span><span style=display:flex><span>    sentences<span style=color:#f92672>=</span>[]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> index, row <span style=color:#f92672>in</span> data<span style=color:#f92672>.</span>iteritems():
</span></span><span style=display:flex><span>        sentences<span style=color:#f92672>.</span>append(LabeledSentence(utils<span style=color:#f92672>.</span>to_unicode(row)<span style=color:#f92672>.</span>split(), [<span style=color:#e6db74>&#39;Text&#39;</span> <span style=color:#f92672>+</span> <span style=color:#e6db74>&#39;_</span><span style=color:#e6db74>%s</span><span style=color:#e6db74>&#39;</span> <span style=color:#f92672>%</span> str(index)]))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> sentences
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>textClean</span>(text):
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> re<span style=color:#f92672>.</span>sub(<span style=color:#e6db74>r</span><span style=color:#e6db74>&#34;[^A-Za-z0-9^,!.\/&#39;+-=]&#34;</span>, <span style=color:#e6db74>&#34; &#34;</span>, str(text))
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> text<span style=color:#f92672>.</span>lower()<span style=color:#f92672>.</span>split()
</span></span><span style=display:flex><span>    stops <span style=color:#f92672>=</span> set(stopwords<span style=color:#f92672>.</span>words(<span style=color:#e6db74>&#34;english&#34;</span>))
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> [w <span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> text <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> w <span style=color:#f92672>in</span> stops]    
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34; &#34;</span><span style=color:#f92672>.</span>join(text)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span>(text)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>cleanup</span>(text):
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> textClean(text)
</span></span><span style=display:flex><span>    text<span style=color:#f92672>=</span> text<span style=color:#f92672>.</span>translate(str<span style=color:#f92672>.</span>maketrans(<span style=color:#e6db74>&#34;&#34;</span>,<span style=color:#e6db74>&#34;&#34;</span>, string<span style=color:#f92672>.</span>punctuation))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> text
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>allText <span style=color:#f92672>=</span> all_data[<span style=color:#e6db74>&#39;Text&#39;</span>]<span style=color:#f92672>.</span>apply(cleanup)
</span></span><span style=display:flex><span>sentences <span style=color:#f92672>=</span> constructLabeledSentences(allText)
</span></span><span style=display:flex><span>allText<span style=color:#f92672>.</span>head()
</span></span></code></pre></div><pre><code>Using TensorFlow backend.





0    cyclindependent kinases cdks regulate variety ...
1    abstract background nonsmall cell lung cancer ...
2    abstract background nonsmall cell lung cancer ...
3    recent evidence demonstrated acquired uniparen...
4    oncogenic mutations monomeric casitas blineage...
Name: Text, dtype: object
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Pre-processing script by Aly Osama https://www.kaggle.com/alyosama/doc2vec-with-keras-0-77</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># PROCESS TEXT DATA</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> gensim.models <span style=color:#f92672>import</span> Doc2Vec
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Text_INPUT_DIM<span style=color:#f92672>=</span><span style=color:#ae81ff>300</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>text_model<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>filename<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;docEmbeddings_5_clean.d2v&#39;</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>isfile(filename):
</span></span><span style=display:flex><span>    text_model <span style=color:#f92672>=</span> Doc2Vec<span style=color:#f92672>.</span>load(filename)
</span></span><span style=display:flex><span><span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>    text_model <span style=color:#f92672>=</span> Doc2Vec(min_count<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, window<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>, size<span style=color:#f92672>=</span>Text_INPUT_DIM, sample<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-4</span>, negative<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>, workers<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>, iter<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>,seed<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    text_model<span style=color:#f92672>.</span>build_vocab(sentences)
</span></span><span style=display:flex><span>    text_model<span style=color:#f92672>.</span>train(sentences, total_examples<span style=color:#f92672>=</span>text_model<span style=color:#f92672>.</span>corpus_count, epochs<span style=color:#f92672>=</span>text_model<span style=color:#f92672>.</span>iter)
</span></span><span style=display:flex><span>    text_model<span style=color:#f92672>.</span>save(filename)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>text_train_arrays <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((train_size, Text_INPUT_DIM))
</span></span><span style=display:flex><span>text_test_arrays <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((test_size, Text_INPUT_DIM))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(train_size):
</span></span><span style=display:flex><span>    text_train_arrays[i] <span style=color:#f92672>=</span> text_model<span style=color:#f92672>.</span>docvecs[<span style=color:#e6db74>&#39;Text_&#39;</span><span style=color:#f92672>+</span>str(i)]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>j<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(train_size,train_size<span style=color:#f92672>+</span>test_size):
</span></span><span style=display:flex><span>    text_test_arrays[j] <span style=color:#f92672>=</span> text_model<span style=color:#f92672>.</span>docvecs[<span style=color:#e6db74>&#39;Text_&#39;</span><span style=color:#f92672>+</span>str(i)]
</span></span><span style=display:flex><span>    j<span style=color:#f92672>=</span>j<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>print(text_train_arrays[<span style=color:#ae81ff>0</span>][:<span style=color:#ae81ff>10</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># PROCESS GENE DATA</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.decomposition <span style=color:#f92672>import</span> TruncatedSVD
</span></span><span style=display:flex><span>Gene_INPUT_DIM<span style=color:#f92672>=</span><span style=color:#ae81ff>25</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>svd <span style=color:#f92672>=</span> TruncatedSVD(n_components<span style=color:#f92672>=</span><span style=color:#ae81ff>25</span>, n_iter<span style=color:#f92672>=</span>Gene_INPUT_DIM, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>12</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>one_hot_gene <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>get_dummies(all_data[<span style=color:#e6db74>&#39;Gene&#39;</span>])
</span></span><span style=display:flex><span>truncated_one_hot_gene <span style=color:#f92672>=</span> svd<span style=color:#f92672>.</span>fit_transform(one_hot_gene<span style=color:#f92672>.</span>values)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>one_hot_variation <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>get_dummies(all_data[<span style=color:#e6db74>&#39;Variation&#39;</span>])
</span></span><span style=display:flex><span>truncated_one_hot_variation <span style=color:#f92672>=</span> svd<span style=color:#f92672>.</span>fit_transform(one_hot_variation<span style=color:#f92672>.</span>values)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ENCODE THE LABELS FROM INTEGERS TO VECTORS</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> keras.utils <span style=color:#f92672>import</span> np_utils
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.preprocessing <span style=color:#f92672>import</span> LabelEncoder
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>label_encoder <span style=color:#f92672>=</span> LabelEncoder()
</span></span><span style=display:flex><span>label_encoder<span style=color:#f92672>.</span>fit(train_y)
</span></span><span style=display:flex><span>encoded_y <span style=color:#f92672>=</span> np_utils<span style=color:#f92672>.</span>to_categorical((label_encoder<span style=color:#f92672>.</span>transform(train_y)))
</span></span><span style=display:flex><span>print(encoded_y[<span style=color:#ae81ff>0</span>])
</span></span></code></pre></div><p>We have processed the train labels, as printed above (<code>encoded_y</code>), into vectors that contain 1 in the index of the category that the sample belongs to, and zeros in all other indexes.</p><p>Moreover, the training and test sets are now stacked together to look like this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span>train_set<span style=color:#f92672>=</span>np<span style=color:#f92672>.</span>hstack((truncated_one_hot_gene[:train_size],truncated_one_hot_variation[:train_size],text_train_arrays))
</span></span><span style=display:flex><span>test_set<span style=color:#f92672>=</span>np<span style=color:#f92672>.</span>hstack((truncated_one_hot_gene[train_size:],truncated_one_hot_variation[train_size:],text_test_arrays))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;Training set shape is: &#39;</span>, train_set<span style=color:#f92672>.</span>shape)  <span style=color:#75715e># (3321, 350)</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;Test set shape is: &#39;</span>, test_set<span style=color:#f92672>.</span>shape)  <span style=color:#75715e># (986, 350)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;Training set example rows:&#39;</span>)
</span></span><span style=display:flex><span>print(train_set[<span style=color:#ae81ff>0</span>][:<span style=color:#ae81ff>10</span>])
</span></span><span style=display:flex><span><span style=color:#75715e># [ -2.46065582e-23  -5.21548048e-19  -1.95048372e-20  -2.44542833e-22</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#  -1.19176742e-22   1.61985461e-25   2.93618862e-25  -6.23860891e-27</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#   1.14583929e-28  -1.79996588e-29]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;Test set example rows:&#39;</span>)
</span></span><span style=display:flex><span>print(test_set[<span style=color:#ae81ff>0</span>][:<span style=color:#ae81ff>10</span>])
</span></span><span style=display:flex><span><span style=color:#75715e># [  9.74220189e-33  -1.31484613e-27   4.37925347e-27  -9.88109317e-29</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#    7.66365772e-27   6.58254980e-26  -3.74901712e-26  -8.97613299e-26</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#   -3.75471102e-23  -1.05563623e-21]</span>
</span></span></code></pre></div><p>Our data is now ready to be fed into a machine learning model, in this case, into a neural network in TensorFlow.</p><h2 id=training-a-4-layer-neural-network-for-classification>Training a 4-layer neural network for classification<a hidden class=anchor aria-hidden=true href=#training-a-4-layer-neural-network-for-classification>#</a></h2><p>The next step is to create a neural network on TensorFlow. I am using a fully-connected neural network with 4 layers. For details on how the network is built, you can check my <a href=https://github.com/dariodata/TensorFlow-MNIST/blob/master/TensorFlow-MNIST.ipynb>TensorFlow MNIST notebook</a>. Wherever necessary, I will explains what adaptations were specifically necessary for this challenge.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> math
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> time
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> tensorflow <span style=color:#66d9ef>as</span> tf
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.model_selection <span style=color:#f92672>import</span> train_test_split
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.python.framework <span style=color:#f92672>import</span> ops
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>%</span>matplotlib inline
</span></span><span style=display:flex><span>np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>seed(<span style=color:#ae81ff>1</span>)
</span></span></code></pre></div><p>I found it useful to add the current timestamp to the name of the files that the code will output. This helped me to uniquely identify the results from each run.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span>timestr <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>strftime(<span style=color:#e6db74>&#34;%Y%m</span><span style=color:#e6db74>%d</span><span style=color:#e6db74>-%H%M%S&#34;</span>)
</span></span><span style=display:flex><span>dirname <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;output/&#39;</span>  <span style=color:#75715e># output directory</span>
</span></span><span style=display:flex><span>filename <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;&#39;</span>
</span></span></code></pre></div><p>I select 20% of the training data to use as a validation set and be able to quantify my variance (watch out for overfitting), as I don&rsquo;t want to have an algorithm that only works well with this specific training data set that was provided, but one that generalizes as well as possible.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># split data into training and validation sets</span>
</span></span><span style=display:flex><span>X_train, X_val, Y_train, Y_val <span style=color:#f92672>=</span> train_test_split(train_set, encoded_y, test_size<span style=color:#f92672>=</span><span style=color:#ae81ff>0.20</span>, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>42</span>)
</span></span><span style=display:flex><span>X_train, X_val, Y_train, Y_val <span style=color:#f92672>=</span> X_train<span style=color:#f92672>.</span>T, X_val<span style=color:#f92672>.</span>T, Y_train<span style=color:#f92672>.</span>T, Y_val<span style=color:#f92672>.</span>T
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># transpose test set</span>
</span></span><span style=display:flex><span>X_test <span style=color:#f92672>=</span> test_set<span style=color:#f92672>.</span>T
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># view data set shapes</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;X_train: &#39;</span>, X_train<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;X_val: &#39;</span>, X_val<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;Y_train: &#39;</span>, Y_train<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;Y_val: &#39;</span>, Y_val<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;X_test: &#39;</span>, X_test<span style=color:#f92672>.</span>shape)
</span></span></code></pre></div><pre><code>X_train:  (350, 2656)
X_val:  (350, 665)
Y_train:  (9, 2656)
Y_val:  (9, 665)
X_test:  (350, 986)
</code></pre><p>Now I define the functions needed to build the neural network.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>create_placeholders</span>(n_x, n_y):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Creates the placeholders for the tensorflow session.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Arguments:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    n_x -- scalar, dimensions of the input
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    n_y -- scalar, number of classes (from 0 to 8, so -&gt; 9)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    X -- placeholder for the data input, of shape [n_x, None] and dtype &#34;float&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Y -- placeholder for the input labels, of shape [n_y, None] and dtype &#34;float&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    X <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>placeholder(tf<span style=color:#f92672>.</span>float32, shape<span style=color:#f92672>=</span>(n_x, <span style=color:#66d9ef>None</span>), name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;X&#39;</span>)
</span></span><span style=display:flex><span>    Y <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>placeholder(tf<span style=color:#f92672>.</span>float32, shape<span style=color:#f92672>=</span>(n_y, <span style=color:#66d9ef>None</span>), name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Y&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> X, Y
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>initialize_parameters</span>():
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Initializes parameters to build a neural network with tensorflow.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    parameters -- a dictionary of tensors containing W and b for every layer
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    tf<span style=color:#f92672>.</span>set_random_seed(<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    W1 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>get_variable(<span style=color:#e6db74>&#39;W1&#39;</span>, [<span style=color:#ae81ff>350</span>, X_train<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]], initializer<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>contrib<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>xavier_initializer(seed<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>    b1 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>get_variable(<span style=color:#e6db74>&#39;b1&#39;</span>, [<span style=color:#ae81ff>350</span>, <span style=color:#ae81ff>1</span>], initializer<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>zeros_initializer())
</span></span><span style=display:flex><span>    W2 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>get_variable(<span style=color:#e6db74>&#39;W2&#39;</span>, [<span style=color:#ae81ff>350</span>, <span style=color:#ae81ff>350</span>], initializer<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>contrib<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>xavier_initializer(seed<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>    b2 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>get_variable(<span style=color:#e6db74>&#39;b2&#39;</span>, [<span style=color:#ae81ff>350</span>, <span style=color:#ae81ff>1</span>], initializer<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>zeros_initializer())
</span></span><span style=display:flex><span>    W3 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>get_variable(<span style=color:#e6db74>&#39;W3&#39;</span>, [<span style=color:#ae81ff>100</span>, <span style=color:#ae81ff>350</span>], initializer<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>contrib<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>xavier_initializer(seed<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>    b3 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>get_variable(<span style=color:#e6db74>&#39;b3&#39;</span>, [<span style=color:#ae81ff>100</span>, <span style=color:#ae81ff>1</span>], initializer<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>zeros_initializer())
</span></span><span style=display:flex><span>    W4 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>get_variable(<span style=color:#e6db74>&#39;W4&#39;</span>, [<span style=color:#ae81ff>9</span>, <span style=color:#ae81ff>100</span>], initializer<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>contrib<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>xavier_initializer(seed<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>    b4 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>get_variable(<span style=color:#e6db74>&#39;b4&#39;</span>, [<span style=color:#ae81ff>9</span>, <span style=color:#ae81ff>1</span>], initializer<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>zeros_initializer())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    parameters <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#34;W1&#34;</span>: W1,
</span></span><span style=display:flex><span>                  <span style=color:#e6db74>&#34;b1&#34;</span>: b1,
</span></span><span style=display:flex><span>                  <span style=color:#e6db74>&#34;W2&#34;</span>: W2,
</span></span><span style=display:flex><span>                  <span style=color:#e6db74>&#34;b2&#34;</span>: b2,
</span></span><span style=display:flex><span>                  <span style=color:#e6db74>&#34;W3&#34;</span>: W3,
</span></span><span style=display:flex><span>                  <span style=color:#e6db74>&#34;b3&#34;</span>: b3,
</span></span><span style=display:flex><span>                  <span style=color:#e6db74>&#34;W4&#34;</span>: W4,
</span></span><span style=display:flex><span>                  <span style=color:#e6db74>&#34;b4&#34;</span>: b4}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> parameters
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward_propagation</span>(X, parameters, keep_prob1, keep_prob2):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Implements the forward propagation for the model: (LINEAR -&gt; RELU)^3 -&gt; LINEAR -&gt; SOFTMAX
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Arguments:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    X -- input dataset placeholder, of shape (input size, number of examples)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    parameters -- python dictionary containing your parameters &#34;W&#34; and &#34;b&#34; for every layer
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                  the shapes are given in initialize_parameters
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Z4 -- the output of the last LINEAR unit (logits)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Retrieve the parameters from the dictionary &#34;parameters&#34;</span>
</span></span><span style=display:flex><span>    W1 <span style=color:#f92672>=</span> parameters[<span style=color:#e6db74>&#39;W1&#39;</span>]
</span></span><span style=display:flex><span>    b1 <span style=color:#f92672>=</span> parameters[<span style=color:#e6db74>&#39;b1&#39;</span>]
</span></span><span style=display:flex><span>    W2 <span style=color:#f92672>=</span> parameters[<span style=color:#e6db74>&#39;W2&#39;</span>]
</span></span><span style=display:flex><span>    b2 <span style=color:#f92672>=</span> parameters[<span style=color:#e6db74>&#39;b2&#39;</span>]
</span></span><span style=display:flex><span>    W3 <span style=color:#f92672>=</span> parameters[<span style=color:#e6db74>&#39;W3&#39;</span>]
</span></span><span style=display:flex><span>    b3 <span style=color:#f92672>=</span> parameters[<span style=color:#e6db74>&#39;b3&#39;</span>]
</span></span><span style=display:flex><span>    W4 <span style=color:#f92672>=</span> parameters[<span style=color:#e6db74>&#39;W4&#39;</span>]
</span></span><span style=display:flex><span>    b4 <span style=color:#f92672>=</span> parameters[<span style=color:#e6db74>&#39;b4&#39;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    Z1 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>matmul(W1, X) <span style=color:#f92672>+</span> b1  <span style=color:#75715e># Z1 = np.dot(W1, X) + b1</span>
</span></span><span style=display:flex><span>    A1 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>relu(Z1)  <span style=color:#75715e># A1 = relu(Z1)</span>
</span></span><span style=display:flex><span>    A1 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>dropout(A1, keep_prob1)  <span style=color:#75715e># add dropout</span>
</span></span><span style=display:flex><span>    Z2 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>matmul(W2, A1) <span style=color:#f92672>+</span> b2  <span style=color:#75715e># Z2 = np.dot(W2, a1) + b2</span>
</span></span><span style=display:flex><span>    A2 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>relu(Z2)  <span style=color:#75715e># A2 = relu(Z2)</span>
</span></span><span style=display:flex><span>    A2 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>dropout(A2, keep_prob2)  <span style=color:#75715e># add dropout</span>
</span></span><span style=display:flex><span>    Z3 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>matmul(W3, A2) <span style=color:#f92672>+</span> b3  <span style=color:#75715e># Z3 = np.dot(W3,Z2) + b3</span>
</span></span><span style=display:flex><span>    A3 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>relu(Z3)
</span></span><span style=display:flex><span>    Z4 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>matmul(W4, A3) <span style=color:#f92672>+</span> b4
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> Z4
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>compute_cost</span>(Z4, Y):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Computes the cost
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Arguments:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Z4 -- output of forward propagation (output of the last LINEAR unit), of shape (n_classes, number of examples)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Y -- &#34;true&#34; labels vector placeholder, same shape as Z4
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    cost - Tensor of the cost function
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># transpose to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)</span>
</span></span><span style=display:flex><span>    logits <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>transpose(Z4)
</span></span><span style=display:flex><span>    labels <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>transpose(Y)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    cost <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>reduce_mean(tf<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>softmax_cross_entropy_with_logits(logits<span style=color:#f92672>=</span>logits, labels<span style=color:#f92672>=</span>labels))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> cost
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>random_mini_batches</span>(X, Y, mini_batch_size, seed<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Creates a list of random minibatches from (X, Y)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Arguments:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    X -- input data, of shape (input size, number of examples)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Y -- true &#34;label&#34; vector, of shape (1, number of examples)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    mini_batch_size - size of the mini-batches, integer
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    seed
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    m <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]  <span style=color:#75715e># number of training examples</span>
</span></span><span style=display:flex><span>    mini_batches <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>seed(seed)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Step 1: Shuffle (X, Y)</span>
</span></span><span style=display:flex><span>    permutation <span style=color:#f92672>=</span> list(np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>permutation(m))
</span></span><span style=display:flex><span>    shuffled_X <span style=color:#f92672>=</span> X[:, permutation]
</span></span><span style=display:flex><span>    shuffled_Y <span style=color:#f92672>=</span> Y[:, permutation]<span style=color:#f92672>.</span>reshape((Y<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>], m))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.</span>
</span></span><span style=display:flex><span>    num_complete_minibatches <span style=color:#f92672>=</span> math<span style=color:#f92672>.</span>floor(
</span></span><span style=display:flex><span>        m <span style=color:#f92672>/</span> mini_batch_size)  <span style=color:#75715e># number of mini batches of size mini_batch_size in your partitioning</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> k <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>0</span>, num_complete_minibatches):
</span></span><span style=display:flex><span>        mini_batch_X <span style=color:#f92672>=</span> shuffled_X[:, k <span style=color:#f92672>*</span> mini_batch_size: k <span style=color:#f92672>*</span> mini_batch_size <span style=color:#f92672>+</span> mini_batch_size]
</span></span><span style=display:flex><span>        mini_batch_Y <span style=color:#f92672>=</span> shuffled_Y[:, k <span style=color:#f92672>*</span> mini_batch_size: k <span style=color:#f92672>*</span> mini_batch_size <span style=color:#f92672>+</span> mini_batch_size]
</span></span><span style=display:flex><span>        mini_batch <span style=color:#f92672>=</span> (mini_batch_X, mini_batch_Y)
</span></span><span style=display:flex><span>        mini_batches<span style=color:#f92672>.</span>append(mini_batch)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Handling the end case (last mini-batch &lt; mini_batch_size)</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> m <span style=color:#f92672>%</span> mini_batch_size <span style=color:#f92672>!=</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>        mini_batch_X <span style=color:#f92672>=</span> shuffled_X[:, num_complete_minibatches <span style=color:#f92672>*</span> mini_batch_size: m]
</span></span><span style=display:flex><span>        mini_batch_Y <span style=color:#f92672>=</span> shuffled_Y[:, num_complete_minibatches <span style=color:#f92672>*</span> mini_batch_size: m]
</span></span><span style=display:flex><span>        mini_batch <span style=color:#f92672>=</span> (mini_batch_X, mini_batch_Y)
</span></span><span style=display:flex><span>        mini_batches<span style=color:#f92672>.</span>append(mini_batch)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> mini_batches
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>predict</span>(X, parameters):
</span></span><span style=display:flex><span>    W1 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>convert_to_tensor(parameters[<span style=color:#e6db74>&#39;W1&#39;</span>])
</span></span><span style=display:flex><span>    b1 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>convert_to_tensor(parameters[<span style=color:#e6db74>&#34;b1&#34;</span>])
</span></span><span style=display:flex><span>    W2 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>convert_to_tensor(parameters[<span style=color:#e6db74>&#34;W2&#34;</span>])
</span></span><span style=display:flex><span>    b2 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>convert_to_tensor(parameters[<span style=color:#e6db74>&#34;b2&#34;</span>])
</span></span><span style=display:flex><span>    W3 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>convert_to_tensor(parameters[<span style=color:#e6db74>&#34;W3&#34;</span>])
</span></span><span style=display:flex><span>    b3 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>convert_to_tensor(parameters[<span style=color:#e6db74>&#34;b3&#34;</span>])
</span></span><span style=display:flex><span>    W4 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>convert_to_tensor(parameters[<span style=color:#e6db74>&#34;W4&#34;</span>])
</span></span><span style=display:flex><span>    b4 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>convert_to_tensor(parameters[<span style=color:#e6db74>&#34;b4&#34;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    params <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#34;W1&#34;</span>: W1,
</span></span><span style=display:flex><span>              <span style=color:#e6db74>&#34;b1&#34;</span>: b1,
</span></span><span style=display:flex><span>              <span style=color:#e6db74>&#34;W2&#34;</span>: W2,
</span></span><span style=display:flex><span>              <span style=color:#e6db74>&#34;b2&#34;</span>: b2,
</span></span><span style=display:flex><span>              <span style=color:#e6db74>&#34;W3&#34;</span>: W3,
</span></span><span style=display:flex><span>              <span style=color:#e6db74>&#34;b3&#34;</span>: b3,
</span></span><span style=display:flex><span>              <span style=color:#e6db74>&#34;W4&#34;</span>: W4,
</span></span><span style=display:flex><span>              <span style=color:#e6db74>&#34;b4&#34;</span>: b4}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>placeholder(<span style=color:#e6db74>&#34;float&#34;</span>, [X_train<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>], <span style=color:#66d9ef>None</span>])
</span></span><span style=display:flex><span>    keep_prob1 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>placeholder(tf<span style=color:#f92672>.</span>float32, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;keep_prob1&#39;</span>)
</span></span><span style=display:flex><span>    keep_prob2 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>placeholder(tf<span style=color:#f92672>.</span>float32, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;keep_prob2&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    z4 <span style=color:#f92672>=</span> forward_propagation(x, params, keep_prob1, keep_prob2)
</span></span><span style=display:flex><span>    p <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>softmax(z4, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)  <span style=color:#75715e># dim=0 because the classes are on that axis</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># p = tf.argmax(z4) # this gives only the predicted class as output</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    sess <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>Session()
</span></span><span style=display:flex><span>    prediction <span style=color:#f92672>=</span> sess<span style=color:#f92672>.</span>run(p, feed_dict<span style=color:#f92672>=</span>{x: X, keep_prob1: <span style=color:#ae81ff>1.0</span>, keep_prob2: <span style=color:#ae81ff>1.0</span>})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> prediction
</span></span></code></pre></div><p>And now I define the model function which is in fact the neural network that we will train afterwards. An important difference with respect to <a href=https://github.com/dariodata/TensorFlow-MNIST/blob/master/TensorFlow-MNIST.ipynb>my previous MNIST example</a> is that I added an additional regularization term to the cost function. I used L2 regularization to penalize the weights in all four layers. The bias was not penalized as this is not necessary. The strictness of this penalty was given by a <code>beta</code> constant defined at 0.01.</p><p>Why use additional regularization? Because this allowed me to decrease the variance, i.e. decrease the difference in performance of the model with the training set compared to the validation set. This produced my best submission in the competition.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>model</span>(X_train, Y_train, X_test, Y_test, learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>0.0001</span>,
</span></span><span style=display:flex><span>          num_epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span>, minibatch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, print_cost<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Implements a four-layer tensorflow neural network: (LINEAR-&gt;RELU)^3-&gt;LINEAR-&gt;SOFTMAX.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Arguments:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    X_train -- training set, of shape (input size, number of training examples)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Y_train -- test set, of shape (output size, number of training examples)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    X_test -- training set, of shape (input size, number of training examples)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Y_test -- test set, of shape (output size, number of test examples)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    learning_rate -- learning rate of the optimization
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    num_epochs -- number of epochs of the optimization loop
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    minibatch_size -- size of a minibatch
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    print_cost -- True to print the cost every 100 epochs
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    parameters -- parameters learnt by the model. They can then be used to predict.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    ops<span style=color:#f92672>.</span>reset_default_graph()  <span style=color:#75715e># to be able to rerun the model without overwriting tf variables</span>
</span></span><span style=display:flex><span>    tf<span style=color:#f92672>.</span>set_random_seed(<span style=color:#ae81ff>1</span>)  <span style=color:#75715e># to keep consistent results</span>
</span></span><span style=display:flex><span>    seed <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>  <span style=color:#75715e># to keep consistent results</span>
</span></span><span style=display:flex><span>    (n_x, m) <span style=color:#f92672>=</span> X_train<span style=color:#f92672>.</span>shape  <span style=color:#75715e># (n_x: input size, m : number of examples in the train set)</span>
</span></span><span style=display:flex><span>    n_y <span style=color:#f92672>=</span> Y_train<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]  <span style=color:#75715e># n_y : output size</span>
</span></span><span style=display:flex><span>    costs <span style=color:#f92672>=</span> []  <span style=color:#75715e># To keep track of the cost</span>
</span></span><span style=display:flex><span>    t0 <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time()  <span style=color:#75715e># to mark the start of the training</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Create Placeholders of shape (n_x, n_y)</span>
</span></span><span style=display:flex><span>    X, Y <span style=color:#f92672>=</span> create_placeholders(n_x, n_y)
</span></span><span style=display:flex><span>    keep_prob1 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>placeholder(tf<span style=color:#f92672>.</span>float32, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;keep_prob1&#39;</span>)  <span style=color:#75715e># probability to keep a unit during dropout</span>
</span></span><span style=display:flex><span>    keep_prob2 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>placeholder(tf<span style=color:#f92672>.</span>float32, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;keep_prob2&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Initialize parameters</span>
</span></span><span style=display:flex><span>    parameters <span style=color:#f92672>=</span> initialize_parameters()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Forward propagation</span>
</span></span><span style=display:flex><span>    Z4 <span style=color:#f92672>=</span> forward_propagation(X, parameters, keep_prob1, keep_prob2)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Cost function</span>
</span></span><span style=display:flex><span>    cost <span style=color:#f92672>=</span> compute_cost(Z4, Y)
</span></span><span style=display:flex><span>    regularizers <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>l2_loss(parameters[<span style=color:#e6db74>&#39;W1&#39;</span>]) <span style=color:#f92672>+</span> tf<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>l2_loss(parameters[<span style=color:#e6db74>&#39;W2&#39;</span>]) <span style=color:#f92672>+</span> tf<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>l2_loss(parameters[<span style=color:#e6db74>&#39;W3&#39;</span>]) \
</span></span><span style=display:flex><span>                   <span style=color:#f92672>+</span> tf<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>l2_loss(parameters[<span style=color:#e6db74>&#39;W4&#39;</span>])  <span style=color:#75715e># add regularization term</span>
</span></span><span style=display:flex><span>    beta <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.01</span>  <span style=color:#75715e># regularization constant</span>
</span></span><span style=display:flex><span>    cost <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>reduce_mean(cost <span style=color:#f92672>+</span> beta <span style=color:#f92672>*</span> regularizers)  <span style=color:#75715e># cost with regularization</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Backpropagation: Define the tensorflow AdamOptimizer.</span>
</span></span><span style=display:flex><span>    optimizer <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>train<span style=color:#f92672>.</span>AdamOptimizer(learning_rate<span style=color:#f92672>=</span>learning_rate)<span style=color:#f92672>.</span>minimize(cost)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Initialize all the variables</span>
</span></span><span style=display:flex><span>    init <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>global_variables_initializer()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Start the session to compute the tensorflow graph</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> tf<span style=color:#f92672>.</span>Session() <span style=color:#66d9ef>as</span> sess:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Run the initialization</span>
</span></span><span style=display:flex><span>        sess<span style=color:#f92672>.</span>run(init)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Do the training loop</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(num_epochs):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            epoch_cost <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.</span>  <span style=color:#75715e># Defines a cost related to an epoch</span>
</span></span><span style=display:flex><span>            num_minibatches <span style=color:#f92672>=</span> int(m <span style=color:#f92672>/</span> minibatch_size)  <span style=color:#75715e># number of minibatches of size minibatch_size in the train set</span>
</span></span><span style=display:flex><span>            seed <span style=color:#f92672>=</span> seed <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>            minibatches <span style=color:#f92672>=</span> random_mini_batches(X_train, Y_train, minibatch_size, seed)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> minibatch <span style=color:#f92672>in</span> minibatches:
</span></span><span style=display:flex><span>                <span style=color:#75715e># Select a minibatch</span>
</span></span><span style=display:flex><span>                (minibatch_X, minibatch_Y) <span style=color:#f92672>=</span> minibatch
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                <span style=color:#75715e># Run the session to execute the &#34;optimizer&#34; and the &#34;cost&#34;</span>
</span></span><span style=display:flex><span>                _, minibatch_cost <span style=color:#f92672>=</span> sess<span style=color:#f92672>.</span>run([optimizer, cost], feed_dict<span style=color:#f92672>=</span>{X: minibatch_X, Y: minibatch_Y,
</span></span><span style=display:flex><span>                                                                           keep_prob1: <span style=color:#ae81ff>0.7</span>, keep_prob2: <span style=color:#ae81ff>0.5</span>})
</span></span><span style=display:flex><span>                epoch_cost <span style=color:#f92672>+=</span> minibatch_cost <span style=color:#f92672>/</span> num_minibatches
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># Print the cost every epoch</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> print_cost <span style=color:#f92672>==</span> <span style=color:#66d9ef>True</span> <span style=color:#f92672>and</span> epoch <span style=color:#f92672>%</span> <span style=color:#ae81ff>100</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>&#34;Cost after epoch </span><span style=color:#e6db74>{}</span><span style=color:#e6db74>: </span><span style=color:#e6db74>{:f}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(epoch, epoch_cost))
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> print_cost <span style=color:#f92672>==</span> <span style=color:#66d9ef>True</span> <span style=color:#f92672>and</span> epoch <span style=color:#f92672>%</span> <span style=color:#ae81ff>5</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                costs<span style=color:#f92672>.</span>append(epoch_cost)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># lets save the parameters in a variable</span>
</span></span><span style=display:flex><span>        parameters <span style=color:#f92672>=</span> sess<span style=color:#f92672>.</span>run(parameters)
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;Parameters have been trained!&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Calculate the correct predictions</span>
</span></span><span style=display:flex><span>        correct_prediction <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>equal(tf<span style=color:#f92672>.</span>argmax(Z4), tf<span style=color:#f92672>.</span>argmax(Y))
</span></span><span style=display:flex><span>        <span style=color:#75715e># Calculate accuracy on the test set</span>
</span></span><span style=display:flex><span>        accuracy <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>reduce_mean(tf<span style=color:#f92672>.</span>cast(correct_prediction, <span style=color:#e6db74>&#34;float&#34;</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        train_cost <span style=color:#f92672>=</span> cost<span style=color:#f92672>.</span>eval({X: X_train, Y: Y_train, keep_prob1: <span style=color:#ae81ff>1.0</span>, keep_prob2: <span style=color:#ae81ff>1.0</span>})
</span></span><span style=display:flex><span>        test_cost <span style=color:#f92672>=</span> cost<span style=color:#f92672>.</span>eval({X: X_test, Y: Y_test, keep_prob1: <span style=color:#ae81ff>1.0</span>, keep_prob2: <span style=color:#ae81ff>1.0</span>})
</span></span><span style=display:flex><span>        train_accuracy <span style=color:#f92672>=</span> accuracy<span style=color:#f92672>.</span>eval({X: X_train, Y: Y_train, keep_prob1: <span style=color:#ae81ff>1.0</span>, keep_prob2: <span style=color:#ae81ff>1.0</span>})
</span></span><span style=display:flex><span>        test_accuracy <span style=color:#f92672>=</span> accuracy<span style=color:#f92672>.</span>eval({X: X_test, Y: Y_test, keep_prob1: <span style=color:#ae81ff>1.0</span>, keep_prob2: <span style=color:#ae81ff>1.0</span>})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#39;Finished training in </span><span style=color:#e6db74>%s</span><span style=color:#e6db74> s&#39;</span> <span style=color:#f92672>%</span> (time<span style=color:#f92672>.</span>time() <span style=color:#f92672>-</span> t0))
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;Train Cost:&#34;</span>, train_cost)
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;Test Cost:&#34;</span>, test_cost)
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;Train Accuracy:&#34;</span>, train_accuracy)
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;Test Accuracy:&#34;</span>, test_accuracy)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># plot the cost</span>
</span></span><span style=display:flex><span>        plt<span style=color:#f92672>.</span>plot(np<span style=color:#f92672>.</span>squeeze(costs))
</span></span><span style=display:flex><span>        plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#39;cost&#39;</span>)
</span></span><span style=display:flex><span>        plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;iterations (per fives)&#39;</span>)
</span></span><span style=display:flex><span>        plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#34;Learning rate = </span><span style=color:#e6db74>{}</span><span style=color:#e6db74>, beta = </span><span style=color:#e6db74>{}</span><span style=color:#e6db74>,</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>                  <span style=color:#e6db74>&#34;test cost = </span><span style=color:#e6db74>{:.6f}</span><span style=color:#e6db74>, test accuracy = </span><span style=color:#e6db74>{:.6f}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(learning_rate, beta, test_cost, test_accuracy))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>global</span> filename
</span></span><span style=display:flex><span>        filename <span style=color:#f92672>=</span> timestr <span style=color:#f92672>+</span> <span style=color:#e6db74>&#39;_NN4Lstage2_lr_</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>_beta_</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>_cost_</span><span style=color:#e6db74>{:.2f}</span><span style=color:#e6db74>-</span><span style=color:#e6db74>{:.2f}</span><span style=color:#e6db74>_acc_</span><span style=color:#e6db74>{:.2f}</span><span style=color:#e6db74>-</span><span style=color:#e6db74>{:.2f}</span><span style=color:#e6db74>&#39;</span><span style=color:#f92672>.</span>format(
</span></span><span style=display:flex><span>            learning_rate, beta, train_cost, test_cost, train_accuracy, test_accuracy)
</span></span><span style=display:flex><span>        plt<span style=color:#f92672>.</span>savefig(dirname <span style=color:#f92672>+</span> filename <span style=color:#f92672>+</span> <span style=color:#e6db74>&#39;.png&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> parameters
</span></span></code></pre></div><p>Note that the model function will return the learned parameters from the network and additionally will plot the cost after each epoch. The plot is also saved as a file that includes the timestamp as well as the learning rate, beta, cost and accuracy information for this particular run.</p><p>Now it&rsquo;s time to train the model using the train and validation data:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># train the model and get learned parameters</span>
</span></span><span style=display:flex><span>parameters <span style=color:#f92672>=</span> model(X_train, Y_train, X_val, Y_val)
</span></span></code></pre></div><pre><code>Cost after epoch 0: 6.607861
Cost after epoch 100: 1.389869
Cost after epoch 200: 0.988806
Cost after epoch 300: 0.882713
Cost after epoch 400: 0.833693
Cost after epoch 500: 0.811457
Cost after epoch 600: 0.793379
Cost after epoch 700: 0.773927
Cost after epoch 800: 0.762247
Cost after epoch 900: 0.767449
Parameters have been trained!
Finished training in 498.4203100204468 s
Train Cost: 0.665462
Test Cost: 1.74987
Train Accuracy: 0.979292
Test Accuracy: 0.643609
</code></pre><p><img alt=png loading=lazy src=/images/2017-10-07-personalized-medicine_files/2017-10-07-personalized-medicine_31_1.png></p><p>From my validation results we can observe that the network learned nicely. However, the final cost of the training data was 0.665462, where as the validation data had a final cost of 1.74987. This is a large difference and an indication that the model is overfitting. Moreover the accuracy (defined here as the fraction of correct predictions) is very high (97.9%) for the training data and only 64.3% for the validation set. Another indication that the model is overfitting even though I have used both dropout and L2 regularization to counteract this.</p><h2 id=make-predictions>Make predictions<a hidden class=anchor aria-hidden=true href=#make-predictions>#</a></h2><p>We use the learned parameteres to make a prediction on the test data.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># use learned parameters to make prediction on test data</span>
</span></span><span style=display:flex><span>prediction <span style=color:#f92672>=</span> predict(X_test, parameters)
</span></span></code></pre></div><p>Let&rsquo;s look at an example of a prediction. As we can see below, the prediction consists of the probabilities of the entry belongin to each of the nine different categories (this was the format needed for this competition).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span>prediction[:,<span style=color:#ae81ff>0</span>]
</span></span></code></pre></div><pre><code>array([ 0.36503336,  0.21219006,  0.01297534,  0.14676626,  0.08375936,
        0.09217557,  0.02737238,  0.03150512,  0.02822249], dtype=float32)
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span>prediction<span style=color:#f92672>.</span>shape
</span></span></code></pre></div><pre><code>(9, 986)
</code></pre><p>All we have to do now is create a submission .csv file to save our prediction results.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># create submission file</span>
</span></span><span style=display:flex><span>submission <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame(prediction<span style=color:#f92672>.</span>T)
</span></span><span style=display:flex><span>submission[<span style=color:#e6db74>&#39;id&#39;</span>] <span style=color:#f92672>=</span> test_index
</span></span><span style=display:flex><span>submission<span style=color:#f92672>.</span>columns <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;class1&#39;</span>, <span style=color:#e6db74>&#39;class2&#39;</span>, <span style=color:#e6db74>&#39;class3&#39;</span>, <span style=color:#e6db74>&#39;class4&#39;</span>, <span style=color:#e6db74>&#39;class5&#39;</span>, <span style=color:#e6db74>&#39;class6&#39;</span>, <span style=color:#e6db74>&#39;class7&#39;</span>, <span style=color:#e6db74>&#39;class8&#39;</span>, <span style=color:#e6db74>&#39;class9&#39;</span>, <span style=color:#e6db74>&#39;id&#39;</span>]
</span></span><span style=display:flex><span>submission<span style=color:#f92672>.</span>to_csv(dirname <span style=color:#f92672>+</span> filename <span style=color:#f92672>+</span> <span style=color:#e6db74>&#39;.csv&#39;</span>, index<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span></code></pre></div><h2 id=results-interpretation>Results interpretation<a hidden class=anchor aria-hidden=true href=#results-interpretation>#</a></h2><p>Using this neural network model, my submission to Kaggle yielded following results:</p><ul><li>Public score (based on a portion of the test data by Kaggle to provide an indication of performance during the competition): Loss = 1.69148</li><li>Private score (based on a different portion of the test data by Kaggle to provide the final score at the end of the competition): Loss = 2.74500</li></ul><p>The discrepancy between these two scores further shows that overfitting is an issue in working with this data in a neural network model. My model could benefit from increasing the training data and a higher regularization.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://www.arcosdiaz.com/blog/tags/machine_learning/>Machine_learning</a></li><li><a href=https://www.arcosdiaz.com/blog/tags/classification/>Classification</a></li><li><a href=https://www.arcosdiaz.com/blog/tags/healthcare/>Healthcare</a></li></ul><nav class=paginav><a class=prev href=https://www.arcosdiaz.com/blog/posts/2018-04-01-fitbit_prophet/><span class=title>« Prev</span><br><span>Fitbit activity and sleep data: a time-series analysis with Generalized Additive Models</span>
</a><a class=next href=https://www.arcosdiaz.com/blog/posts/2017-02-06-medicare-drug-cost/><span class=title>Next »</span><br><span>Exploratory analysis of Medicare drug cost data 2011-2015</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://www.arcosdiaz.com/blog/>Dario Arcos-Díaz, PhD</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>