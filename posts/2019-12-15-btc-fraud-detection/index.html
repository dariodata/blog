<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Graph Convolutional Networks for Fraud Detection of Bitcoin Transactions | Dario Arcos-Díaz, PhD</title><meta name=keywords content="graph neural networks,fraud detection"><meta name=description content="Detecting fraudulent transactions is essential in keeping financial systems trustworthy. Here I illustrate an end-to-end approach of node classification by graph neural networks to identify suspicious transactions."><meta name=author content="Dario Arcos-Díaz"><link rel=canonical href=https://www.arcosdiaz.com/blog/posts/2019-12-15-btc-fraud-detection/><link crossorigin=anonymous href=/blog/assets/css/stylesheet.da3211e5ef867bf2b75fd5a6515cfed7195c011e8ab735694e203810a827097b.css integrity="sha256-2jIR5e+Ge/K3X9WmUVz+1xlcAR6KtzVpTiA4EKgnCXs=" rel="preload stylesheet" as=style><link rel=icon href=https://www.arcosdiaz.com/blog/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.arcosdiaz.com/blog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.arcosdiaz.com/blog/favicon-32x32.png><link rel=apple-touch-icon href=https://www.arcosdiaz.com/blog/apple-touch-icon.png><link rel=mask-icon href=https://www.arcosdiaz.com/blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://www.arcosdiaz.com/blog/posts/2019-12-15-btc-fraud-detection/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-2Q04SCXNNC"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-2Q04SCXNNC")}</script><meta property="og:url" content="https://www.arcosdiaz.com/blog/posts/2019-12-15-btc-fraud-detection/"><meta property="og:site_name" content="Dario Arcos-Díaz, PhD"><meta property="og:title" content="Graph Convolutional Networks for Fraud Detection of Bitcoin Transactions"><meta property="og:description" content="Detecting fraudulent transactions is essential in keeping financial systems trustworthy. Here I illustrate an end-to-end approach of node classification by graph neural networks to identify suspicious transactions."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-12-15T00:00:00+00:00"><meta property="article:modified_time" content="2019-12-15T00:00:00+00:00"><meta property="article:tag" content="Graph Neural Networks"><meta property="article:tag" content="Fraud Detection"><meta property="og:image" content="https://www.arcosdiaz.com/images/bitcoin_graph_thumb.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://www.arcosdiaz.com/images/bitcoin_graph_thumb.png"><meta name=twitter:title content="Graph Convolutional Networks for Fraud Detection of Bitcoin Transactions"><meta name=twitter:description content="Detecting fraudulent transactions is essential in keeping financial systems trustworthy. Here I illustrate an end-to-end approach of node classification by graph neural networks to identify suspicious transactions."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://www.arcosdiaz.com/blog/posts/"},{"@type":"ListItem","position":2,"name":"Graph Convolutional Networks for Fraud Detection of Bitcoin Transactions","item":"https://www.arcosdiaz.com/blog/posts/2019-12-15-btc-fraud-detection/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Graph Convolutional Networks for Fraud Detection of Bitcoin Transactions","name":"Graph Convolutional Networks for Fraud Detection of Bitcoin Transactions","description":"Detecting fraudulent transactions is essential in keeping financial systems trustworthy. Here I illustrate an end-to-end approach of node classification by graph neural networks to identify suspicious transactions.","keywords":["graph neural networks","fraud detection"],"articleBody":"tl;dr I trained 4 different types of models to classify bitcoin transactions. For each, two versions of the feature set were used: all features (local + neighborhood-aggregated) and only local features (without neighborhood information).\nThe best model was a Random Forest trained with all features: its performance was impaired when the aggregated features were removed. The best graph-based neural network model was APPNP and its performance was better when only local features were used. APPNP performed better than an MLP with comparable complexity, indicating that the graph structure information gave it an advantage. Finally, the best GCN model required using all features and several strategies to reduce overfitting. The excellent performance of a Random Forest shows that it makes sense to consider simple models when faced with a new task. It also indicates that the individual node features in the Elliptic dataset are already informative enough to make good predictions. It would be interesting to explore how the model performs, when fewer samples and/or features are available for training.\nA shallow GCN with 2 layers might not be a good choice for node classification of a graph as sparse as the bitcoin transaction graph. If a node has few incoming edges, a graph convolution may not have enough neighbors with features to aggregate.\nAn interesting solution is provided by the APPNP model, which combines message passing with the teleportation principle of personalized pagerank. The long-range (20 iterations in the best model) of the predictions propagation through the network is an aspect that deserves further attention in the future.\nThe main performance metrics for comparison were:\nModel Features Dropout Precision Recall F1 score GCN all 0.5 0.8051 0.4958 0.6137 GCN local 0. 0.6667 0.4617 0.5456 APPNP all 0.2 0.7791 0.6251 0.6936 APPNP local 0. 0.8158 0.6787 0.7409 MLP all 0.2 0.6538 0.6593 0.6565 MLP local 0. 0.7799 0.6740 0.7231 RandomForest all 0.9167 0.7211 0.8072 RandomForest local 0.8749 0.7036 0.7799 Disclaimer: This was a hobby project done mostly nocturnally and on the weekends out of pure fascination for graph theory and neural networks. Although I made every effort to apply scientific rigor, these results constitute an initial exploration and should not be considered an exhaustive analysis. Importantly, due to the random nature of certain parameters (e.g. dropout), multiple repetitions of the experiments using different random seeds and/or different validation splits are necessary for a conclusive judgement.\n#hide_input import os import pandas as pd import seaborn as sns import matplotlib import matplotlib.pyplot as plt %matplotlib inline matplotlib.rcParams['figure.dpi'] = 300 path = os.path.realpath('.') runs_config = pd.read_csv(path+'/experiments_summary.csv') runs_metrics = pd.read_csv(path+'/experiments_metrics.csv') runs = runs_metrics.merge(runs_config, left_on='name', right_on='name', suffixes=('','_')) runs.rename(columns={'_step':'epoch'}, inplace=True) runs['nobias'] = runs['nobias'].astype(str) runs['dropout'] = runs['dropout'].astype(float) runs['k'] = runs['k'].astype(str) runs['alpha'] = runs['alpha'].astype(str) runs.query('nhidden==\"100\" and _step_\u003e=999.0', inplace=True) query = '''((onlylocal==True and dropout==0) or (onlylocal==False and (dropout==0.2 or dropout==0.5))) and ((model==\"appnp\" and k==\"20.0\" and alpha==\"0.2\" and (dropout==0. or dropout==0.2)) or (model==\"mlp\" and (dropout==0. or dropout==0.2)) or (model==\"gcn\" and (dropout==0. or dropout==0.5))) and bidirectional==True and weight_decay==0.0005 and nobias==\"False\"'''.replace('\\n',' ') g1 = sns.relplot('epoch', 'val_f1_score', col='onlylocal', hue='model', style='dropout', palette=sns.color_palette(\"Set1\", 3), kind='line', data=runs.query(query)); g1.axes.flat[0].axhline(0.8072, c='k', alpha=0.8, ls='-.', lw=1) g1.axes.flat[0].text(1,0.815,'RandomForest') g1.axes.flat[1].axhline(0.7799, c='k', alpha=0.8, ls='-.', lw=1) g1.axes.flat[1].text(1,0.785,'RandomForest') plt.suptitle('Performance of the best models of each class using all features vs. only local features', y=1.02); Introduction The Elliptic Data Set consists of anonymized transactions collected from the bitcoin exchange during 49 distinct time-periods. The transactions are represented as a graph containing 203769 nodes (transactions) and 234355 edges (bitcoin flow from one transaction to another). A subset of the transactions are labeled as licit or illicit. A detailed description of the dataset and an initial approach applying graph convolutional networks (GCNs) for the task of node classification has been addressed by:\nM. Weber, G. Domeniconi, J. Chen, D. K. I. Weidele, C. Bellei, T. Robinson, C. E. Leiserson, “Anti-Money Laundering in Bitcoin: Experimenting with Graph Convolutional Networks for Financial Forensics”, KDD ’19 Workshop on Anomaly Detection in Finance, August 2019, Anchorage, AK, USA.\nIn this notebook, I will take a closer look to how graph-based neural networks can be applied to this task and propose possible directions for future analyses.\nDue to the longer training times and for reproducibility, the experiments were all run using the script in models.py and all runs and metrics were tracked on Weights\u0026Biases. All results were exported to a csv file using this script and loaded onto this notebook for visualization.\n#collapse-hide import os import random import time import dgl import networkx as nx import numpy as np import pandas as pd import torch import torch.nn as nn import torch.nn.functional as F from dgl.nn.pytorch import GraphConv from sklearn.metrics import confusion_matrix, precision_recall_fscore_support import matplotlib import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline matplotlib.rcParams['figure.dpi'] = 300 # set random seeds seed = 0 random.seed(seed) np.random.seed(seed) dgl.random.seed(seed) torch.manual_seed(seed); path = os.path.realpath('.') #collapse-hide # load experiment results exported from Weights\u0026Biases runs_config = pd.read_csv(path+'/experiments_summary.csv') runs_metrics = pd.read_csv(path+'/experiments_metrics.csv') runs = runs_metrics.merge(runs_config, left_on='name', right_on='name', suffixes=('','_')) runs.rename(columns={'_step':'epoch'}, inplace=True) runs['nobias'] = runs['nobias'].astype(str) runs['dropout'] = runs['dropout'].astype(float) runs['k'] = runs['k'].astype(str) runs['alpha'] = runs['alpha'].astype(str) runs.query('nhidden==\"100\" and _step_\u003e=999.0', inplace=True) Transaction data Three tables are initially available to download from Kaggle’s dataset repository:\nAn edgelist: the edges between bitcoin transactions (nodes identified by transaction id) necessary to build the graph A classes table: label for each transaction can be licit, illicit, or unknown A features table with 167 columns Transaction id Timestep: consecutive periods of time for which all bitcoin flows are translated to edges in a graph Edges exist only between transactions within the same timestep 93 local features, i.e. intrinsic properties of the transactions themselves such as amount, transaction fee, etc. 72 aggregated features with information about the immediate neighborhood of each node, e.g. sum of amounts of the neighboring transactions # load data df_edges = pd.read_csv(path + \"/elliptic_bitcoin_dataset/elliptic_txs_edgelist.csv\") df_classes = pd.read_csv(path + \"/elliptic_bitcoin_dataset/elliptic_txs_classes.csv\") df_features = pd.read_csv( path + \"/elliptic_bitcoin_dataset/elliptic_txs_features.csv\", header=None ) # rename the classes to ints that can be handled by pytorch as labels df_classes[\"label\"] = df_classes[\"class\"].replace( {\"unknown\": -1, # unlabeled nodes \"2\": 0, # labeled licit nodes #\"1\": 1, # labeled illicit nodes } ).astype(int) # rename features according to data description in paper rename_dict = dict( zip( range(0, 167), [\"txId\", \"time_step\"] + [f\"local_{i:02d}\" for i in range(1, 94)] + [f\"aggr_{i:02d}\" for i in range(1, 73)], ) ) df_features.rename(columns=rename_dict, inplace=True) # check missing data print(f\"Number of missing data points: {df_features.isna().sum().sum()+df_classes.isna().sum().sum()}\") print(f\"Number of nodes (transactions): {df_features['txId'].nunique()}\") print(f\"Number of edges: {df_edges.shape[0]}\") print(f\"Number of classes: {df_classes['class'].nunique()}\") print(f\"Timesteps range from {df_features['time_step'].min()} to {df_features['time_step'].max()}\") Number of missing data points: 0 Number of nodes (transactions): 203769 Number of edges: 234355 Number of classes: 3 Timesteps range from 1 to 49 Correlation analysis Additionally, the dataset was analyzed using the handy pandas-profiling package. The complete script for the analysis is in eda.py, which generates a detailed report including multiple correlations. The main findings from the report can be summarized as:\n29 features are highly skewed 76 features are highly correlated to other features in the dataset (Spearman correlation coefficient $\\rho \u003e 0.90$) 21 aggregated features are highly correlated to other aggregated features 54 local features are highly correlated to other local features time_step is highly correlated with aggr_43 ($\\rho = 0.91$) Constructing the transaction graph We now have our data prepared in table format, but we want to be able to work on the graph constructed from the data. In order to create our transaction graph, we use the networkx package. We create a directed multigraph (a directed graph that allows for multiple edges between two nodes) and add the label attribute to each transaction.\n# create networkx graph from the pandas dataframes g_nx = nx.MultiDiGraph() g_nx.add_nodes_from( zip(df_classes[\"txId\"], [{\"label\": v} for v in df_classes[\"label\"]]) ) g_nx.add_edges_from(zip(df_edges[\"txId1\"], df_edges[\"txId2\"])); print(f\"Graph with {g_nx.number_of_nodes()} nodes and {g_nx.number_of_edges()} edges.\") print(f\"Number of connected components: {len(list(nx.weakly_connected_components(g_nx)))}\") Graph with 203769 nodes and 234355 edges. Number of connected components: 49 We can confirm that there are 49 connected components (weakly connected compoments in the case of directed graphs) was constructed for each timestep. This means that the dataset consists of 49 different subgraphs, each corresponding to one timestep.\n# create list of graphs, one for each timestep components = list(nx.weakly_connected_components(g_nx)) g_nx_t_list = [g_nx.subgraph(components[i]) for i in range(0,len(components))] with sns.axes_style('white'): fig, ax = plt.subplots(1,2, figsize=(12,6)) for i,t in enumerate([26,48]): node_label = list(nx.get_node_attributes(g_nx_t_list[t], 'label').values()) mapping = {-1:'grey', 0:'C0', 1:'C3'} node_color = [mapping[l] for l in node_label] nx.draw_networkx(g_nx_t_list[t], node_size=10, node_color=node_color, with_labels=False, width=0.2, alpha=0.8, arrowsize=8, ax=ax[i]) leg = ax[0].legend(['unlabeled', 'licit', 'illicit']) leg.legendHandles[0].set_color('grey') leg.legendHandles[1].set_color('C0') leg.legendHandles[2].set_color('C3') plt.tight_layout() /anaconda3/envs/dgl/lib/python3.7/site-packages/networkx/drawing/nx_pylab.py:579: MatplotlibDeprecationWarning: The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. if not cb.iterable(width): /anaconda3/envs/dgl/lib/python3.7/site-packages/networkx/drawing/nx_pylab.py:676: MatplotlibDeprecationWarning: The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. if cb.iterable(node_size): # many node sizes We can see that most of the transactions are not labeled and that only a minority of the labeled nodes correspond to illicit transactions. Moreover, there graph does not seem to be particularly dense. There seem to be chains of transactions one after the other. Also, many of these chains seem to concentrate one type of labeled transaction: either licit or illicit. Finally, a minority of nodes seems to have a larger number of edges, connecting with transactions in multiple chains or further away from their inmediate neighborhood.\nGraph metrics We can calculate selected graph metrics to better quantify some important structural properties of the transaction graph.\ng_metrics = {} g_metrics['timestep'] = np.arange(1,50) g_metrics['number_of_nodes'] = [graph.number_of_nodes() for graph in g_nx_t_list] g_metrics['avg_degree'] = [np.mean(list(dict(nx.degree(graph)).values())) for graph in g_nx_t_list] g_metrics['density'] = [nx.density(graph) for graph in g_nx_t_list] g_metrics['avg_clustering'] = [nx.average_clustering(nx.DiGraph(graph)) for graph in g_nx_t_list] g_metrics['avg_shortest_path'] = [nx.average_shortest_path_length(nx.DiGraph(graph)) for graph in g_nx_t_list] fig, ax = plt.subplots(len(g_metrics)-1,1, figsize=(10,6), sharex=True) for i,label in enumerate(list(g_metrics.keys())[1:]): ax[i].bar(g_metrics['timestep'], g_metrics[label], label=label) ax[i].legend() plt.xlabel('timestep'); print(f\"Average density of the graphs across all timesteps: {np.mean(g_metrics['density']):.6f}\") print(f\"Average degree of all nodes across all timesteps: {np.mean(list(dict(nx.degree(g_nx)).values())):.2f}\") Average density of the graphs across all timesteps: 0.000318 Average degree of all nodes across all timesteps: 2.30 Given the density of the transaction graph, it seems to be rather sparse. The average density accross all timesteps lies around 0.0003177 and each node has an average of 2.30 edges. In comparison, the Cora dataset (popularly used as a benchmark of node classification algorithms) has an average degree of 3.90 and a density of 0.0014812. With only 2708 nodes, Cora is a much smaller and denser graph.\nTraining a Graph Convolutional Network To build and train the GCN, I used DGL as a framework for deep learning on graphs DGL is based on pytorch and uses DGLGraph objects that can be easily created from networkx graphs. Moreover, several implementations of graph-based neural layers are available in DGL.\nGraph creation First we create the DGLGraph from a networkx Graph. We also add the label information as a tensor to the node data in the DGLGraph (from now on simply “graph”). Similarly, we add the node feature matrix to the graph as a tensor of shape (number of nodes, number of features) = (203769, 166).\nImportantly, I tested the performance of the GCN using two options for constructing the graph:\nUnidirectional edges: edges going from one transaction to the next. In a GCN, this would mean that information used by the model to classify nodes would flow in one direction (downstream) only. In this case, we use g_nx directly. Bidirectional edges: edges are made bidirectional, which would allow information flow in a GCN to travel in both directions (downstream and upstream). In this case, we use g_nx.to_undirected().to_directed(), i.e. we first make the edges undirected, and then back again directed. By doing so, networkx makes the edges in the resulting graph bidirectional. It is not explicitely stated in the paper, which of these two options was used by Weber et al. However, from the performance metrics, it is likely that they used the bidirectional edges version.\n# create unidirectional graph g = dgl.DGLGraph() g.from_networkx(g_nx) g.ndata[\"label\"] = torch.tensor( df_classes.set_index(\"txId\").loc[sorted(g_nx.nodes()), \"label\"].values ) g.ndata[\"features_matrix\"] = torch.tensor( df_features.set_index(\"txId\").loc[sorted(g_nx.nodes()), :].values ) print(g) DGLGraph(num_nodes=203769, num_edges=234355, ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'features_matrix': Scheme(shape=(166,), dtype=torch.float64)} edata_schemes={}) # make unidirectional edges bidirectional in networkx g_nx_bidirectional = g_nx.to_undirected().to_directed() # create bidirectional graph g_bi = dgl.DGLGraph() g_bi.from_networkx(g_nx_bidirectional) g_bi.ndata[\"label\"] = torch.tensor( df_classes.set_index(\"txId\").loc[sorted(g_nx.nodes()), \"label\"].values ) g_bi.ndata[\"features_matrix\"] = torch.tensor( df_features.set_index(\"txId\").loc[sorted(g_nx.nodes()), :].values ) print(g_bi) DGLGraph(num_nodes=203769, num_edges=468710, ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'features_matrix': Scheme(shape=(166,), dtype=torch.float64)} edata_schemes={}) Graph normalizaiton The performance of the GCN benefits from using a normalized version of the adjacency matrix. I applied a common normalization approach used in the literature. It starts by adding a self-loop to each node, which is the equivalent of adding the identity matrix to the adjacency matrix A $$\\tilde{A} = A + I$$The rest of the normalization consists of obtaining the normalized graph Laplacian (here is a great overview explaining what the Laplacian means), which can be calculated as $$\\hat{A} = D^{-1/2}\\tilde{A}D^{-1/2}$$ where $D$ is a matrix whose diagonal contains the degree of each node of $\\tilde{A}$.\nThis matrix multiplication notation effectively means that the normalized Laplacian can have as a value in each $(i,j)$ position: $$\\hat{A}(i,j) = \\begin{cases} 1 \u0026 \\text{if $i=j$} \\\\[2ex] {-1 \\over {\\sqrt{\\deg(i)\\deg(j)}}} \u0026 \\text{if $i \\neq j$ and $(i,j) \\in E$},\\\\[2ex] 0 \u0026 \\text{otherwise} \\end{cases}$$ where $E$ is the set of edges of the graph. As we can see, nodes with an (in)degree of zero could be troublesome, which is why we add the self-loops.\n# add self loop g.add_edges(g.nodes(), g.nodes()) print(g) # add self loop to the bidirectional edges graph g_bi.add_edges(g_bi.nodes(), g_bi.nodes()) print(g_bi) DGLGraph(num_nodes=203769, num_edges=438124, ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'features_matrix': Scheme(shape=(166,), dtype=torch.float64)} edata_schemes={}) DGLGraph(num_nodes=203769, num_edges=672479, ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'features_matrix': Scheme(shape=(166,), dtype=torch.float64)} edata_schemes={}) Train/validation data splitting Now that our graph is ready, we need to split our data into training and validation sets. We will follow the same approach taken by Weber et al., which consists in a time-based split using the initial 70% of the timesteps to train the model and the remaining 30% for validation. This temporal split makes sense in the context of possible applications of such a model. A company could continuosly use their older historical data and labels for training and then predict the node classes for more recent transactions.\nfeatures = g.ndata[\"features_matrix\"].float() labels = g.ndata[\"label\"].long() # format required for cross entropy loss in_feats = features.shape[1] n_classes = 2 # licit or illicit (unknown label is ignored) n_edges = g.number_of_edges() dataset_size = df_classes[\"label\"].notna().sum() train_ratio = 0.7 train_time_steps = round(len(np.unique(features[:, 0])) * train_ratio) shutdown_timestep = 43 train_indices = ( ((features[:, 0] \u003c= train_time_steps) \u0026 (labels != -1)).nonzero().view(-1) ) val_indices = ( ((features[:, 0] \u003e train_time_steps) \u0026 (labels != -1)).nonzero().view(-1) ) # timestep_indices = { # t:( # ((features[:, 0] == t) \u0026 (labels != -1)).nonzero().view(-1) # ) for t in range(train_time_steps+1,50) # } print(f\"\"\"Number of timesteps used for training: {train_time_steps} Number of timesteps used for validation: {dataset_size-train_time_steps}\"\"\") Number of timesteps used for training: 34 Number of timesteps used for validation: 203735 GCN model architecture For the GCN model, I used the implementation from DGL, which is based on the original implementation used by Kipf et al. (2016). In short, the algorithm is given by the formula\n$$H^{(l+1)} = \\sigma (D^{-1/2}\\tilde{A}D^{-1/2}H^{(l)}W^{(l)})$$where $\\sigma$ is the activation function (ReLu), $D^{-1/2}\\tilde{A}D^{-1/2}$ is the normalized graph Laplacian, $H^{(l)}$ are the logits and $W^{(l)}$ are the learnable weights of the $l$th layer of the neural network. It is also possible to add a learnable bias to each layer. There are some very good explanations of how this algorithm works available for reference.\nclass GCN(nn.Module): def __init__( self, g, in_feats, n_hidden, n_classes, n_layers, activation, dropout, bias ): super(GCN, self).__init__() self.g = g self.layers = nn.ModuleList() # input layer self.layers.append( GraphConv(in_feats, n_hidden, activation=activation, bias=bias) ) # hidden layers for _ in range(n_layers - 2): self.layers.append( GraphConv(n_hidden, n_hidden, activation=activation, bias=bias) ) # output layer self.layers.append(GraphConv(n_hidden, n_classes, bias=bias)) self.dropout = nn.Dropout(p=dropout) def forward(self, features): h = features for i, layer in enumerate(self.layers): if i != 0: h = self.dropout(h) h = layer(self.g, h) return h # utility function to evaluate the model def evaluate(model, loss_fcn, features, labels, mask): \"\"\"Calculate the loss, accuracy, precision, recall and f1_score for the masked data\"\"\" model.eval() with torch.no_grad(): logits = model(features) logits = logits[mask] labels = labels[mask] loss = loss_fcn(logits, labels) _, indices = torch.max(logits, dim=1) correct = torch.sum(indices == labels) p, r, f, _ = precision_recall_fscore_support(labels, indices) return loss, correct.item() * 1.0 / len(labels), p[1], r[1], f[1] # utility function to obtain a confusion matrix def eval_confusion_matrix(model, features, labels, mask): model.eval() with torch.no_grad(): logits = model(features) logits = logits[mask] labels = labels[mask] _, indices = torch.max(logits, dim=1) print(confusion_matrix(labels, indices)) Model training Now we can train the model using the specifications from the paper by Weber et al.:\ncross entropy loss function putting higher weight for the positive (illicit) samples: 0.7 positive vs 0.3 negative adam optimizer with learning rate 1e-3 no weight decay is mentioned no bias is mentioned no dropout train for 1000 epochs two Graph Convolutional layers with 100 and 2 neurons respectively We can now define the Graph Convolutional Network architecture using DGL. In this case, it consists of n_layers GCN layers with n_hidden hidden units per layer:\n# train and evaluate the model def train_eval_model(model_class, g, features, **params): #bidirectional = params[\"bidirectional\"] if \"bidirectional\" in params else None in_feats = features.shape[1] n_classes = 2 n_hidden = params[\"n_hidden\"] n_layers = params[\"n_layers\"] weight_decay = params[\"weight_decay\"] bias = params[\"bias\"] dropout = params[\"dropout\"] epochs = params[\"epochs\"] lr = params[\"lr\"] posweight = params[\"posweight\"] model = model_class(g, in_feats, n_hidden, n_classes, n_layers, F.relu, dropout, bias) # weighted cross entropy loss function loss_fcn = torch.nn.CrossEntropyLoss( weight=torch.tensor([1 - posweight, posweight]) ) # use optimizer optimizer = torch.optim.Adam( model.parameters(), lr=lr, weight_decay=weight_decay ) dur = [] metrics = {\"loss\":{\"train\": [], \"val\": []}, \"accuracy\":{\"train\": [], \"val\": []}, \"precision\":{\"train\": [], \"val\": []}, \"recall\":{\"train\": [], \"val\": []}, \"f1_score\":{\"train\": [], \"val\": []}, } for epoch in range(epochs): model.train() if epoch \u003e= 3: t0 = time.time() # forward pass logits = model(features) loss = loss_fcn(logits[train_indices], labels[train_indices]) metrics[\"loss\"][\"train\"].append(loss) # backward pass optimizer.zero_grad() loss.backward() optimizer.step() # duration if epoch \u003e= 3: dur.append(time.time() - t0) # evaluate on training set _, train_acc, train_precision, train_recall, train_f1_score = evaluate( model, loss_fcn, features, labels, train_indices ) metrics[\"accuracy\"][\"train\"].append(train_acc) metrics[\"precision\"][\"train\"].append(train_precision) metrics[\"recall\"][\"train\"].append(train_recall) metrics[\"f1_score\"][\"train\"].append(train_f1_score) # evaluate on validation set val_loss, val_acc, val_precision, val_recall, val_f1_score = evaluate( model, loss_fcn, features, labels, val_indices ) metrics[\"loss\"][\"val\"].append(val_loss) metrics[\"accuracy\"][\"val\"].append(val_acc) metrics[\"precision\"][\"val\"].append(val_precision) metrics[\"recall\"][\"val\"].append(val_recall) metrics[\"f1_score\"][\"val\"].append(val_f1_score) if (epoch + 1) % 100 == 0: print( f\"Epoch {epoch:05d} | Time(s) {np.mean(dur):.2f} | val_loss {val_loss.item():.4f} \" f\"| Precision {val_precision:.4f} | Recall {val_recall:.4f} | Acc {val_acc:.4f} \" f\"| F1_score {val_f1_score:.4f}\" ) print(\"Confusion matrix:\") eval_confusion_matrix(model, features, labels, val_indices) return model, metrics # GCN model parameters params = { \"n_hidden\" : 100, \"n_layers\" : 2, \"weight_decay\" : 0., \"bias\" : False, \"dropout\" : 0., \"epochs\" : 1000, \"lr\" : 1e-3, \"posweight\" : 0.7, } # train on graph with unidirectional edges model, metrics = train_eval_model(GCN, g, features, **params) Epoch 00099 | Time(s) 0.60 | val_loss 0.3580 | Precision 0.2701 | Recall 0.5125 | Acc 0.8783 | F1_score 0.3537 Epoch 00199 | Time(s) 0.60 | val_loss 0.3381 | Precision 0.3649 | Recall 0.4589 | Acc 0.9130 | F1_score 0.4065 Epoch 00299 | Time(s) 0.59 | val_loss 0.3537 | Precision 0.4101 | Recall 0.4358 | Acc 0.9226 | F1_score 0.4226 Epoch 00399 | Time(s) 0.58 | val_loss 0.4015 | Precision 0.4039 | Recall 0.4192 | Acc 0.9221 | F1_score 0.4114 Epoch 00499 | Time(s) 0.58 | val_loss 0.4556 | Precision 0.3895 | Recall 0.3989 | Acc 0.9203 | F1_score 0.3942 Epoch 00599 | Time(s) 0.58 | val_loss 0.5079 | Precision 0.3622 | Recall 0.3860 | Acc 0.9160 | F1_score 0.3737 Epoch 00699 | Time(s) 0.59 | val_loss 0.5570 | Precision 0.3533 | Recall 0.3813 | Acc 0.9145 | F1_score 0.3668 Epoch 00799 | Time(s) 0.59 | val_loss 0.6020 | Precision 0.3702 | Recall 0.3860 | Acc 0.9175 | F1_score 0.3779 Epoch 00899 | Time(s) 0.60 | val_loss 0.6472 | Precision 0.3677 | Recall 0.3915 | Acc 0.9167 | F1_score 0.3792 Epoch 00999 | Time(s) 0.60 | val_loss 0.7006 | Precision 0.3618 | Recall 0.3869 | Acc 0.9158 | F1_score 0.3739 Confusion matrix: [[14848 739] [ 664 419]] # train on graph with bidirectional edges model_bi, metrics_bi = train_eval_model(GCN, g_bi, features, **params) Epoch 00099 | Time(s) 0.62 | val_loss 0.3113 | Precision 0.3251 | Recall 0.4765 | Acc 0.9017 | F1_score 0.3865 Epoch 00199 | Time(s) 0.62 | val_loss 0.3229 | Precision 0.4209 | Recall 0.4543 | Acc 0.9239 | F1_score 0.4369 Epoch 00299 | Time(s) 0.62 | val_loss 0.3302 | Precision 0.4776 | Recall 0.4331 | Acc 0.9324 | F1_score 0.4542 Epoch 00399 | Time(s) 0.63 | val_loss 0.3456 | Precision 0.5940 | Recall 0.4054 | Acc 0.9434 | F1_score 0.4819 Epoch 00499 | Time(s) 0.63 | val_loss 0.3675 | Precision 0.7227 | Recall 0.3850 | Acc 0.9504 | F1_score 0.5024 Epoch 00599 | Time(s) 0.63 | val_loss 0.3933 | Precision 0.7757 | Recall 0.3832 | Acc 0.9527 | F1_score 0.5130 Epoch 00699 | Time(s) 0.63 | val_loss 0.4211 | Precision 0.7784 | Recall 0.3795 | Acc 0.9527 | F1_score 0.5102 Epoch 00799 | Time(s) 0.63 | val_loss 0.4465 | Precision 0.7778 | Recall 0.3813 | Acc 0.9527 | F1_score 0.5118 Epoch 00899 | Time(s) 0.63 | val_loss 0.4769 | Precision 0.7874 | Recall 0.3795 | Acc 0.9530 | F1_score 0.5121 Epoch 00999 | Time(s) 0.63 | val_loss 0.5053 | Precision 0.7684 | Recall 0.3767 | Acc 0.9521 | F1_score 0.5056 Confusion matrix: [[15464 123] [ 675 408]] # plot the metrics during training fig, ax = plt.subplots(1,3, figsize=(18,5), sharex=True) ax[0].plot(metrics[\"loss\"]['train'], label='unidir. train_loss', color='C0') ax[0].plot(metrics[\"loss\"]['val'], label='unidir. val_loss', color='C0', ls=':') ax[1].plot(metrics['f1_score']['val'], label='unidir. val_f1_score', color='C0') ax[2].plot(metrics['precision']['val'], label='unidir. val_precision', color='C0') ax[2].plot(metrics['recall']['val'], label='unidir. val_recall', color='C0', ls=':') ax[0].plot(metrics_bi[\"loss\"]['train'], label='bidir. train_loss', color='C3') ax[0].plot(metrics_bi[\"loss\"]['val'], label='bidir. val_loss', color='C3', ls=':') ax[1].plot(metrics_bi['f1_score']['val'], label='bidir. val_f1_score', color='C3') ax[2].plot(metrics_bi['precision']['val'], label='bidir. val_precision', color='C3') ax[2].plot(metrics_bi['recall']['val'], label='bidir. val_recall', color='C3', ls=':') ax[0].legend() ax[1].legend() ax[2].legend(); Training the GCN model with these parameters led to a poorer performance than the one reported in the paper by Weber et al. The bidirectional graph variant produced the better results. Therefore it is also probably the setting used in the paper. Even though the Bitcoin flow from one transaction to another is intuitively one-directional, this would mean that, in a GCN, the information would only flow downstream. Having a bidirectional flow of information through the edges of the graph in the GCN makes information available to each node both upstream and downstream from it. This greatly improved the performance of the GCN.\nModel Edges Dropout Precision Recall F1 score GCN (Weber et al.) 0. 0.812 0.512 0.628 GCN unidirectional 0. 0.3764 0.3823 0.3793 GCN bidirectional 0. 0.7860 0.3832 0.5152 In order to figure out, whether there are additional parameters that I did not yet consider in replicating the GCN approach, I performed a series of experiments changing further training parameters and comparing the results. This, in turn, was useful in understanding in what ways the model could be modified to increase its performance.\nAdditional experiments with GCNs Address overfitting by weight decay L2-regularization One observation from the previous learning curves is that the validation loss starts to increase again after ca. 400 epochs, a clear sign of overfitting. One way to address this is to add dropout to the model. In this case, I added weight-decay L2-regularization to improve training.\nI further added a learnable bias to the GCN to see if this improved its performance (it did slightly). The other parameters were left intact.\nquery = 'model==\"gcn\" and onlylocal==False and dropout==\"0\"' print(f\"Plotting {runs.query(query)['name'].nunique()} runs: {runs.query(query)['name'].unique()}\") g = sns.relplot('epoch', 'train_loss', col='weight_decay', hue='bidirectional', style='nobias', palette=sns.color_palette(\"Set1\", 2), kind='line', data=runs.query(query)); plt.ylim(0,1); g = sns.relplot('epoch', 'val_loss', col='weight_decay', hue='bidirectional', style='nobias', palette=sns.color_palette(\"Set1\", 2), kind='line', data=runs.query(query)) plt.ylim(0,1); Plotting 8 runs: ['super-sweep-14' 'blooming-sweep-13' 'vague-sweep-10' 'sandy-sweep-9' 'eternal-sweep-6' 'resilient-sweep-5' 'ethereal-sweep-2' 'vibrant-sweep-1'] query = 'model==\"gcn\" and onlylocal==False and dropout==0' print(f\"Plotting {runs.query(query)['name'].nunique()} runs: {runs.query(query)['name'].unique()}\") sns.relplot('epoch', 'val_f1_score', col='weight_decay', hue='bidirectional', style='nobias', palette=sns.color_palette(\"Set1\", 2), kind='line', data=runs.query(query)); Plotting 8 runs: ['super-sweep-14' 'blooming-sweep-13' 'vague-sweep-10' 'sandy-sweep-9' 'eternal-sweep-6' 'resilient-sweep-5' 'ethereal-sweep-2' 'vibrant-sweep-1'] Address overfitting by adding dropout Another way to address this is to add dropout to the model. In this case, I added dropout before the second GCN layer (meaning that the inputs to the 2nd layer will be dropped out with a certain probability. Adding dropout considerably increased the precision, meaning that the model predicts far fewer false negatives. The recall, on the other hand, remains largely unchanged. The best model was obtained with a dropout $p = 0.5$\nquery = 'model==\"gcn\" and bidirectional==True and onlylocal==False and nobias==\"False\" and weight_decay==\"0.0005\" and (dropout==0. or dropout==0.25 or dropout==\"0.5\")' print(f\"Plotting {runs.query(query)['name'].nunique()} runs: {runs.query(query)['name'].unique()}\") fig, ax = plt.subplots(1,2, figsize=(12,5)) g1 = sns.relplot('epoch', 'val_loss', col='bidirectional', hue='dropout', kind='line', palette=sns.color_palette(\"Set1\", 3), data=runs.query(query), ax=ax[0]) g2 = sns.relplot('epoch', 'train_loss', col='bidirectional', hue='dropout', kind='line', palette=sns.color_palette(\"Set1\", 3), data=runs.query(query), ax=ax[1]) plt.close(g1.fig) plt.close(g2.fig) plt.legend(title='dropout', labels=['0.0', '0.25', '0.5']) plt.setp(ax, ylim=(0,0.8)); Plotting 3 runs: ['faithful-sweep-3' 'quiet-deluge-19' 'sandy-sweep-9'] query = 'model==\"gcn\" and bidirectional==True and onlylocal==False and nobias==\"False\" and weight_decay==\"0.0005\" and (dropout==0. or dropout==0.25 or dropout==\"0.5\")' print(f\"Plotting {runs.query(query)['name'].nunique()} runs: {runs.query(query)['name'].unique()}\") fig, ax = plt.subplots(1,3, figsize=(18,5)) g1 = sns.relplot('epoch', 'val_f1_score', col='bidirectional', hue='dropout', kind='line', palette=sns.color_palette(\"Set1\", 3), data=runs.query(query), ax=ax[0]) g2 = sns.relplot('epoch', 'val_precision', col='bidirectional', hue='dropout', kind='line', palette=sns.color_palette(\"Set1\", 3), data=runs.query(query), ax=ax[1]) g3 = sns.relplot('epoch', 'val_recall', col='bidirectional', hue='dropout', kind='line', palette=sns.color_palette(\"Set1\", 3), data=runs.query(query), ax=ax[2]) plt.close(g1.fig) plt.close(g2.fig) plt.close(g3.fig) plt.legend(title='dropout', labels=['0.0', '0.25', '0.5']); Plotting 3 runs: ['faithful-sweep-3' 'quiet-deluge-19' 'sandy-sweep-9'] Training a GCN with local features only A question that arises from the paper is: how much does the graph-based information contribute to the performance of a GCN model compared to a more traditional non-graph-based approach? Weber et al. show that the node embeddings that can be extracted from a GCN can help boost other traditional models. This makes sense intuitively: because of the networked nature of the bitcoin transactions, knowing the context or “neighborhood” of a transaction should add important information.\nHowever, from the description of the Elliptic dataset we know that some of the features already contain information regarding the context of the transactions. In fact, 72 out of the 166 features are aggregated features. Therefore, I was curious to find out how would a GCN model perform with only the remaining 94 local features (including timestep) as inputs.\nI modified the model to limit the set of features to only the local ones (including timestep). The input node features thus now have a shape of (94,). This way we can assess the performance of a GCN without having to manually engineer features from their neighbors. In other words, we leave the feature engineering to the neural network itself.\n# consider only the first 94 features of the node feature matrix features_local = g_bi.ndata[\"features_matrix\"][:,0:94].float() print(f\"\"\"Number of features (all): {features.shape[1]}\"\"\") print(f\"\"\"Number of features (only local): {features_local.shape[1]}\"\"\") Number of features (all): 166 Number of features (only local): 94 # GCN model parameters #params = { # \"bidirectional\" : True, # \"n_hidden\" : 100, # \"n_layers\" : 2, # \"weight_decay\" : 5e-4, # \"bias\" : True, # \"dropout\" : 0.25, # \"epochs\" : 1000, # \"lr\" : 1e-3, # \"posweight\" : 0.7, #} # #model, metrics = train_eval_model(GCN, g_bi, features_local, **params) query = 'model==\"gcn\" and bidirectional==True and weight_decay==0.0005 and nobias==\"False\" and (dropout==0 or dropout==0.25 or dropout==0.5)' print(f\"Plotting {runs.query(query)['name'].nunique()} runs: {runs.query(query)['name'].unique()}\") sns.relplot('epoch', 'val_f1_score', col='onlylocal', hue='dropout', palette=sns.color_palette(\"Set1\", 3), kind='line', data=runs.query(query)); Plotting 6 runs: ['divine-sweep-4' 'faithful-sweep-3' 'zesty-microwave-76' 'quiet-deluge-19' 'divine-sweep-11' 'sandy-sweep-9'] We can see that the perfomance of the GCN improved by addition of dropout when all features were considered, but not when using only local features. This makes sense, as the aggregated features are likely less essential than the local ones.\nRandom Forest benchmark A sobering additional finding from the paper by Weber et al. was the out-of-the-box excellent performance of a simple Random Forest in correctly classifying the transactions as licit or illicit. I was able to replicate these results too for two different sets of features:\nAll features Only local features from sklearn.ensemble import RandomForestClassifier # function to evaluate the model def evaluate_rfc(model, features, labels, mask): \"\"\"Calculate the loss, accuracy, precision, recall and f1_score for the masked data\"\"\" pred_rfc = model.predict(features[mask]) labels = labels[mask] p, r, f, _ = precision_recall_fscore_support(labels, pred_rfc) return p[1], r[1], f[1] # confusion matrix def eval_confusion_matrix_rfc(model, features, labels, mask): pred_rfc = model.predict(features[mask]) labels = labels[mask] print(confusion_matrix(labels, pred_rfc)) Using all features (local + aggregated) rfc = RandomForestClassifier(n_estimators=50, max_features=50, random_state=seed) rfc.fit(features[train_indices], labels[train_indices]) p, r, f1 = evaluate_rfc(rfc, features, labels, val_indices) print( f\"Precision {p:.4f} | Recall {r:.4f} | \" f\"F1 score {f1:.4f}\" ) print(\"Confusion matrix:\") eval_confusion_matrix_rfc(rfc, features, labels, val_indices) Precision 0.9167 | Recall 0.7211 | F1 score 0.8072 Confusion matrix: [[15516 71] [ 302 781]] The best results were obtained using as input all available features (local (including timestep) + aggregated). Both precision and recall of this model were high. This is confirmed by looking at the confusion matrix. The model predicts nearly no false positives and less than 30% of the illicit transactions are falsely labeled as negatives. It is a very good performance for such a simple model.\nUsing local features only rfc_local = RandomForestClassifier(n_estimators=50, max_features=50, random_state=seed) rfc_local.fit(features_local[train_indices], labels[train_indices]) p, r, f1 = evaluate_rfc(rfc_local, features_local, labels, val_indices) print( f\"Precision {p:.4f} | Recall {r:.4f} | \" f\"F1 score {f1:.4f}\" ) eval_confusion_matrix_rfc(rfc_local, features_local, labels, val_indices) Precision 0.8749 | Recall 0.7036 | F1 score 0.7799 [[15478 109] [ 321 762]] Removing the aggregated features from the input to the Random Forest (leaving other parameters equal) impairs its performance both in precision and recall, but not dramatically. It still is a very good model working out of the box. It makes sense that not having information about the immediate neighbors of a transaction would produce a worse-performing model.\nComparison to GCN So what do the resuls of the Random Forest tell us about GCNs and other deep learning techniques? Should we dismiss them and focus on simpler models instead? While this analysis shows that it pays to start simple and see how well we can tackle a task using classic machine learning methods first, there still are valid reasons why one should consider (graph) neural networks too.\nDeep learning on graphs is cool! Now, seriously,\nThere is information contained in the connections between data points, which is not being considered by a classical machine learning approach, unless carefully crafted features are available, which requires time and specific knowledge The features available to a different dataset may be less informative than those in the Elliptic dataset, and therefore insufficient to produce a good-enough Random Forest model There may even be situations when no intrinsic node features are available and we still want to be able to classify transactions. This would still be possible using a GCN but not with a Random Forest Progress in unleashing the potential of GCNs can only be obtained by researching these networks Now let’s take a look at a different kind of graph-based model that, I figured, might be a good option for the bitcoin transaction classification task.\nLong-range propagation of label predictions using APPNP Let’s recapitulate.\nWe trained a complex GCN model to classify bitcoin transactions as licit or illicit and improved its performance by better parameters and training We found that a Random Forest model performed better than our complex GCN, almost effortlessly Why is this? In order to look for the answer it pays to take a closer look at how the transaction graph is structured. Let’s take for example the transactions of the last timestep.\nwith sns.axes_style('white'): plt.figure(figsize=(8,6)) node_label = list(nx.get_node_attributes(g_nx_t_list[49-1], 'label').values()) mapping = {-1:'grey', 0:'C0', 1:'C3'} node_color = [mapping[l] for l in node_label] nx.draw_networkx(g_nx_t_list[49-1], node_size=10, node_color=node_color, with_labels=False, width=0.2, alpha=0.8, arrowsize=8) leg = plt.legend(['unlabeled', 'licit', 'illicit']) leg.legendHandles[0].set_color('grey') leg.legendHandles[1].set_color('C0') leg.legendHandles[2].set_color('C3') plt.show() /anaconda3/envs/dgl/lib/python3.7/site-packages/networkx/drawing/nx_pylab.py:579: MatplotlibDeprecationWarning: The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. if not cb.iterable(width): /anaconda3/envs/dgl/lib/python3.7/site-packages/networkx/drawing/nx_pylab.py:676: MatplotlibDeprecationWarning: The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. if cb.iterable(node_size): # many node sizes The original paper describing the GCN applied it to the node classification of papers in the Cora dataset. The bitcoin transaction graph is much larger and less dense than the Cora graph. One can hipothesize that for a denser graph with a higher average degree, each node could receive more information from its neighbors that would help make a GCN make a better prediction.\nGraph Nodes Density Average degree Elliptic (bitcoin transactions) 203769 0.0003177 2.30 Cora (citations) 2708 0.0014812 3.90 Furthermore, if we consider a simple chain of transactions (like the ones seen in the graph visualization), a node would receive information only from the previous node and pass it on only to the following node in the chain. In such a situation, it could be that the range of neighbors that feed the classification of any given node is too short and often not sufficient for a correct prediction. If this is the case, then considering a longer-ranging neighborhood could help train a better classification model.\nApproximated Personalized Propagation of Neural Predictions (APPNP) Enter APPNP. This model was recently proposed as a way to reconcile the best of two worlds: neural message passing algorithms (in principle like the GCN), and personalized pagerank.\nIn PageRank, a measure of how central or important a node is calculated as a function of its connections and the importance of its neighbors. The pagerank $PR$ of a node $u$ is:\n$$PR(u) = (1-d){1 \\over {N}} + d \\sum_{v \\in \\mathcal{N} (u)} {PR(v) \\over D_{out}(v)}$$where $N$ is the total number of nodes, $D_{out}$ is the outdegree, and $v$ is a node of the set of neighbors $\\mathcal{N}$ of $u$. Pagerank ultimately requires each node to iteratively receive information from its neighbors until the pagerank stops changing and converges. The algorithm assigns a damping factor $d$, which can be understood by imagining a random surfer visiting nodes in the graph. The surfer would visit any given node moving along the edges of the graph until it reaches it with probability $d$. However, that surfer could also visit a node by randomly teleporting to it from elsewhere in the graph with probability $(1-d)$.\nIn APPNP, a multilayer perceptron takes the node features as input and outputs prediction probabilities $H^{0} = f_{MLP}(X)$. These are then propagated through graph for a $K$ number of iterations.\n$$H^{t+1} = (1-\\alpha)\\left(\\hat{D}^{-1/2} \\hat{A} \\hat{D}^{-1/2} H^{t}\\right) + \\alpha H^{0}$$If you think the APPNP and the pagerank equations look similar is because they are. The teleportation probability $\\alpha$ corresponds to the damping factor in pagerank. It tells us that for each iteration, the predictions for a node will depend on the normalized graph laplacian with probability $(1-\\alpha)$ or on the output of the MLP with probability $\\alpha$.\nSo how does do we train the APPNP model? I modified an implementation of an APPNP layer from DGL. In the original paper by Klicpera et al. (2019) they use an architecture consisting of a 2-layer MLP with 64 hidden units each, followed by the propagation component given by the APPNP equation. In order to use a comparable architecture to the GCN model, I decided to set the number of hidden units to 100 (same as previous section).\nI tested several combinations of values for $K$ and $\\alpha$. However, a more exhaustive hyperparameter search is needed to find the best possible configuration. Moreover, I used two versions of the feature set: all features vs. only local features.\nThe best F1 score was obtained for $K = 20$ propagation iterations and a teleportation probability $\\alpha = 0.2$. Interestingly, the model trained with only local features performed better than using all features. Furthermore, the addition of dropout to the network had a positive effect when using all features, but a negative effect if only local features were considered.\nclass APPNP(nn.Module): def __init__( self, g, in_feats, n_hidden, n_classes, n_layers, activation, feat_drop, edge_drop, alpha, k, ): super(APPNP, self).__init__() self.g = g self.layers = nn.ModuleList() # input layer self.layers.append(nn.Linear(in_feats, n_hidden)) # hidden layers for _ in range(n_layers - 2): self.layers.append(nn.Linear(n_hidden, n_hidden)) # output layer self.layers.append(nn.Linear(n_hidden, n_classes)) self.activation = activation if feat_drop: self.feat_drop = nn.Dropout(feat_drop) else: self.feat_drop = lambda x: x self.propagate = APPNPConv(k, alpha, edge_drop) self.reset_parameters() def reset_parameters(self): for layer in self.layers: layer.reset_parameters() def forward(self, features): # prediction step h = features h = self.feat_drop(h) h = self.activation(self.layers[0](h)) for layer in self.layers[1:-1]: h = self.activation(layer(h)) h = self.layers[-1](self.feat_drop(h)) # propagation step h = self.propagate(self.g, h) return h query = 'model==\"appnp\" and nhidden==100 and bidirectional==True and weight_decay==0.0005 and nobias==\"False\" and (dropout==0 or dropout==0.2 or dropout==0.25)' print(f\"Plotting {runs.query(query)['name'].nunique()} runs: {runs.query(query)['name'].unique()}\") sns.relplot('epoch', 'val_f1_score', row='onlylocal', col='alpha', hue='k', style='dropout', palette=sns.color_palette(\"Set1\", 2), kind='line', data=runs.query(query)); Plotting 12 runs: ['tough-sweep-4' 'fluent-sweep-3' 'happy-sweep-2' 'eager-sweep-1' 'vivid-sweep-2' 'copper-sweep-1' 'classic-sweep-10' 'confused-sweep-9' 'generous-sweep-6' 'dulcet-sweep-5' 'avid-sweep-2' 'giddy-sweep-1'] MLP benchmark But just how much of an effect does the incorporation of the graph structure have on the performance of the model? How much of it is simply due to the MLP that is trained on top of the propagation phase of the APPNP? In order to assess this, I trained an MLP model with the same architecture and for the two different versions of the features set.\nThe results show that the APPNP model has a better F1 score than the MLP after 1000 epochs, both for the only local feature set and the full feature set. With respect to precision and F1 score, the addition of dropout was beneficial when using all features and prejudicial when using the local features only. In contrast, recall was slightly improved by adding dropout to both feature variants.\nclass MLP(nn.Module): def __init__( self, in_feats, n_hidden, n_classes, n_layers, activation, feat_drop, ): super(MLP, self).__init__() self.layers = nn.ModuleList() # input layer self.layers.append(nn.Linear(in_feats, n_hidden)) # hidden layers for _ in range(n_layers - 2): self.layers.append(nn.Linear(n_hidden, n_hidden)) # output layer self.layers.append(nn.Linear(n_hidden, n_classes)) self.activation = activation if feat_drop: self.feat_drop = nn.Dropout(feat_drop) else: self.feat_drop = lambda x: x self.reset_parameters() def reset_parameters(self): for layer in self.layers: layer.reset_parameters() def forward(self, features): # prediction step h = features h = self.feat_drop(h) h = self.activation(self.layers[0](h)) for layer in self.layers[1:-1]: h = self.activation(layer(h)) h = self.layers[-1](self.feat_drop(h)) return h #hide_input query = '((model==\"appnp\" and k==\"20.0\" and alpha==\"0.2\") or model==\"mlp\") and bidirectional==True and weight_decay==0.0005 and nobias==\"False\" and (dropout==0 or dropout==0.2)' print(f\"Plotting {runs.query(query)['name'].nunique()} runs: {runs.query(query)['name'].unique()}\") sns.relplot('epoch', 'val_f1_score', col='onlylocal', hue='model', style='dropout', palette=sns.color_palette(\"Set1\", 2), kind='line', data=runs.query(query)); Plotting 8 runs: ['tough-sweep-4' 'fluent-sweep-3' 'happy-sweep-2' 'crisp-sweep-4' 'lucky-sweep-3' 'summer-sweep-2' 'eager-sweep-1' 'sandy-sweep-1'] #hide_input query = '((model==\"appnp\" and k==\"20.0\" and alpha==\"0.2\") or model==\"mlp\") and bidirectional==True and weight_decay==0.0005 and nobias==\"False\" and (dropout==0 or dropout==0.2)' print(f\"Plotting {runs.query(query)['name'].nunique()} runs: {runs.query(query)['name'].unique()}\") sns.relplot('epoch', 'val_precision', col='onlylocal', hue='model', style='dropout', palette=sns.color_palette(\"Set1\", 2), kind='line', data=runs.query(query)); Plotting 8 runs: ['tough-sweep-4' 'fluent-sweep-3' 'happy-sweep-2' 'crisp-sweep-4' 'lucky-sweep-3' 'summer-sweep-2' 'eager-sweep-1' 'sandy-sweep-1'] #hide_input query = '((model==\"appnp\" and k==\"20.0\" and alpha==\"0.2\") or model==\"mlp\") and bidirectional==True and weight_decay==0.0005 and nobias==\"False\" and (dropout==0 or dropout==0.2)' print(f\"Plotting {runs.query(query)['name'].nunique()} runs: {runs.query(query)['name'].unique()}\") sns.relplot('epoch', 'val_recall', col='onlylocal', hue='model', style='dropout', palette=sns.color_palette(\"Set1\", 2), kind='line', data=runs.query(query)); Plotting 8 runs: ['tough-sweep-4' 'fluent-sweep-3' 'happy-sweep-2' 'crisp-sweep-4' 'lucky-sweep-3' 'summer-sweep-2' 'eager-sweep-1' 'sandy-sweep-1'] Putting it all together Please refer to my tl;dr :)\n#hide_input query = '''((onlylocal==True and dropout==0) or (onlylocal==False and (dropout==0.2 or dropout==0.5))) and ((model==\"appnp\" and k==\"20.0\" and alpha==\"0.2\" and (dropout==0. or dropout==0.2)) or (model==\"mlp\" and (dropout==0. or dropout==0.2)) or (model==\"gcn\" and (dropout==0. or dropout==0.5))) and bidirectional==True and weight_decay==0.0005 and nobias==\"False\"'''.replace('\\n',' ') g1 = sns.relplot('epoch', 'val_f1_score', col='onlylocal', hue='model', style='dropout', palette=sns.color_palette(\"Set1\", 3), kind='line', data=runs.query(query)); g1.axes.flat[0].axhline(0.8072, c='k', alpha=0.8, ls='-.', lw=1) g1.axes.flat[0].text(1,0.815,'RandomForest') g1.axes.flat[1].axhline(0.7799, c='k', alpha=0.8, ls='-.', lw=1) g1.axes.flat[1].text(1,0.785,'RandomForest') plt.suptitle('Performance of the best models of each class using all features vs. only local features', y=1.02); ","wordCount":"6433","inLanguage":"en","image":"https://www.arcosdiaz.com/images/bitcoin_graph_thumb.png","datePublished":"2019-12-15T00:00:00Z","dateModified":"2019-12-15T00:00:00Z","author":{"@type":"Person","name":"Dario Arcos-Díaz"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.arcosdiaz.com/blog/posts/2019-12-15-btc-fraud-detection/"},"publisher":{"@type":"Organization","name":"Dario Arcos-Díaz, PhD","logo":{"@type":"ImageObject","url":"https://www.arcosdiaz.com/blog/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://www.arcosdiaz.com/blog/ accesskey=h title="Dario Arcos-Díaz, PhD (Alt + H)">Dario Arcos-Díaz, PhD</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://www.arcosdiaz.com/blog/about/ title=About><span>About</span></a></li><li><a href=https://www.arcosdiaz.com/blog/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://www.arcosdiaz.com/blog/>Home</a>&nbsp;»&nbsp;<a href=https://www.arcosdiaz.com/blog/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Graph Convolutional Networks for Fraud Detection of Bitcoin Transactions</h1><div class=post-description>Detecting fraudulent transactions is essential in keeping financial systems trustworthy. Here I illustrate an end-to-end approach of node classification by graph neural networks to identify suspicious transactions.</div><div class=post-meta><span title='2019-12-15 00:00:00 +0000 UTC'>December 15, 2019</span>&nbsp;·&nbsp;<span>31 min</span>&nbsp;·&nbsp;<span>Dario Arcos-Díaz</span></div></header><figure class=entry-cover><img loading=eager src=https://www.arcosdiaz.com/images/bitcoin_graph_thumb.png alt></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#tldr aria-label=tl;dr>tl;dr</a></li><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#transaction-data aria-label="Transaction data">Transaction data</a><ul><li><a href=#correlation-analysis aria-label="Correlation analysis">Correlation analysis</a></li></ul></li><li><a href=#constructing-the-transaction-graph aria-label="Constructing the transaction graph">Constructing the transaction graph</a><ul><li><a href=#graph-metrics aria-label="Graph metrics">Graph metrics</a></li></ul></li><li><a href=#training-a-graph-convolutional-network aria-label="Training a Graph Convolutional Network">Training a Graph Convolutional Network</a><ul><li><a href=#graph-creation aria-label="Graph creation">Graph creation</a></li><li><a href=#graph-normalizaiton aria-label="Graph normalizaiton">Graph normalizaiton</a></li><li><a href=#trainvalidation-data-splitting aria-label="Train/validation data splitting">Train/validation data splitting</a></li><li><a href=#gcn-model-architecture aria-label="GCN model architecture">GCN model architecture</a></li><li><a href=#model-training aria-label="Model training">Model training</a></li></ul></li><li><a href=#additional-experiments-with-gcns aria-label="Additional experiments with GCNs">Additional experiments with GCNs</a><ul><li><a href=#address-overfitting-by-weight-decay-l2-regularization aria-label="Address overfitting by weight decay L2-regularization">Address overfitting by weight decay L2-regularization</a></li><li><a href=#address-overfitting-by-adding-dropout aria-label="Address overfitting by adding dropout">Address overfitting by adding dropout</a></li><li><a href=#training-a-gcn-with-local-features-only aria-label="Training a GCN with local features only">Training a GCN with local features only</a></li></ul></li><li><a href=#random-forest-benchmark aria-label="Random Forest benchmark">Random Forest benchmark</a><ul><li><a href=#using-all-features-local--aggregated aria-label="Using all features (local + aggregated)">Using all features (local + aggregated)</a></li><li><a href=#using-local-features-only aria-label="Using local features only">Using local features only</a></li><li><a href=#comparison-to-gcn aria-label="Comparison to GCN">Comparison to GCN</a></li></ul></li><li><a href=#long-range-propagation-of-label-predictions-using-appnp aria-label="Long-range propagation of label predictions using APPNP">Long-range propagation of label predictions using APPNP</a></li><li><a href=#approximated-personalized-propagation-of-neural-predictions-appnp aria-label="Approximated Personalized Propagation of Neural Predictions (APPNP)">Approximated Personalized Propagation of Neural Predictions (APPNP)</a><ul><li><a href=#mlp-benchmark aria-label="MLP benchmark">MLP benchmark</a></li><li><a href=#putting-it-all-together aria-label="Putting it all together">Putting it all together</a></li></ul></li></ul></div></details></div><div class=post-content><h2 id=tldr>tl;dr<a hidden class=anchor aria-hidden=true href=#tldr>#</a></h2><p>I trained 4 different types of models to classify bitcoin transactions. For each, two versions of the feature set were used: <em>all features</em> (local + neighborhood-aggregated) and <em>only local features</em> (without neighborhood information).</p><ul><li>The best model was a Random Forest trained with all features: its performance was impaired when the aggregated features were removed.</li><li>The best graph-based neural network model was APPNP and its performance was better when only local features were used. APPNP performed better than an MLP with comparable complexity, indicating that the graph structure information gave it an advantage.</li><li>Finally, the best GCN model required using all features and several strategies to reduce overfitting.</li></ul><p>The excellent performance of a Random Forest shows that it makes sense to consider simple models when faced with a new task. It also indicates that the individual node features in the Elliptic dataset are already informative enough to make good predictions. It would be interesting to explore how the model performs, when fewer samples and/or features are available for training.</p><p>A shallow GCN with 2 layers might not be a good choice for node classification of a graph as sparse as the bitcoin transaction graph. If a node has few incoming edges, a graph convolution may not have enough neighbors with features to aggregate.</p><p>An interesting solution is provided by the APPNP model, which combines message passing with the teleportation principle of personalized pagerank. The long-range (20 iterations in the best model) of the predictions propagation through the network is an aspect that deserves further attention in the future.</p><p>The main performance metrics for comparison were:</p><table><thead><tr><th>Model</th><th>Features</th><th style=text-align:right>Dropout</th><th style=text-align:right>Precision</th><th style=text-align:right>Recall</th><th style=text-align:right>F1 score</th></tr></thead><tbody><tr><td>GCN</td><td>all</td><td style=text-align:right>0.5</td><td style=text-align:right>0.8051</td><td style=text-align:right>0.4958</td><td style=text-align:right>0.6137</td></tr><tr><td>GCN</td><td>local</td><td style=text-align:right>0.</td><td style=text-align:right>0.6667</td><td style=text-align:right>0.4617</td><td style=text-align:right>0.5456</td></tr><tr><td>APPNP</td><td>all</td><td style=text-align:right>0.2</td><td style=text-align:right>0.7791</td><td style=text-align:right>0.6251</td><td style=text-align:right>0.6936</td></tr><tr><td>APPNP</td><td>local</td><td style=text-align:right>0.</td><td style=text-align:right><strong>0.8158</strong></td><td style=text-align:right><strong>0.6787</strong></td><td style=text-align:right><strong>0.7409</strong></td></tr><tr><td>MLP</td><td>all</td><td style=text-align:right>0.2</td><td style=text-align:right>0.6538</td><td style=text-align:right>0.6593</td><td style=text-align:right>0.6565</td></tr><tr><td>MLP</td><td>local</td><td style=text-align:right>0.</td><td style=text-align:right>0.7799</td><td style=text-align:right>0.6740</td><td style=text-align:right>0.7231</td></tr><tr><td>RandomForest</td><td>all</td><td style=text-align:right></td><td style=text-align:right><strong>0.9167</strong></td><td style=text-align:right><strong>0.7211</strong></td><td style=text-align:right><strong>0.8072</strong></td></tr><tr><td>RandomForest</td><td>local</td><td style=text-align:right></td><td style=text-align:right>0.8749</td><td style=text-align:right>0.7036</td><td style=text-align:right>0.7799</td></tr></tbody></table><p><strong>Disclaimer:</strong> This was a hobby project done mostly nocturnally and on the weekends out of pure fascination for graph theory and neural networks. Although I made every effort to apply scientific rigor, these results constitute an initial exploration and should not be considered an exhaustive analysis. Importantly, due to the random nature of certain parameters (e.g. dropout), multiple repetitions of the experiments using different random seeds and/or different validation splits are necessary for a conclusive judgement.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#hide_input</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> seaborn <span style=color:#66d9ef>as</span> sns
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span><span style=color:#f92672>%</span>matplotlib inline
</span></span><span style=display:flex><span>matplotlib<span style=color:#f92672>.</span>rcParams[<span style=color:#e6db74>&#39;figure.dpi&#39;</span>] <span style=color:#f92672>=</span> <span style=color:#ae81ff>300</span>
</span></span><span style=display:flex><span>path <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>realpath(<span style=color:#e6db74>&#39;.&#39;</span>)
</span></span><span style=display:flex><span>runs_config <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(path<span style=color:#f92672>+</span><span style=color:#e6db74>&#39;/experiments_summary.csv&#39;</span>)
</span></span><span style=display:flex><span>runs_metrics <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(path<span style=color:#f92672>+</span><span style=color:#e6db74>&#39;/experiments_metrics.csv&#39;</span>)
</span></span><span style=display:flex><span>runs <span style=color:#f92672>=</span> runs_metrics<span style=color:#f92672>.</span>merge(runs_config, left_on<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;name&#39;</span>, right_on<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;name&#39;</span>, suffixes<span style=color:#f92672>=</span>(<span style=color:#e6db74>&#39;&#39;</span>,<span style=color:#e6db74>&#39;_&#39;</span>))
</span></span><span style=display:flex><span>runs<span style=color:#f92672>.</span>rename(columns<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#39;_step&#39;</span>:<span style=color:#e6db74>&#39;epoch&#39;</span>}, inplace<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>runs[<span style=color:#e6db74>&#39;nobias&#39;</span>] <span style=color:#f92672>=</span> runs[<span style=color:#e6db74>&#39;nobias&#39;</span>]<span style=color:#f92672>.</span>astype(str)
</span></span><span style=display:flex><span>runs[<span style=color:#e6db74>&#39;dropout&#39;</span>] <span style=color:#f92672>=</span> runs[<span style=color:#e6db74>&#39;dropout&#39;</span>]<span style=color:#f92672>.</span>astype(float)
</span></span><span style=display:flex><span>runs[<span style=color:#e6db74>&#39;k&#39;</span>] <span style=color:#f92672>=</span> runs[<span style=color:#e6db74>&#39;k&#39;</span>]<span style=color:#f92672>.</span>astype(str)
</span></span><span style=display:flex><span>runs[<span style=color:#e6db74>&#39;alpha&#39;</span>] <span style=color:#f92672>=</span> runs[<span style=color:#e6db74>&#39;alpha&#39;</span>]<span style=color:#f92672>.</span>astype(str)
</span></span><span style=display:flex><span>runs<span style=color:#f92672>.</span>query(<span style=color:#e6db74>&#39;nhidden==&#34;100&#34; and _step_&gt;=999.0&#39;</span>, inplace<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>query <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;&#39;&#39;((onlylocal==True and dropout==0) or (onlylocal==False  and (dropout==0.2 or dropout==0.5))) 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>and ((model==&#34;appnp&#34; and k==&#34;20.0&#34; and alpha==&#34;0.2&#34; and (dropout==0. or dropout==0.2)) 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>or (model==&#34;mlp&#34; and (dropout==0. or dropout==0.2)) 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>or (model==&#34;gcn&#34;  and (dropout==0. or dropout==0.5))) 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>and bidirectional==True and weight_decay==0.0005 and nobias==&#34;False&#34;&#39;&#39;&#39;</span><span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#39;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#39;</span>,<span style=color:#e6db74>&#39; &#39;</span>)
</span></span><span style=display:flex><span>g1 <span style=color:#f92672>=</span> sns<span style=color:#f92672>.</span>relplot(<span style=color:#e6db74>&#39;epoch&#39;</span>, <span style=color:#e6db74>&#39;val_f1_score&#39;</span>, col<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;onlylocal&#39;</span>, hue<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;model&#39;</span>, style<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;dropout&#39;</span>, palette<span style=color:#f92672>=</span>sns<span style=color:#f92672>.</span>color_palette(<span style=color:#e6db74>&#34;Set1&#34;</span>, <span style=color:#ae81ff>3</span>), kind<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;line&#39;</span>, data<span style=color:#f92672>=</span>runs<span style=color:#f92672>.</span>query(query));
</span></span><span style=display:flex><span>g1<span style=color:#f92672>.</span>axes<span style=color:#f92672>.</span>flat[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>axhline(<span style=color:#ae81ff>0.8072</span>, c<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;k&#39;</span>, alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.8</span>, ls<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;-.&#39;</span>, lw<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>g1<span style=color:#f92672>.</span>axes<span style=color:#f92672>.</span>flat[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>text(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>0.815</span>,<span style=color:#e6db74>&#39;RandomForest&#39;</span>)
</span></span><span style=display:flex><span>g1<span style=color:#f92672>.</span>axes<span style=color:#f92672>.</span>flat[<span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>axhline(<span style=color:#ae81ff>0.7799</span>, c<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;k&#39;</span>, alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.8</span>, ls<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;-.&#39;</span>, lw<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>g1<span style=color:#f92672>.</span>axes<span style=color:#f92672>.</span>flat[<span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>text(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>0.785</span>,<span style=color:#e6db74>&#39;RandomForest&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>suptitle(<span style=color:#e6db74>&#39;Performance of the best models of each class using all features vs. only local features&#39;</span>, y<span style=color:#f92672>=</span><span style=color:#ae81ff>1.02</span>);
</span></span></code></pre></div><p><img alt=png loading=lazy src=/images/2019-12-15-btc-fraud-detection_files/2019-12-15-btc-fraud-detection_2_0.png></p><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>The <a href=https://www.kaggle.com/ellipticco/elliptic-data-set>Elliptic Data Set</a> consists of anonymized transactions collected from the bitcoin exchange during 49 distinct time-periods. The transactions are represented as a graph containing 203769 nodes (transactions) and 234355 edges (bitcoin flow from one transaction to another). A subset of the transactions are labeled as licit or illicit.
A detailed description of the dataset and an initial approach applying graph convolutional networks (GCNs) for the task of node classification has been addressed by:</p><blockquote><p>M. Weber, G. Domeniconi, J. Chen, D. K. I. Weidele, C. Bellei, T. Robinson, C. E. Leiserson, <a href=https://arxiv.org/abs/1908.02591>&ldquo;Anti-Money Laundering in Bitcoin: Experimenting with Graph Convolutional Networks for Financial Forensics&rdquo;</a>, KDD ’19 Workshop on Anomaly Detection in Finance, August 2019, Anchorage, AK, USA.</p></blockquote><p>In this notebook, I will take a closer look to how graph-based neural networks can be applied to this task and propose possible directions for future analyses.</p><p>Due to the longer training times and for reproducibility, the experiments were all run using the script in <a href>models.py</a> and all runs and metrics were tracked on <a href=wandb.com>Weights&amp;Biases</a>. All results were exported to a csv file using <a href>this script</a> and loaded onto this notebook for visualization.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#collapse-hide</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> random
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> time
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> dgl
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> networkx <span style=color:#66d9ef>as</span> nx
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn.functional <span style=color:#66d9ef>as</span> F
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> dgl.nn.pytorch <span style=color:#f92672>import</span> GraphConv
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.metrics <span style=color:#f92672>import</span> confusion_matrix, precision_recall_fscore_support
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> seaborn <span style=color:#66d9ef>as</span> sns
</span></span><span style=display:flex><span><span style=color:#f92672>%</span>matplotlib inline
</span></span><span style=display:flex><span>matplotlib<span style=color:#f92672>.</span>rcParams[<span style=color:#e6db74>&#39;figure.dpi&#39;</span>] <span style=color:#f92672>=</span> <span style=color:#ae81ff>300</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># set random seeds</span>
</span></span><span style=display:flex><span>seed <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>random<span style=color:#f92672>.</span>seed(seed)
</span></span><span style=display:flex><span>np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>seed(seed)
</span></span><span style=display:flex><span>dgl<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>seed(seed)
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>manual_seed(seed);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>path <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>realpath(<span style=color:#e6db74>&#39;.&#39;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#collapse-hide</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># load experiment results exported from Weights&amp;Biases</span>
</span></span><span style=display:flex><span>runs_config <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(path<span style=color:#f92672>+</span><span style=color:#e6db74>&#39;/experiments_summary.csv&#39;</span>)
</span></span><span style=display:flex><span>runs_metrics <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(path<span style=color:#f92672>+</span><span style=color:#e6db74>&#39;/experiments_metrics.csv&#39;</span>)
</span></span><span style=display:flex><span>runs <span style=color:#f92672>=</span> runs_metrics<span style=color:#f92672>.</span>merge(runs_config, left_on<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;name&#39;</span>, right_on<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;name&#39;</span>, suffixes<span style=color:#f92672>=</span>(<span style=color:#e6db74>&#39;&#39;</span>,<span style=color:#e6db74>&#39;_&#39;</span>))
</span></span><span style=display:flex><span>runs<span style=color:#f92672>.</span>rename(columns<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#39;_step&#39;</span>:<span style=color:#e6db74>&#39;epoch&#39;</span>}, inplace<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>runs[<span style=color:#e6db74>&#39;nobias&#39;</span>] <span style=color:#f92672>=</span> runs[<span style=color:#e6db74>&#39;nobias&#39;</span>]<span style=color:#f92672>.</span>astype(str)
</span></span><span style=display:flex><span>runs[<span style=color:#e6db74>&#39;dropout&#39;</span>] <span style=color:#f92672>=</span> runs[<span style=color:#e6db74>&#39;dropout&#39;</span>]<span style=color:#f92672>.</span>astype(float)
</span></span><span style=display:flex><span>runs[<span style=color:#e6db74>&#39;k&#39;</span>] <span style=color:#f92672>=</span> runs[<span style=color:#e6db74>&#39;k&#39;</span>]<span style=color:#f92672>.</span>astype(str)
</span></span><span style=display:flex><span>runs[<span style=color:#e6db74>&#39;alpha&#39;</span>] <span style=color:#f92672>=</span> runs[<span style=color:#e6db74>&#39;alpha&#39;</span>]<span style=color:#f92672>.</span>astype(str)
</span></span><span style=display:flex><span>runs<span style=color:#f92672>.</span>query(<span style=color:#e6db74>&#39;nhidden==&#34;100&#34; and _step_&gt;=999.0&#39;</span>, inplace<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><h2 id=transaction-data>Transaction data<a hidden class=anchor aria-hidden=true href=#transaction-data>#</a></h2><p>Three tables are initially available to download from Kaggle&rsquo;s dataset repository:</p><ul><li>An edgelist: the edges between bitcoin transactions (nodes identified by transaction id) necessary to build the graph</li><li>A classes table: label for each transaction can be licit, illicit, or unknown</li><li>A features table with 167 columns</li><li>Transaction id</li><li>Timestep: consecutive periods of time for which all bitcoin flows are translated to edges in a graph Edges exist only between transactions within the same timestep</li><li>93 local features, i.e. intrinsic properties of the transactions themselves such as amount, transaction fee, etc.</li><li>72 aggregated features with information about the immediate neighborhood of each node, e.g. sum of amounts of the neighboring transactions</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># load data</span>
</span></span><span style=display:flex><span>df_edges <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(path <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;/elliptic_bitcoin_dataset/elliptic_txs_edgelist.csv&#34;</span>)
</span></span><span style=display:flex><span>df_classes <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(path <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;/elliptic_bitcoin_dataset/elliptic_txs_classes.csv&#34;</span>)
</span></span><span style=display:flex><span>df_features <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(
</span></span><span style=display:flex><span>    path <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;/elliptic_bitcoin_dataset/elliptic_txs_features.csv&#34;</span>, header<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># rename the classes to ints that can be handled by pytorch as labels</span>
</span></span><span style=display:flex><span>df_classes[<span style=color:#e6db74>&#34;label&#34;</span>] <span style=color:#f92672>=</span> df_classes[<span style=color:#e6db74>&#34;class&#34;</span>]<span style=color:#f92672>.</span>replace(
</span></span><span style=display:flex><span>    {<span style=color:#e6db74>&#34;unknown&#34;</span>: <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>,  <span style=color:#75715e># unlabeled nodes</span>
</span></span><span style=display:flex><span>     <span style=color:#e6db74>&#34;2&#34;</span>: <span style=color:#ae81ff>0</span>,  <span style=color:#75715e># labeled licit nodes</span>
</span></span><span style=display:flex><span>     <span style=color:#75715e>#&#34;1&#34;: 1,  # labeled illicit nodes</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>)<span style=color:#f92672>.</span>astype(int)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># rename features according to data description in paper</span>
</span></span><span style=display:flex><span>rename_dict <span style=color:#f92672>=</span> dict(
</span></span><span style=display:flex><span>    zip(
</span></span><span style=display:flex><span>        range(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>167</span>),
</span></span><span style=display:flex><span>        [<span style=color:#e6db74>&#34;txId&#34;</span>, <span style=color:#e6db74>&#34;time_step&#34;</span>]
</span></span><span style=display:flex><span>        <span style=color:#f92672>+</span> [<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;local_</span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>:</span><span style=color:#e6db74>02d</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span> <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>94</span>)]
</span></span><span style=display:flex><span>        <span style=color:#f92672>+</span> [<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;aggr_</span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>:</span><span style=color:#e6db74>02d</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span> <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>73</span>)],
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>df_features<span style=color:#f92672>.</span>rename(columns<span style=color:#f92672>=</span>rename_dict, inplace<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># check missing data</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Number of missing data points: </span><span style=color:#e6db74>{</span>df_features<span style=color:#f92672>.</span>isna()<span style=color:#f92672>.</span>sum()<span style=color:#f92672>.</span>sum()<span style=color:#f92672>+</span>df_classes<span style=color:#f92672>.</span>isna()<span style=color:#f92672>.</span>sum()<span style=color:#f92672>.</span>sum()<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Number of nodes (transactions): </span><span style=color:#e6db74>{</span>df_features[<span style=color:#e6db74>&#39;txId&#39;</span>]<span style=color:#f92672>.</span>nunique()<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Number of edges: </span><span style=color:#e6db74>{</span>df_edges<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Number of classes: </span><span style=color:#e6db74>{</span>df_classes[<span style=color:#e6db74>&#39;class&#39;</span>]<span style=color:#f92672>.</span>nunique()<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Timesteps range from </span><span style=color:#e6db74>{</span>df_features[<span style=color:#e6db74>&#39;time_step&#39;</span>]<span style=color:#f92672>.</span>min()<span style=color:#e6db74>}</span><span style=color:#e6db74> to </span><span style=color:#e6db74>{</span>df_features[<span style=color:#e6db74>&#39;time_step&#39;</span>]<span style=color:#f92672>.</span>max()<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><pre><code>Number of missing data points: 0
Number of nodes (transactions): 203769
Number of edges: 234355
Number of classes: 3
Timesteps range from 1 to 49
</code></pre><h3 id=correlation-analysis>Correlation analysis<a hidden class=anchor aria-hidden=true href=#correlation-analysis>#</a></h3><p>Additionally, the dataset was analyzed using the handy pandas-profiling package. The complete script for the analysis is in <a href>eda.py</a>, which generates <a href>a detailed report</a> including multiple correlations. The main findings from the report can be summarized as:</p><ul><li>29 features are highly skewed</li><li>76 features are highly correlated to other features in the dataset (Spearman correlation coefficient $\rho > 0.90$)<ul><li>21 aggregated features are highly correlated to other aggregated features</li><li>54 local features are highly correlated to other local features</li><li>time_step is highly correlated with aggr_43 ($\rho = 0.91$)</li></ul></li></ul><h2 id=constructing-the-transaction-graph>Constructing the transaction graph<a hidden class=anchor aria-hidden=true href=#constructing-the-transaction-graph>#</a></h2><p>We now have our data prepared in table format, but we want to be able to work on the graph constructed from the data. In order to create our transaction graph, we use the networkx package. We create a directed multigraph (a directed graph that allows for multiple edges between two nodes) and add the label attribute to each transaction.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># create networkx graph from the pandas dataframes</span>
</span></span><span style=display:flex><span>g_nx <span style=color:#f92672>=</span> nx<span style=color:#f92672>.</span>MultiDiGraph()
</span></span><span style=display:flex><span>g_nx<span style=color:#f92672>.</span>add_nodes_from(
</span></span><span style=display:flex><span>    zip(df_classes[<span style=color:#e6db74>&#34;txId&#34;</span>], [{<span style=color:#e6db74>&#34;label&#34;</span>: v} <span style=color:#66d9ef>for</span> v <span style=color:#f92672>in</span> df_classes[<span style=color:#e6db74>&#34;label&#34;</span>]])
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>g_nx<span style=color:#f92672>.</span>add_edges_from(zip(df_edges[<span style=color:#e6db74>&#34;txId1&#34;</span>], df_edges[<span style=color:#e6db74>&#34;txId2&#34;</span>]));
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Graph with </span><span style=color:#e6db74>{</span>g_nx<span style=color:#f92672>.</span>number_of_nodes()<span style=color:#e6db74>}</span><span style=color:#e6db74> nodes and </span><span style=color:#e6db74>{</span>g_nx<span style=color:#f92672>.</span>number_of_edges()<span style=color:#e6db74>}</span><span style=color:#e6db74> edges.&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Number of connected components: </span><span style=color:#e6db74>{</span>len(list(nx<span style=color:#f92672>.</span>weakly_connected_components(g_nx)))<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><pre><code>Graph with 203769 nodes and 234355 edges.
Number of connected components: 49
</code></pre><p>We can confirm that there are 49 connected components (weakly connected compoments in the case of directed graphs) was constructed for each timestep. This means that the dataset consists of 49 different subgraphs, each corresponding to one timestep.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># create list of graphs, one for each timestep</span>
</span></span><span style=display:flex><span>components <span style=color:#f92672>=</span> list(nx<span style=color:#f92672>.</span>weakly_connected_components(g_nx))
</span></span><span style=display:flex><span>g_nx_t_list <span style=color:#f92672>=</span> [g_nx<span style=color:#f92672>.</span>subgraph(components[i]) <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>0</span>,len(components))]
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>with</span> sns<span style=color:#f92672>.</span>axes_style(<span style=color:#e6db74>&#39;white&#39;</span>):
</span></span><span style=display:flex><span>    fig, ax <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>subplots(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>, figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>12</span>,<span style=color:#ae81ff>6</span>))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i,t <span style=color:#f92672>in</span> enumerate([<span style=color:#ae81ff>26</span>,<span style=color:#ae81ff>48</span>]):
</span></span><span style=display:flex><span>        node_label <span style=color:#f92672>=</span> list(nx<span style=color:#f92672>.</span>get_node_attributes(g_nx_t_list[t], <span style=color:#e6db74>&#39;label&#39;</span>)<span style=color:#f92672>.</span>values())
</span></span><span style=display:flex><span>        mapping <span style=color:#f92672>=</span> {<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>:<span style=color:#e6db74>&#39;grey&#39;</span>, <span style=color:#ae81ff>0</span>:<span style=color:#e6db74>&#39;C0&#39;</span>, <span style=color:#ae81ff>1</span>:<span style=color:#e6db74>&#39;C3&#39;</span>}
</span></span><span style=display:flex><span>        node_color <span style=color:#f92672>=</span> [mapping[l] <span style=color:#66d9ef>for</span> l <span style=color:#f92672>in</span> node_label]
</span></span><span style=display:flex><span>        nx<span style=color:#f92672>.</span>draw_networkx(g_nx_t_list[t], node_size<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>, node_color<span style=color:#f92672>=</span>node_color, with_labels<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, width<span style=color:#f92672>=</span><span style=color:#ae81ff>0.2</span>, alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.8</span>, arrowsize<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span>, ax<span style=color:#f92672>=</span>ax[i])
</span></span><span style=display:flex><span>    leg <span style=color:#f92672>=</span> ax[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>legend([<span style=color:#e6db74>&#39;unlabeled&#39;</span>, <span style=color:#e6db74>&#39;licit&#39;</span>, <span style=color:#e6db74>&#39;illicit&#39;</span>])
</span></span><span style=display:flex><span>    leg<span style=color:#f92672>.</span>legendHandles[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>set_color(<span style=color:#e6db74>&#39;grey&#39;</span>)
</span></span><span style=display:flex><span>    leg<span style=color:#f92672>.</span>legendHandles[<span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>set_color(<span style=color:#e6db74>&#39;C0&#39;</span>)
</span></span><span style=display:flex><span>    leg<span style=color:#f92672>.</span>legendHandles[<span style=color:#ae81ff>2</span>]<span style=color:#f92672>.</span>set_color(<span style=color:#e6db74>&#39;C3&#39;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>tight_layout()
</span></span></code></pre></div><pre><code>/anaconda3/envs/dgl/lib/python3.7/site-packages/networkx/drawing/nx_pylab.py:579: MatplotlibDeprecationWarning: 
The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead.
  if not cb.iterable(width):
/anaconda3/envs/dgl/lib/python3.7/site-packages/networkx/drawing/nx_pylab.py:676: MatplotlibDeprecationWarning: 
The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead.
  if cb.iterable(node_size):  # many node sizes
</code></pre><p><img alt=png loading=lazy src=/images/2019-12-15-btc-fraud-detection_files/2019-12-15-btc-fraud-detection_15_1.png></p><p>We can see that most of the transactions are not labeled and that only a minority of the labeled nodes correspond to illicit transactions. Moreover, there graph does not seem to be particularly dense. There seem to be chains of transactions one after the other. Also, many of these chains seem to concentrate one type of labeled transaction: either licit or illicit. Finally, a minority of nodes seems to have a larger number of edges, connecting with transactions in multiple chains or further away from their inmediate neighborhood.</p><h3 id=graph-metrics>Graph metrics<a hidden class=anchor aria-hidden=true href=#graph-metrics>#</a></h3><p>We can calculate selected graph metrics to better quantify some important structural properties of the transaction graph.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span>g_metrics <span style=color:#f92672>=</span> {}
</span></span><span style=display:flex><span>g_metrics[<span style=color:#e6db74>&#39;timestep&#39;</span>] <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>50</span>)
</span></span><span style=display:flex><span>g_metrics[<span style=color:#e6db74>&#39;number_of_nodes&#39;</span>] <span style=color:#f92672>=</span> [graph<span style=color:#f92672>.</span>number_of_nodes() <span style=color:#66d9ef>for</span> graph <span style=color:#f92672>in</span> g_nx_t_list]
</span></span><span style=display:flex><span>g_metrics[<span style=color:#e6db74>&#39;avg_degree&#39;</span>] <span style=color:#f92672>=</span> [np<span style=color:#f92672>.</span>mean(list(dict(nx<span style=color:#f92672>.</span>degree(graph))<span style=color:#f92672>.</span>values())) <span style=color:#66d9ef>for</span> graph <span style=color:#f92672>in</span> g_nx_t_list]
</span></span><span style=display:flex><span>g_metrics[<span style=color:#e6db74>&#39;density&#39;</span>] <span style=color:#f92672>=</span> [nx<span style=color:#f92672>.</span>density(graph) <span style=color:#66d9ef>for</span> graph <span style=color:#f92672>in</span> g_nx_t_list]
</span></span><span style=display:flex><span>g_metrics[<span style=color:#e6db74>&#39;avg_clustering&#39;</span>] <span style=color:#f92672>=</span> [nx<span style=color:#f92672>.</span>average_clustering(nx<span style=color:#f92672>.</span>DiGraph(graph)) <span style=color:#66d9ef>for</span> graph <span style=color:#f92672>in</span> g_nx_t_list]
</span></span><span style=display:flex><span>g_metrics[<span style=color:#e6db74>&#39;avg_shortest_path&#39;</span>] <span style=color:#f92672>=</span> [nx<span style=color:#f92672>.</span>average_shortest_path_length(nx<span style=color:#f92672>.</span>DiGraph(graph)) <span style=color:#66d9ef>for</span> graph <span style=color:#f92672>in</span> g_nx_t_list]
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span>fig, ax <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>subplots(len(g_metrics)<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>1</span>, figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>10</span>,<span style=color:#ae81ff>6</span>), sharex<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i,label <span style=color:#f92672>in</span> enumerate(list(g_metrics<span style=color:#f92672>.</span>keys())[<span style=color:#ae81ff>1</span>:]):
</span></span><span style=display:flex><span>    ax[i]<span style=color:#f92672>.</span>bar(g_metrics[<span style=color:#e6db74>&#39;timestep&#39;</span>], g_metrics[label], label<span style=color:#f92672>=</span>label)
</span></span><span style=display:flex><span>    ax[i]<span style=color:#f92672>.</span>legend()
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;timestep&#39;</span>);
</span></span></code></pre></div><p><img alt=png loading=lazy src=/images/2019-12-15-btc-fraud-detection_files/2019-12-15-btc-fraud-detection_19_0.png></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Average density of the graphs across all timesteps: </span><span style=color:#e6db74>{</span>np<span style=color:#f92672>.</span>mean(g_metrics[<span style=color:#e6db74>&#39;density&#39;</span>])<span style=color:#e6db74>:</span><span style=color:#e6db74>.6f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Average degree of all nodes across all timesteps: </span><span style=color:#e6db74>{</span>np<span style=color:#f92672>.</span>mean(list(dict(nx<span style=color:#f92672>.</span>degree(g_nx))<span style=color:#f92672>.</span>values()))<span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><pre><code>Average density of the graphs across all timesteps: 0.000318
Average degree of all nodes across all timesteps: 2.30
</code></pre><p>Given the density of the transaction graph, it seems to be rather sparse. The average density accross all timesteps lies around 0.0003177 and each node has an average of 2.30 edges. In comparison, the Cora dataset (popularly used as a benchmark of node classification algorithms) has an average degree of 3.90 and a density of 0.0014812. With only 2708 nodes, Cora is a much smaller and denser graph.</p><h2 id=training-a-graph-convolutional-network>Training a Graph Convolutional Network<a hidden class=anchor aria-hidden=true href=#training-a-graph-convolutional-network>#</a></h2><p>To build and train the GCN, I used <a href=https://dgl.ai>DGL</a> as a framework for deep learning on graphs DGL is based on pytorch and uses DGLGraph objects that can be easily created from networkx graphs. Moreover, several implementations of graph-based neural layers are available in DGL.</p><h3 id=graph-creation>Graph creation<a hidden class=anchor aria-hidden=true href=#graph-creation>#</a></h3><p>First we create the DGLGraph from a networkx Graph. We also add the label information as a tensor to the node data in the DGLGraph (from now on simply &ldquo;graph&rdquo;). Similarly, we add the node feature matrix to the graph as a tensor of shape (number of nodes, number of features) = (203769, 166).</p><p>Importantly, I tested the performance of the GCN using two options for constructing the graph:</p><ul><li>Unidirectional edges: edges going from one transaction to the next. In a GCN, this would mean that information used by the model to classify nodes would flow in one direction (downstream) only. In this case, we use <code>g_nx</code> directly.</li><li>Bidirectional edges: edges are made bidirectional, which would allow information flow in a GCN to travel in both directions (downstream and upstream). In this case, we use <code>g_nx.to_undirected().to_directed()</code>, i.e. we first make the edges undirected, and then back again directed. By doing so, networkx makes the edges in the resulting graph bidirectional.</li></ul><p>It is not explicitely stated in the paper, which of these two options was used by Weber et al. However, from the performance metrics, it is likely that they used the bidirectional edges version.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># create unidirectional graph</span>
</span></span><span style=display:flex><span>g <span style=color:#f92672>=</span> dgl<span style=color:#f92672>.</span>DGLGraph()
</span></span><span style=display:flex><span>g<span style=color:#f92672>.</span>from_networkx(g_nx)
</span></span><span style=display:flex><span>g<span style=color:#f92672>.</span>ndata[<span style=color:#e6db74>&#34;label&#34;</span>] <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(
</span></span><span style=display:flex><span>    df_classes<span style=color:#f92672>.</span>set_index(<span style=color:#e6db74>&#34;txId&#34;</span>)<span style=color:#f92672>.</span>loc[sorted(g_nx<span style=color:#f92672>.</span>nodes()), <span style=color:#e6db74>&#34;label&#34;</span>]<span style=color:#f92672>.</span>values
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>g<span style=color:#f92672>.</span>ndata[<span style=color:#e6db74>&#34;features_matrix&#34;</span>] <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(
</span></span><span style=display:flex><span>    df_features<span style=color:#f92672>.</span>set_index(<span style=color:#e6db74>&#34;txId&#34;</span>)<span style=color:#f92672>.</span>loc[sorted(g_nx<span style=color:#f92672>.</span>nodes()), :]<span style=color:#f92672>.</span>values
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>print(g)
</span></span></code></pre></div><pre><code>DGLGraph(num_nodes=203769, num_edges=234355,
         ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'features_matrix': Scheme(shape=(166,), dtype=torch.float64)}
         edata_schemes={})
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># make unidirectional edges bidirectional in networkx</span>
</span></span><span style=display:flex><span>g_nx_bidirectional <span style=color:#f92672>=</span> g_nx<span style=color:#f92672>.</span>to_undirected()<span style=color:#f92672>.</span>to_directed()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># create bidirectional graph</span>
</span></span><span style=display:flex><span>g_bi <span style=color:#f92672>=</span> dgl<span style=color:#f92672>.</span>DGLGraph()
</span></span><span style=display:flex><span>g_bi<span style=color:#f92672>.</span>from_networkx(g_nx_bidirectional)
</span></span><span style=display:flex><span>g_bi<span style=color:#f92672>.</span>ndata[<span style=color:#e6db74>&#34;label&#34;</span>] <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(
</span></span><span style=display:flex><span>    df_classes<span style=color:#f92672>.</span>set_index(<span style=color:#e6db74>&#34;txId&#34;</span>)<span style=color:#f92672>.</span>loc[sorted(g_nx<span style=color:#f92672>.</span>nodes()), <span style=color:#e6db74>&#34;label&#34;</span>]<span style=color:#f92672>.</span>values
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>g_bi<span style=color:#f92672>.</span>ndata[<span style=color:#e6db74>&#34;features_matrix&#34;</span>] <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(
</span></span><span style=display:flex><span>    df_features<span style=color:#f92672>.</span>set_index(<span style=color:#e6db74>&#34;txId&#34;</span>)<span style=color:#f92672>.</span>loc[sorted(g_nx<span style=color:#f92672>.</span>nodes()), :]<span style=color:#f92672>.</span>values
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>print(g_bi)
</span></span></code></pre></div><pre><code>DGLGraph(num_nodes=203769, num_edges=468710,
         ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'features_matrix': Scheme(shape=(166,), dtype=torch.float64)}
         edata_schemes={})
</code></pre><h3 id=graph-normalizaiton>Graph normalizaiton<a hidden class=anchor aria-hidden=true href=#graph-normalizaiton>#</a></h3><p>The performance of the GCN benefits from using a normalized version of the adjacency matrix. I applied a common normalization approach used in the literature. It starts by adding a self-loop to each node, which is the equivalent of adding the identity matrix to the adjacency matrix A</p>$$\tilde{A} = A + I$$<p>The rest of the normalization consists of obtaining the normalized graph Laplacian (<a href=https://samidavies.wordpress.com/2016/09/20/whats-up-with-the-graph-laplacian/>here is a great overview</a> explaining what the Laplacian means), which can be calculated as</p>$$\hat{A} = D^{-1/2}\tilde{A}D^{-1/2}$$<p>where $D$ is a matrix whose diagonal contains the degree of each node of $\tilde{A}$.</p><p>This matrix multiplication notation effectively means that the normalized Laplacian can have as a value in each $(i,j)$ position:</p>$$\hat{A}(i,j) = \begin{cases}
1 & \text{if $i=j$} \\[2ex]
{-1 \over {\sqrt{\deg(i)\deg(j)}}} & \text{if $i \neq j$ and $(i,j) \in E$},\\[2ex]
0 & \text{otherwise}
\end{cases}$$<p>where $E$ is the set of edges of the graph. As we can see, nodes with an (in)degree of zero could be troublesome, which is why we add the self-loops.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># add self loop</span>
</span></span><span style=display:flex><span>g<span style=color:#f92672>.</span>add_edges(g<span style=color:#f92672>.</span>nodes(), g<span style=color:#f92672>.</span>nodes())
</span></span><span style=display:flex><span>print(g)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># add self loop to the bidirectional edges graph</span>
</span></span><span style=display:flex><span>g_bi<span style=color:#f92672>.</span>add_edges(g_bi<span style=color:#f92672>.</span>nodes(), g_bi<span style=color:#f92672>.</span>nodes())
</span></span><span style=display:flex><span>print(g_bi)
</span></span></code></pre></div><pre><code>DGLGraph(num_nodes=203769, num_edges=438124,
         ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'features_matrix': Scheme(shape=(166,), dtype=torch.float64)}
         edata_schemes={})
DGLGraph(num_nodes=203769, num_edges=672479,
         ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'features_matrix': Scheme(shape=(166,), dtype=torch.float64)}
         edata_schemes={})
</code></pre><h3 id=trainvalidation-data-splitting>Train/validation data splitting<a hidden class=anchor aria-hidden=true href=#trainvalidation-data-splitting>#</a></h3><p>Now that our graph is ready, we need to split our data into training and validation sets. We will follow the same approach taken by Weber et al., which consists in a time-based split using the initial 70% of the timesteps to train the model and the remaining 30% for validation. This temporal split makes sense in the context of possible applications of such a model. A company could continuosly use their older historical data and labels for training and then predict the node classes for more recent transactions.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span>features <span style=color:#f92672>=</span> g<span style=color:#f92672>.</span>ndata[<span style=color:#e6db74>&#34;features_matrix&#34;</span>]<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>labels <span style=color:#f92672>=</span> g<span style=color:#f92672>.</span>ndata[<span style=color:#e6db74>&#34;label&#34;</span>]<span style=color:#f92672>.</span>long()  <span style=color:#75715e># format required for cross entropy loss</span>
</span></span><span style=display:flex><span>in_feats <span style=color:#f92672>=</span> features<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>n_classes <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>  <span style=color:#75715e># licit or illicit (unknown label is ignored)</span>
</span></span><span style=display:flex><span>n_edges <span style=color:#f92672>=</span> g<span style=color:#f92672>.</span>number_of_edges()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>dataset_size <span style=color:#f92672>=</span> df_classes[<span style=color:#e6db74>&#34;label&#34;</span>]<span style=color:#f92672>.</span>notna()<span style=color:#f92672>.</span>sum()
</span></span><span style=display:flex><span>train_ratio <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.7</span>
</span></span><span style=display:flex><span>train_time_steps <span style=color:#f92672>=</span> round(len(np<span style=color:#f92672>.</span>unique(features[:, <span style=color:#ae81ff>0</span>])) <span style=color:#f92672>*</span> train_ratio)
</span></span><span style=display:flex><span>shutdown_timestep <span style=color:#f92672>=</span> <span style=color:#ae81ff>43</span>
</span></span><span style=display:flex><span>train_indices <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>    ((features[:, <span style=color:#ae81ff>0</span>] <span style=color:#f92672>&lt;=</span> train_time_steps) <span style=color:#f92672>&amp;</span> (labels <span style=color:#f92672>!=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>))<span style=color:#f92672>.</span>nonzero()<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>val_indices <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>    ((features[:, <span style=color:#ae81ff>0</span>] <span style=color:#f92672>&gt;</span> train_time_steps) <span style=color:#f92672>&amp;</span> (labels <span style=color:#f92672>!=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>))<span style=color:#f92672>.</span>nonzero()<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span><span style=color:#75715e># timestep_indices = {</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#     t:(</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#     ((features[:, 0] == t) &amp; (labels != -1)).nonzero().view(-1)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># ) for t in range(train_time_steps+1,50)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># }</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;&#34;&#34;Number of timesteps used for training: </span><span style=color:#e6db74>{</span>train_time_steps<span style=color:#e6db74>}</span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>Number of timesteps used for validation: </span><span style=color:#e6db74>{</span>dataset_size<span style=color:#f92672>-</span>train_time_steps<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;&#34;&#34;</span>)
</span></span></code></pre></div><pre><code>Number of timesteps used for training: 34
Number of timesteps used for validation: 203735
</code></pre><h3 id=gcn-model-architecture>GCN model architecture<a hidden class=anchor aria-hidden=true href=#gcn-model-architecture>#</a></h3><p>For the GCN model, I used the implementation from DGL, which is based on <a href=https://arxiv.org/abs/1609.02907>the original implementation used by Kipf et al. (2016)</a>. In short, the algorithm is given by the formula</p>$$H^{(l+1)} = \sigma (D^{-1/2}\tilde{A}D^{-1/2}H^{(l)}W^{(l)})$$<p>where $\sigma$ is the activation function (ReLu), $D^{-1/2}\tilde{A}D^{-1/2}$ is the normalized graph Laplacian, $H^{(l)}$ are the logits and $W^{(l)}$ are the learnable weights of the $l$th layer of the neural network. It is also possible to add a learnable bias to each layer. There are <a href=https://tkipf.github.io/graph-convolutional-networks/>some very good</a> <a href=https://arxiv.org/abs/1901.00596>explanations</a> of how this algorithm works available for reference.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>GCN</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(
</span></span><span style=display:flex><span>        self, g, in_feats, n_hidden, n_classes, n_layers, activation, dropout, bias
</span></span><span style=display:flex><span>    ):
</span></span><span style=display:flex><span>        super(GCN, self)<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>g <span style=color:#f92672>=</span> g
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>layers <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ModuleList()
</span></span><span style=display:flex><span>        <span style=color:#75715e># input layer</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>append(
</span></span><span style=display:flex><span>            GraphConv(in_feats, n_hidden, activation<span style=color:#f92672>=</span>activation, bias<span style=color:#f92672>=</span>bias)
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        <span style=color:#75715e># hidden layers</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(n_layers <span style=color:#f92672>-</span> <span style=color:#ae81ff>2</span>):
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>append(
</span></span><span style=display:flex><span>                GraphConv(n_hidden, n_hidden, activation<span style=color:#f92672>=</span>activation, bias<span style=color:#f92672>=</span>bias)
</span></span><span style=display:flex><span>            )
</span></span><span style=display:flex><span>        <span style=color:#75715e># output layer</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>append(GraphConv(n_hidden, n_classes, bias<span style=color:#f92672>=</span>bias))
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>dropout <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dropout(p<span style=color:#f92672>=</span>dropout)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, features):
</span></span><span style=display:flex><span>        h <span style=color:#f92672>=</span> features
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> i, layer <span style=color:#f92672>in</span> enumerate(self<span style=color:#f92672>.</span>layers):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> i <span style=color:#f92672>!=</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                h <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>dropout(h)
</span></span><span style=display:flex><span>            h <span style=color:#f92672>=</span> layer(self<span style=color:#f92672>.</span>g, h)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> h
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># utility function to evaluate the model</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>evaluate</span>(model, loss_fcn, features, labels, mask):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Calculate the loss, accuracy, precision, recall and f1_score for the masked data&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    model<span style=color:#f92672>.</span>eval()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>        logits <span style=color:#f92672>=</span> model(features)
</span></span><span style=display:flex><span>        logits <span style=color:#f92672>=</span> logits[mask]
</span></span><span style=display:flex><span>        labels <span style=color:#f92672>=</span> labels[mask]
</span></span><span style=display:flex><span>        loss <span style=color:#f92672>=</span> loss_fcn(logits, labels)
</span></span><span style=display:flex><span>        _, indices <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>max(logits, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        correct <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>sum(indices <span style=color:#f92672>==</span> labels)
</span></span><span style=display:flex><span>        p, r, f, _ <span style=color:#f92672>=</span> precision_recall_fscore_support(labels, indices)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> loss, correct<span style=color:#f92672>.</span>item() <span style=color:#f92672>*</span> <span style=color:#ae81ff>1.0</span> <span style=color:#f92672>/</span> len(labels), p[<span style=color:#ae81ff>1</span>], r[<span style=color:#ae81ff>1</span>], f[<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># utility function to obtain a confusion matrix</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>eval_confusion_matrix</span>(model, features, labels, mask):
</span></span><span style=display:flex><span>    model<span style=color:#f92672>.</span>eval()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>        logits <span style=color:#f92672>=</span> model(features)
</span></span><span style=display:flex><span>        logits <span style=color:#f92672>=</span> logits[mask]
</span></span><span style=display:flex><span>        labels <span style=color:#f92672>=</span> labels[mask]
</span></span><span style=display:flex><span>        _, indices <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>max(logits, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    print(confusion_matrix(labels, indices))
</span></span></code></pre></div><h3 id=model-training>Model training<a hidden class=anchor aria-hidden=true href=#model-training>#</a></h3><p>Now we can train the model using the specifications from the paper by Weber et al.:</p><ul><li>cross entropy loss function putting higher weight for the positive (illicit) samples: 0.7 positive vs 0.3 negative</li><li>adam optimizer with learning rate <code>1e-3</code></li><li>no weight decay is mentioned</li><li>no bias is mentioned</li><li>no dropout</li><li>train for 1000 epochs</li><li>two Graph Convolutional layers with 100 and 2 neurons respectively</li></ul><p>We can now define the Graph Convolutional Network architecture using DGL. In this case, it consists of <code>n_layers</code> GCN layers with <code>n_hidden</code> hidden units per layer:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># train and evaluate the model</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_eval_model</span>(model_class, g, features, <span style=color:#f92672>**</span>params):
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e>#bidirectional = params[&#34;bidirectional&#34;] if &#34;bidirectional&#34; in params else None</span>
</span></span><span style=display:flex><span>    in_feats <span style=color:#f92672>=</span> features<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>    n_classes <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>    n_hidden <span style=color:#f92672>=</span> params[<span style=color:#e6db74>&#34;n_hidden&#34;</span>]
</span></span><span style=display:flex><span>    n_layers <span style=color:#f92672>=</span> params[<span style=color:#e6db74>&#34;n_layers&#34;</span>]
</span></span><span style=display:flex><span>    weight_decay <span style=color:#f92672>=</span> params[<span style=color:#e6db74>&#34;weight_decay&#34;</span>]
</span></span><span style=display:flex><span>    bias <span style=color:#f92672>=</span> params[<span style=color:#e6db74>&#34;bias&#34;</span>]
</span></span><span style=display:flex><span>    dropout <span style=color:#f92672>=</span> params[<span style=color:#e6db74>&#34;dropout&#34;</span>]
</span></span><span style=display:flex><span>    epochs <span style=color:#f92672>=</span> params[<span style=color:#e6db74>&#34;epochs&#34;</span>]
</span></span><span style=display:flex><span>    lr <span style=color:#f92672>=</span> params[<span style=color:#e6db74>&#34;lr&#34;</span>]
</span></span><span style=display:flex><span>    posweight <span style=color:#f92672>=</span> params[<span style=color:#e6db74>&#34;posweight&#34;</span>]
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> model_class(g, in_feats, n_hidden, n_classes, n_layers, F<span style=color:#f92672>.</span>relu, dropout, bias)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># weighted cross entropy loss function</span>
</span></span><span style=display:flex><span>    loss_fcn <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>CrossEntropyLoss(
</span></span><span style=display:flex><span>        weight<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> posweight, posweight])
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># use optimizer</span>
</span></span><span style=display:flex><span>    optimizer <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>optim<span style=color:#f92672>.</span>Adam(
</span></span><span style=display:flex><span>        model<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span>lr, weight_decay<span style=color:#f92672>=</span>weight_decay
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    dur <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    metrics <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#34;loss&#34;</span>:{<span style=color:#e6db74>&#34;train&#34;</span>: [], <span style=color:#e6db74>&#34;val&#34;</span>: []},
</span></span><span style=display:flex><span>               <span style=color:#e6db74>&#34;accuracy&#34;</span>:{<span style=color:#e6db74>&#34;train&#34;</span>: [], <span style=color:#e6db74>&#34;val&#34;</span>: []},
</span></span><span style=display:flex><span>               <span style=color:#e6db74>&#34;precision&#34;</span>:{<span style=color:#e6db74>&#34;train&#34;</span>: [], <span style=color:#e6db74>&#34;val&#34;</span>: []},
</span></span><span style=display:flex><span>               <span style=color:#e6db74>&#34;recall&#34;</span>:{<span style=color:#e6db74>&#34;train&#34;</span>: [], <span style=color:#e6db74>&#34;val&#34;</span>: []},
</span></span><span style=display:flex><span>               <span style=color:#e6db74>&#34;f1_score&#34;</span>:{<span style=color:#e6db74>&#34;train&#34;</span>: [], <span style=color:#e6db74>&#34;val&#34;</span>: []},
</span></span><span style=display:flex><span>              }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(epochs):
</span></span><span style=display:flex><span>        model<span style=color:#f92672>.</span>train()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> epoch <span style=color:#f92672>&gt;=</span> <span style=color:#ae81ff>3</span>:
</span></span><span style=display:flex><span>            t0 <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time()
</span></span><span style=display:flex><span>        <span style=color:#75715e># forward pass</span>
</span></span><span style=display:flex><span>        logits <span style=color:#f92672>=</span> model(features)
</span></span><span style=display:flex><span>        loss <span style=color:#f92672>=</span> loss_fcn(logits[train_indices], labels[train_indices])
</span></span><span style=display:flex><span>        metrics[<span style=color:#e6db74>&#34;loss&#34;</span>][<span style=color:#e6db74>&#34;train&#34;</span>]<span style=color:#f92672>.</span>append(loss)
</span></span><span style=display:flex><span>        <span style=color:#75715e># backward pass</span>
</span></span><span style=display:flex><span>        optimizer<span style=color:#f92672>.</span>zero_grad()
</span></span><span style=display:flex><span>        loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>        optimizer<span style=color:#f92672>.</span>step()
</span></span><span style=display:flex><span>        <span style=color:#75715e># duration</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> epoch <span style=color:#f92672>&gt;=</span> <span style=color:#ae81ff>3</span>:
</span></span><span style=display:flex><span>            dur<span style=color:#f92672>.</span>append(time<span style=color:#f92672>.</span>time() <span style=color:#f92672>-</span> t0)
</span></span><span style=display:flex><span>        <span style=color:#75715e># evaluate on training set</span>
</span></span><span style=display:flex><span>        _, train_acc, train_precision, train_recall, train_f1_score <span style=color:#f92672>=</span> evaluate(
</span></span><span style=display:flex><span>            model, loss_fcn, features, labels, train_indices
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        metrics[<span style=color:#e6db74>&#34;accuracy&#34;</span>][<span style=color:#e6db74>&#34;train&#34;</span>]<span style=color:#f92672>.</span>append(train_acc)
</span></span><span style=display:flex><span>        metrics[<span style=color:#e6db74>&#34;precision&#34;</span>][<span style=color:#e6db74>&#34;train&#34;</span>]<span style=color:#f92672>.</span>append(train_precision)
</span></span><span style=display:flex><span>        metrics[<span style=color:#e6db74>&#34;recall&#34;</span>][<span style=color:#e6db74>&#34;train&#34;</span>]<span style=color:#f92672>.</span>append(train_recall)
</span></span><span style=display:flex><span>        metrics[<span style=color:#e6db74>&#34;f1_score&#34;</span>][<span style=color:#e6db74>&#34;train&#34;</span>]<span style=color:#f92672>.</span>append(train_f1_score)
</span></span><span style=display:flex><span>        <span style=color:#75715e># evaluate on validation set</span>
</span></span><span style=display:flex><span>        val_loss, val_acc, val_precision, val_recall, val_f1_score <span style=color:#f92672>=</span> evaluate(
</span></span><span style=display:flex><span>            model, loss_fcn, features, labels, val_indices
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        metrics[<span style=color:#e6db74>&#34;loss&#34;</span>][<span style=color:#e6db74>&#34;val&#34;</span>]<span style=color:#f92672>.</span>append(val_loss)
</span></span><span style=display:flex><span>        metrics[<span style=color:#e6db74>&#34;accuracy&#34;</span>][<span style=color:#e6db74>&#34;val&#34;</span>]<span style=color:#f92672>.</span>append(val_acc)
</span></span><span style=display:flex><span>        metrics[<span style=color:#e6db74>&#34;precision&#34;</span>][<span style=color:#e6db74>&#34;val&#34;</span>]<span style=color:#f92672>.</span>append(val_precision)
</span></span><span style=display:flex><span>        metrics[<span style=color:#e6db74>&#34;recall&#34;</span>][<span style=color:#e6db74>&#34;val&#34;</span>]<span style=color:#f92672>.</span>append(val_recall)
</span></span><span style=display:flex><span>        metrics[<span style=color:#e6db74>&#34;f1_score&#34;</span>][<span style=color:#e6db74>&#34;val&#34;</span>]<span style=color:#f92672>.</span>append(val_f1_score)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> (epoch <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>%</span> <span style=color:#ae81ff>100</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>            print(
</span></span><span style=display:flex><span>                <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Epoch </span><span style=color:#e6db74>{</span>epoch<span style=color:#e6db74>:</span><span style=color:#e6db74>05d</span><span style=color:#e6db74>}</span><span style=color:#e6db74> | Time(s) </span><span style=color:#e6db74>{</span>np<span style=color:#f92672>.</span>mean(dur)<span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74> | val_loss </span><span style=color:#e6db74>{</span>val_loss<span style=color:#f92672>.</span>item()<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74> &#34;</span>
</span></span><span style=display:flex><span>                <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;| Precision </span><span style=color:#e6db74>{</span>val_precision<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74> | Recall </span><span style=color:#e6db74>{</span>val_recall<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74> | Acc </span><span style=color:#e6db74>{</span>val_acc<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74> &#34;</span>
</span></span><span style=display:flex><span>                <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;| F1_score </span><span style=color:#e6db74>{</span>val_f1_score<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>            )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Confusion matrix:&#34;</span>)
</span></span><span style=display:flex><span>    eval_confusion_matrix(model, features, labels, val_indices)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> model, metrics
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># GCN model parameters</span>
</span></span><span style=display:flex><span>params <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;n_hidden&#34;</span> : <span style=color:#ae81ff>100</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;n_layers&#34;</span> : <span style=color:#ae81ff>2</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;weight_decay&#34;</span> : <span style=color:#ae81ff>0.</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;bias&#34;</span> : <span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;dropout&#34;</span> : <span style=color:#ae81ff>0.</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;epochs&#34;</span> : <span style=color:#ae81ff>1000</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;lr&#34;</span> : <span style=color:#ae81ff>1e-3</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;posweight&#34;</span> : <span style=color:#ae81ff>0.7</span>,
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># train on graph with unidirectional edges</span>
</span></span><span style=display:flex><span>model, metrics <span style=color:#f92672>=</span> train_eval_model(GCN, g, features, <span style=color:#f92672>**</span>params)
</span></span></code></pre></div><pre><code>Epoch 00099 | Time(s) 0.60 | val_loss 0.3580 | Precision 0.2701 | Recall 0.5125 | Acc 0.8783 | F1_score 0.3537
Epoch 00199 | Time(s) 0.60 | val_loss 0.3381 | Precision 0.3649 | Recall 0.4589 | Acc 0.9130 | F1_score 0.4065
Epoch 00299 | Time(s) 0.59 | val_loss 0.3537 | Precision 0.4101 | Recall 0.4358 | Acc 0.9226 | F1_score 0.4226
Epoch 00399 | Time(s) 0.58 | val_loss 0.4015 | Precision 0.4039 | Recall 0.4192 | Acc 0.9221 | F1_score 0.4114
Epoch 00499 | Time(s) 0.58 | val_loss 0.4556 | Precision 0.3895 | Recall 0.3989 | Acc 0.9203 | F1_score 0.3942
Epoch 00599 | Time(s) 0.58 | val_loss 0.5079 | Precision 0.3622 | Recall 0.3860 | Acc 0.9160 | F1_score 0.3737
Epoch 00699 | Time(s) 0.59 | val_loss 0.5570 | Precision 0.3533 | Recall 0.3813 | Acc 0.9145 | F1_score 0.3668
Epoch 00799 | Time(s) 0.59 | val_loss 0.6020 | Precision 0.3702 | Recall 0.3860 | Acc 0.9175 | F1_score 0.3779
Epoch 00899 | Time(s) 0.60 | val_loss 0.6472 | Precision 0.3677 | Recall 0.3915 | Acc 0.9167 | F1_score 0.3792
Epoch 00999 | Time(s) 0.60 | val_loss 0.7006 | Precision 0.3618 | Recall 0.3869 | Acc 0.9158 | F1_score 0.3739
Confusion matrix:
[[14848   739]
 [  664   419]]
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># train on graph with bidirectional edges</span>
</span></span><span style=display:flex><span>model_bi, metrics_bi <span style=color:#f92672>=</span> train_eval_model(GCN, g_bi, features, <span style=color:#f92672>**</span>params)
</span></span></code></pre></div><pre><code>Epoch 00099 | Time(s) 0.62 | val_loss 0.3113 | Precision 0.3251 | Recall 0.4765 | Acc 0.9017 | F1_score 0.3865
Epoch 00199 | Time(s) 0.62 | val_loss 0.3229 | Precision 0.4209 | Recall 0.4543 | Acc 0.9239 | F1_score 0.4369
Epoch 00299 | Time(s) 0.62 | val_loss 0.3302 | Precision 0.4776 | Recall 0.4331 | Acc 0.9324 | F1_score 0.4542
Epoch 00399 | Time(s) 0.63 | val_loss 0.3456 | Precision 0.5940 | Recall 0.4054 | Acc 0.9434 | F1_score 0.4819
Epoch 00499 | Time(s) 0.63 | val_loss 0.3675 | Precision 0.7227 | Recall 0.3850 | Acc 0.9504 | F1_score 0.5024
Epoch 00599 | Time(s) 0.63 | val_loss 0.3933 | Precision 0.7757 | Recall 0.3832 | Acc 0.9527 | F1_score 0.5130
Epoch 00699 | Time(s) 0.63 | val_loss 0.4211 | Precision 0.7784 | Recall 0.3795 | Acc 0.9527 | F1_score 0.5102
Epoch 00799 | Time(s) 0.63 | val_loss 0.4465 | Precision 0.7778 | Recall 0.3813 | Acc 0.9527 | F1_score 0.5118
Epoch 00899 | Time(s) 0.63 | val_loss 0.4769 | Precision 0.7874 | Recall 0.3795 | Acc 0.9530 | F1_score 0.5121
Epoch 00999 | Time(s) 0.63 | val_loss 0.5053 | Precision 0.7684 | Recall 0.3767 | Acc 0.9521 | F1_score 0.5056
Confusion matrix:
[[15464   123]
 [  675   408]]
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># plot the metrics during training</span>
</span></span><span style=display:flex><span>fig, ax <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>subplots(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>3</span>, figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>18</span>,<span style=color:#ae81ff>5</span>), sharex<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>ax[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>plot(metrics[<span style=color:#e6db74>&#34;loss&#34;</span>][<span style=color:#e6db74>&#39;train&#39;</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;unidir. train_loss&#39;</span>, color<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;C0&#39;</span>)
</span></span><span style=display:flex><span>ax[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>plot(metrics[<span style=color:#e6db74>&#34;loss&#34;</span>][<span style=color:#e6db74>&#39;val&#39;</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;unidir. val_loss&#39;</span>, color<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;C0&#39;</span>, ls<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;:&#39;</span>)
</span></span><span style=display:flex><span>ax[<span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>plot(metrics[<span style=color:#e6db74>&#39;f1_score&#39;</span>][<span style=color:#e6db74>&#39;val&#39;</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;unidir. val_f1_score&#39;</span>, color<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;C0&#39;</span>)
</span></span><span style=display:flex><span>ax[<span style=color:#ae81ff>2</span>]<span style=color:#f92672>.</span>plot(metrics[<span style=color:#e6db74>&#39;precision&#39;</span>][<span style=color:#e6db74>&#39;val&#39;</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;unidir. val_precision&#39;</span>, color<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;C0&#39;</span>)
</span></span><span style=display:flex><span>ax[<span style=color:#ae81ff>2</span>]<span style=color:#f92672>.</span>plot(metrics[<span style=color:#e6db74>&#39;recall&#39;</span>][<span style=color:#e6db74>&#39;val&#39;</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;unidir. val_recall&#39;</span>, color<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;C0&#39;</span>, ls<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;:&#39;</span>)
</span></span><span style=display:flex><span>ax[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>plot(metrics_bi[<span style=color:#e6db74>&#34;loss&#34;</span>][<span style=color:#e6db74>&#39;train&#39;</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;bidir. train_loss&#39;</span>, color<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;C3&#39;</span>)
</span></span><span style=display:flex><span>ax[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>plot(metrics_bi[<span style=color:#e6db74>&#34;loss&#34;</span>][<span style=color:#e6db74>&#39;val&#39;</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;bidir. val_loss&#39;</span>, color<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;C3&#39;</span>, ls<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;:&#39;</span>)
</span></span><span style=display:flex><span>ax[<span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>plot(metrics_bi[<span style=color:#e6db74>&#39;f1_score&#39;</span>][<span style=color:#e6db74>&#39;val&#39;</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;bidir. val_f1_score&#39;</span>, color<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;C3&#39;</span>)
</span></span><span style=display:flex><span>ax[<span style=color:#ae81ff>2</span>]<span style=color:#f92672>.</span>plot(metrics_bi[<span style=color:#e6db74>&#39;precision&#39;</span>][<span style=color:#e6db74>&#39;val&#39;</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;bidir. val_precision&#39;</span>, color<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;C3&#39;</span>)
</span></span><span style=display:flex><span>ax[<span style=color:#ae81ff>2</span>]<span style=color:#f92672>.</span>plot(metrics_bi[<span style=color:#e6db74>&#39;recall&#39;</span>][<span style=color:#e6db74>&#39;val&#39;</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;bidir. val_recall&#39;</span>, color<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;C3&#39;</span>, ls<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;:&#39;</span>)
</span></span><span style=display:flex><span>ax[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>legend()
</span></span><span style=display:flex><span>ax[<span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>legend()
</span></span><span style=display:flex><span>ax[<span style=color:#ae81ff>2</span>]<span style=color:#f92672>.</span>legend();
</span></span></code></pre></div><p><img alt=png loading=lazy src=/images/2019-12-15-btc-fraud-detection_files/2019-12-15-btc-fraud-detection_36_0.png></p><p>Training the GCN model with these parameters led to a poorer performance than the one reported in the paper by Weber et al. The bidirectional graph variant produced the better results. Therefore it is also probably the setting used in the paper. Even though the Bitcoin flow from one transaction to another is intuitively one-directional, this would mean that, in a GCN, the information would only flow downstream. Having a bidirectional flow of information through the edges of the graph in the GCN makes information available to each node both upstream and downstream from it. This greatly improved the performance of the GCN.</p><table><thead><tr><th>Model</th><th>Edges</th><th style=text-align:right>Dropout</th><th style=text-align:right>Precision</th><th style=text-align:right>Recall</th><th style=text-align:right>F1 score</th></tr></thead><tbody><tr><td>GCN (Weber et al.)</td><td></td><td style=text-align:right>0.</td><td style=text-align:right>0.812</td><td style=text-align:right>0.512</td><td style=text-align:right>0.628</td></tr><tr><td>GCN</td><td>unidirectional</td><td style=text-align:right>0.</td><td style=text-align:right>0.3764</td><td style=text-align:right>0.3823</td><td style=text-align:right>0.3793</td></tr><tr><td>GCN</td><td>bidirectional</td><td style=text-align:right>0.</td><td style=text-align:right>0.7860</td><td style=text-align:right>0.3832</td><td style=text-align:right>0.5152</td></tr></tbody></table><p>In order to figure out, whether there are additional parameters that I did not yet consider in replicating the GCN approach, I performed a series of experiments changing further training parameters and comparing the results. This, in turn, was useful in understanding in what ways the model could be modified to increase its performance.</p><h2 id=additional-experiments-with-gcns>Additional experiments with GCNs<a hidden class=anchor aria-hidden=true href=#additional-experiments-with-gcns>#</a></h2><h3 id=address-overfitting-by-weight-decay-l2-regularization>Address overfitting by weight decay L2-regularization<a hidden class=anchor aria-hidden=true href=#address-overfitting-by-weight-decay-l2-regularization>#</a></h3><p>One observation from the previous learning curves is that the validation loss starts to increase again after ca. 400 epochs, a clear sign of overfitting. One way to address this is to add dropout to the model. In this case, I added weight-decay L2-regularization to improve training.</p><p>I further added a learnable bias to the GCN to see if this improved its performance (it did slightly). The other parameters were left intact.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span>query <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;model==&#34;gcn&#34; and onlylocal==False and dropout==&#34;0&#34;&#39;</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Plotting </span><span style=color:#e6db74>{</span>runs<span style=color:#f92672>.</span>query(query)[<span style=color:#e6db74>&#39;name&#39;</span>]<span style=color:#f92672>.</span>nunique()<span style=color:#e6db74>}</span><span style=color:#e6db74> runs: </span><span style=color:#e6db74>{</span>runs<span style=color:#f92672>.</span>query(query)[<span style=color:#e6db74>&#39;name&#39;</span>]<span style=color:#f92672>.</span>unique()<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>g <span style=color:#f92672>=</span> sns<span style=color:#f92672>.</span>relplot(<span style=color:#e6db74>&#39;epoch&#39;</span>, <span style=color:#e6db74>&#39;train_loss&#39;</span>, col<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;weight_decay&#39;</span>, hue<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;bidirectional&#39;</span>, style<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;nobias&#39;</span>, palette<span style=color:#f92672>=</span>sns<span style=color:#f92672>.</span>color_palette(<span style=color:#e6db74>&#34;Set1&#34;</span>, <span style=color:#ae81ff>2</span>), kind<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;line&#39;</span>, data<span style=color:#f92672>=</span>runs<span style=color:#f92672>.</span>query(query));
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>ylim(<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>1</span>);
</span></span><span style=display:flex><span>g <span style=color:#f92672>=</span> sns<span style=color:#f92672>.</span>relplot(<span style=color:#e6db74>&#39;epoch&#39;</span>, <span style=color:#e6db74>&#39;val_loss&#39;</span>, col<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;weight_decay&#39;</span>, hue<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;bidirectional&#39;</span>, style<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;nobias&#39;</span>, palette<span style=color:#f92672>=</span>sns<span style=color:#f92672>.</span>color_palette(<span style=color:#e6db74>&#34;Set1&#34;</span>, <span style=color:#ae81ff>2</span>), kind<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;line&#39;</span>, data<span style=color:#f92672>=</span>runs<span style=color:#f92672>.</span>query(query))
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>ylim(<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>1</span>);
</span></span></code></pre></div><pre><code>Plotting 8 runs: ['super-sweep-14' 'blooming-sweep-13' 'vague-sweep-10' 'sandy-sweep-9'
 'eternal-sweep-6' 'resilient-sweep-5' 'ethereal-sweep-2'
 'vibrant-sweep-1']
</code></pre><p><img alt=png loading=lazy src=/images/2019-12-15-btc-fraud-detection_files/2019-12-15-btc-fraud-detection_40_1.png></p><p><img alt=png loading=lazy src=/images/2019-12-15-btc-fraud-detection_files/2019-12-15-btc-fraud-detection_40_2.png></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span>query <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;model==&#34;gcn&#34; and onlylocal==False and dropout==0&#39;</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Plotting </span><span style=color:#e6db74>{</span>runs<span style=color:#f92672>.</span>query(query)[<span style=color:#e6db74>&#39;name&#39;</span>]<span style=color:#f92672>.</span>nunique()<span style=color:#e6db74>}</span><span style=color:#e6db74> runs: </span><span style=color:#e6db74>{</span>runs<span style=color:#f92672>.</span>query(query)[<span style=color:#e6db74>&#39;name&#39;</span>]<span style=color:#f92672>.</span>unique()<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>sns<span style=color:#f92672>.</span>relplot(<span style=color:#e6db74>&#39;epoch&#39;</span>, <span style=color:#e6db74>&#39;val_f1_score&#39;</span>, col<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;weight_decay&#39;</span>, hue<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;bidirectional&#39;</span>, style<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;nobias&#39;</span>, palette<span style=color:#f92672>=</span>sns<span style=color:#f92672>.</span>color_palette(<span style=color:#e6db74>&#34;Set1&#34;</span>, <span style=color:#ae81ff>2</span>), kind<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;line&#39;</span>, data<span style=color:#f92672>=</span>runs<span style=color:#f92672>.</span>query(query));
</span></span></code></pre></div><pre><code>Plotting 8 runs: ['super-sweep-14' 'blooming-sweep-13' 'vague-sweep-10' 'sandy-sweep-9'
 'eternal-sweep-6' 'resilient-sweep-5' 'ethereal-sweep-2'
 'vibrant-sweep-1']
</code></pre><p><img alt=png loading=lazy src=/images/2019-12-15-btc-fraud-detection_files/2019-12-15-btc-fraud-detection_41_1.png></p><h3 id=address-overfitting-by-adding-dropout>Address overfitting by adding dropout<a hidden class=anchor aria-hidden=true href=#address-overfitting-by-adding-dropout>#</a></h3><p>Another way to address this is to add dropout to the model. In this case, I added dropout before the second GCN layer (meaning that the inputs to the 2nd layer will be dropped out with a certain probability. Adding dropout considerably increased the precision, meaning that the model predicts far fewer false negatives. The recall, on the other hand, remains largely unchanged. The best model was obtained with a dropout $p = 0.5$</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span>query <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;model==&#34;gcn&#34; and bidirectional==True and onlylocal==False and nobias==&#34;False&#34; and weight_decay==&#34;0.0005&#34; and (dropout==0. or dropout==0.25 or dropout==&#34;0.5&#34;)&#39;</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Plotting </span><span style=color:#e6db74>{</span>runs<span style=color:#f92672>.</span>query(query)[<span style=color:#e6db74>&#39;name&#39;</span>]<span style=color:#f92672>.</span>nunique()<span style=color:#e6db74>}</span><span style=color:#e6db74> runs: </span><span style=color:#e6db74>{</span>runs<span style=color:#f92672>.</span>query(query)[<span style=color:#e6db74>&#39;name&#39;</span>]<span style=color:#f92672>.</span>unique()<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>fig, ax <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>subplots(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>, figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>12</span>,<span style=color:#ae81ff>5</span>))
</span></span><span style=display:flex><span>g1 <span style=color:#f92672>=</span> sns<span style=color:#f92672>.</span>relplot(<span style=color:#e6db74>&#39;epoch&#39;</span>, <span style=color:#e6db74>&#39;val_loss&#39;</span>, col<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;bidirectional&#39;</span>, hue<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;dropout&#39;</span>, kind<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;line&#39;</span>, palette<span style=color:#f92672>=</span>sns<span style=color:#f92672>.</span>color_palette(<span style=color:#e6db74>&#34;Set1&#34;</span>, <span style=color:#ae81ff>3</span>), data<span style=color:#f92672>=</span>runs<span style=color:#f92672>.</span>query(query), ax<span style=color:#f92672>=</span>ax[<span style=color:#ae81ff>0</span>])
</span></span><span style=display:flex><span>g2 <span style=color:#f92672>=</span> sns<span style=color:#f92672>.</span>relplot(<span style=color:#e6db74>&#39;epoch&#39;</span>, <span style=color:#e6db74>&#39;train_loss&#39;</span>, col<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;bidirectional&#39;</span>, hue<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;dropout&#39;</span>, kind<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;line&#39;</span>, palette<span style=color:#f92672>=</span>sns<span style=color:#f92672>.</span>color_palette(<span style=color:#e6db74>&#34;Set1&#34;</span>, <span style=color:#ae81ff>3</span>), data<span style=color:#f92672>=</span>runs<span style=color:#f92672>.</span>query(query), ax<span style=color:#f92672>=</span>ax[<span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>close(g1<span style=color:#f92672>.</span>fig)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>close(g2<span style=color:#f92672>.</span>fig)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend(title<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;dropout&#39;</span>, labels<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;0.0&#39;</span>, <span style=color:#e6db74>&#39;0.25&#39;</span>, <span style=color:#e6db74>&#39;0.5&#39;</span>])
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>setp(ax, ylim<span style=color:#f92672>=</span>(<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>0.8</span>));
</span></span></code></pre></div><pre><code>Plotting 3 runs: ['faithful-sweep-3' 'quiet-deluge-19' 'sandy-sweep-9']
</code></pre><p><img alt=png loading=lazy src=/images/2019-12-15-btc-fraud-detection_files/2019-12-15-btc-fraud-detection_43_1.png></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span>query <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;model==&#34;gcn&#34; and bidirectional==True and onlylocal==False and nobias==&#34;False&#34; and weight_decay==&#34;0.0005&#34; and (dropout==0. or dropout==0.25 or dropout==&#34;0.5&#34;)&#39;</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Plotting </span><span style=color:#e6db74>{</span>runs<span style=color:#f92672>.</span>query(query)[<span style=color:#e6db74>&#39;name&#39;</span>]<span style=color:#f92672>.</span>nunique()<span style=color:#e6db74>}</span><span style=color:#e6db74> runs: </span><span style=color:#e6db74>{</span>runs<span style=color:#f92672>.</span>query(query)[<span style=color:#e6db74>&#39;name&#39;</span>]<span style=color:#f92672>.</span>unique()<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>fig, ax <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>subplots(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>3</span>, figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>18</span>,<span style=color:#ae81ff>5</span>))
</span></span><span style=display:flex><span>g1 <span style=color:#f92672>=</span> sns<span style=color:#f92672>.</span>relplot(<span style=color:#e6db74>&#39;epoch&#39;</span>, <span style=color:#e6db74>&#39;val_f1_score&#39;</span>, col<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;bidirectional&#39;</span>, hue<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;dropout&#39;</span>, kind<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;line&#39;</span>, palette<span style=color:#f92672>=</span>sns<span style=color:#f92672>.</span>color_palette(<span style=color:#e6db74>&#34;Set1&#34;</span>, <span style=color:#ae81ff>3</span>), data<span style=color:#f92672>=</span>runs<span style=color:#f92672>.</span>query(query), ax<span style=color:#f92672>=</span>ax[<span style=color:#ae81ff>0</span>])
</span></span><span style=display:flex><span>g2 <span style=color:#f92672>=</span> sns<span style=color:#f92672>.</span>relplot(<span style=color:#e6db74>&#39;epoch&#39;</span>, <span style=color:#e6db74>&#39;val_precision&#39;</span>, col<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;bidirectional&#39;</span>, hue<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;dropout&#39;</span>, kind<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;line&#39;</span>, palette<span style=color:#f92672>=</span>sns<span style=color:#f92672>.</span>color_palette(<span style=color:#e6db74>&#34;Set1&#34;</span>, <span style=color:#ae81ff>3</span>), data<span style=color:#f92672>=</span>runs<span style=color:#f92672>.</span>query(query), ax<span style=color:#f92672>=</span>ax[<span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>g3 <span style=color:#f92672>=</span> sns<span style=color:#f92672>.</span>relplot(<span style=color:#e6db74>&#39;epoch&#39;</span>, <span style=color:#e6db74>&#39;val_recall&#39;</span>, col<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;bidirectional&#39;</span>, hue<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;dropout&#39;</span>, kind<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;line&#39;</span>, palette<span style=color:#f92672>=</span>sns<span style=color:#f92672>.</span>color_palette(<span style=color:#e6db74>&#34;Set1&#34;</span>, <span style=color:#ae81ff>3</span>), data<span style=color:#f92672>=</span>runs<span style=color:#f92672>.</span>query(query), ax<span style=color:#f92672>=</span>ax[<span style=color:#ae81ff>2</span>])
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>close(g1<span style=color:#f92672>.</span>fig)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>close(g2<span style=color:#f92672>.</span>fig)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>close(g3<span style=color:#f92672>.</span>fig)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend(title<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;dropout&#39;</span>, labels<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;0.0&#39;</span>, <span style=color:#e6db74>&#39;0.25&#39;</span>, <span style=color:#e6db74>&#39;0.5&#39;</span>]);
</span></span></code></pre></div><pre><code>Plotting 3 runs: ['faithful-sweep-3' 'quiet-deluge-19' 'sandy-sweep-9']
</code></pre><p><img alt=png loading=lazy src=/images/2019-12-15-btc-fraud-detection_files/2019-12-15-btc-fraud-detection_44_1.png></p><h3 id=training-a-gcn-with-local-features-only>Training a GCN with local features only<a hidden class=anchor aria-hidden=true href=#training-a-gcn-with-local-features-only>#</a></h3><p>A question that arises from the paper is: how much does the graph-based information contribute to the performance of a GCN model compared to a more traditional non-graph-based approach?
Weber et al. show that the node embeddings that can be extracted from a GCN can help boost other traditional models. This makes sense intuitively: because of the networked nature of the bitcoin transactions, knowing the context or &ldquo;neighborhood&rdquo; of a transaction should add important information.</p><p>However, from the description of the Elliptic dataset we know that some of the features already contain information regarding the context of the transactions. In fact, 72 out of the 166 features are aggregated features. Therefore, I was curious to find out how would a GCN model perform with only the remaining 94 local features (including timestep) as inputs.</p><p>I modified the model to limit the set of features to only the local ones (including timestep). The input node features thus now have a shape of <code>(94,)</code>. This way we can assess the performance of a GCN without having to manually engineer features from their neighbors. In other words, we leave the feature engineering to the neural network itself.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># consider only the first 94 features of the node feature matrix</span>
</span></span><span style=display:flex><span>features_local <span style=color:#f92672>=</span> g_bi<span style=color:#f92672>.</span>ndata[<span style=color:#e6db74>&#34;features_matrix&#34;</span>][:,<span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>94</span>]<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;&#34;&#34;Number of features (all): </span><span style=color:#e6db74>{</span>features<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;&#34;&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;&#34;&#34;Number of features (only local): </span><span style=color:#e6db74>{</span>features_local<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;&#34;&#34;</span>)
</span></span></code></pre></div><pre><code>Number of features (all): 166
Number of features (only local): 94
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># GCN model parameters</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#params = {</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#    &#34;bidirectional&#34; : True,</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#    &#34;n_hidden&#34; : 100,</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#    &#34;n_layers&#34; : 2,</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#    &#34;weight_decay&#34; : 5e-4,</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#    &#34;bias&#34; : True,</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#    &#34;dropout&#34; : 0.25,</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#    &#34;epochs&#34; : 1000,</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#    &#34;lr&#34; : 1e-3,</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#    &#34;posweight&#34; : 0.7,</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#}</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#model, metrics = train_eval_model(GCN, g_bi, features_local, **params)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span>query <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;model==&#34;gcn&#34; and bidirectional==True and weight_decay==0.0005 and nobias==&#34;False&#34; and (dropout==0 or dropout==0.25 or dropout==0.5)&#39;</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Plotting </span><span style=color:#e6db74>{</span>runs<span style=color:#f92672>.</span>query(query)[<span style=color:#e6db74>&#39;name&#39;</span>]<span style=color:#f92672>.</span>nunique()<span style=color:#e6db74>}</span><span style=color:#e6db74> runs: </span><span style=color:#e6db74>{</span>runs<span style=color:#f92672>.</span>query(query)[<span style=color:#e6db74>&#39;name&#39;</span>]<span style=color:#f92672>.</span>unique()<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>sns<span style=color:#f92672>.</span>relplot(<span style=color:#e6db74>&#39;epoch&#39;</span>, <span style=color:#e6db74>&#39;val_f1_score&#39;</span>, col<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;onlylocal&#39;</span>, hue<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;dropout&#39;</span>, palette<span style=color:#f92672>=</span>sns<span style=color:#f92672>.</span>color_palette(<span style=color:#e6db74>&#34;Set1&#34;</span>, <span style=color:#ae81ff>3</span>), kind<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;line&#39;</span>, data<span style=color:#f92672>=</span>runs<span style=color:#f92672>.</span>query(query));
</span></span></code></pre></div><pre><code>Plotting 6 runs: ['divine-sweep-4' 'faithful-sweep-3' 'zesty-microwave-76'
 'quiet-deluge-19' 'divine-sweep-11' 'sandy-sweep-9']
</code></pre><p><img alt=png loading=lazy src=/images/2019-12-15-btc-fraud-detection_files/2019-12-15-btc-fraud-detection_48_1.png></p><p>We can see that the perfomance of the GCN improved by addition of dropout when all features were considered, but not when using only local features. This makes sense, as the aggregated features are likely less essential than the local ones.</p><h2 id=random-forest-benchmark>Random Forest benchmark<a hidden class=anchor aria-hidden=true href=#random-forest-benchmark>#</a></h2><p>A sobering additional finding from the paper by Weber et al. was the out-of-the-box excellent performance of a simple Random Forest in correctly classifying the transactions as licit or illicit. I was able to replicate these results too for two different sets of features:</p><ul><li>All features</li><li>Only local features</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.ensemble <span style=color:#f92672>import</span> RandomForestClassifier
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># function to evaluate the model</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>evaluate_rfc</span>(model, features, labels, mask):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Calculate the loss, accuracy, precision, recall and f1_score for the masked data&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    pred_rfc <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>predict(features[mask])
</span></span><span style=display:flex><span>    labels <span style=color:#f92672>=</span> labels[mask]
</span></span><span style=display:flex><span>    p, r, f, _ <span style=color:#f92672>=</span> precision_recall_fscore_support(labels, pred_rfc)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> p[<span style=color:#ae81ff>1</span>], r[<span style=color:#ae81ff>1</span>], f[<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span><span style=color:#75715e># confusion matrix</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>eval_confusion_matrix_rfc</span>(model, features, labels, mask):
</span></span><span style=display:flex><span>    pred_rfc <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>predict(features[mask])
</span></span><span style=display:flex><span>    labels <span style=color:#f92672>=</span> labels[mask]
</span></span><span style=display:flex><span>    print(confusion_matrix(labels, pred_rfc))
</span></span></code></pre></div><h3 id=using-all-features-local--aggregated>Using all features (local + aggregated)<a hidden class=anchor aria-hidden=true href=#using-all-features-local--aggregated>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span>rfc <span style=color:#f92672>=</span> RandomForestClassifier(n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>50</span>, max_features<span style=color:#f92672>=</span><span style=color:#ae81ff>50</span>, random_state<span style=color:#f92672>=</span>seed)
</span></span><span style=display:flex><span>rfc<span style=color:#f92672>.</span>fit(features[train_indices], labels[train_indices])
</span></span><span style=display:flex><span>p, r, f1 <span style=color:#f92672>=</span> evaluate_rfc(rfc, features, labels, val_indices)
</span></span><span style=display:flex><span>print(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Precision </span><span style=color:#e6db74>{</span>p<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74> | Recall </span><span style=color:#e6db74>{</span>r<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74> | &#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;F1 score </span><span style=color:#e6db74>{</span>f1<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Confusion matrix:&#34;</span>)
</span></span><span style=display:flex><span>eval_confusion_matrix_rfc(rfc, features, labels, val_indices)
</span></span></code></pre></div><pre><code>Precision 0.9167 | Recall 0.7211 | F1 score 0.8072
Confusion matrix:
[[15516    71]
 [  302   781]]
</code></pre><p>The best results were obtained using as input all available features (local (including timestep) + aggregated). Both precision and recall of this model were high. This is confirmed by looking at the confusion matrix. The model predicts nearly no false positives and less than 30% of the illicit transactions are falsely labeled as negatives. It is a very good performance for such a simple model.</p><h3 id=using-local-features-only>Using local features only<a hidden class=anchor aria-hidden=true href=#using-local-features-only>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span>rfc_local <span style=color:#f92672>=</span> RandomForestClassifier(n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>50</span>, max_features<span style=color:#f92672>=</span><span style=color:#ae81ff>50</span>, random_state<span style=color:#f92672>=</span>seed)
</span></span><span style=display:flex><span>rfc_local<span style=color:#f92672>.</span>fit(features_local[train_indices], labels[train_indices])
</span></span><span style=display:flex><span>p, r, f1 <span style=color:#f92672>=</span> evaluate_rfc(rfc_local, features_local, labels, val_indices)
</span></span><span style=display:flex><span>print(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Precision </span><span style=color:#e6db74>{</span>p<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74> | Recall </span><span style=color:#e6db74>{</span>r<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74> | &#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;F1 score </span><span style=color:#e6db74>{</span>f1<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>eval_confusion_matrix_rfc(rfc_local, features_local, labels, val_indices)
</span></span></code></pre></div><pre><code>Precision 0.8749 | Recall 0.7036 | F1 score 0.7799
[[15478   109]
 [  321   762]]
</code></pre><p>Removing the aggregated features from the input to the Random Forest (leaving other parameters equal) impairs its performance both in precision and recall, but not dramatically. It still is a very good model working out of the box. It makes sense that not having information about the immediate neighbors of a transaction would produce a worse-performing model.</p><h3 id=comparison-to-gcn>Comparison to GCN<a hidden class=anchor aria-hidden=true href=#comparison-to-gcn>#</a></h3><p>So what do the resuls of the Random Forest tell us about GCNs and other deep learning techniques? Should we dismiss them and focus on simpler models instead? While this analysis shows that it pays to start simple and see how well we can tackle a task using classic machine learning methods first, there still are valid reasons why one should consider (graph) neural networks too.</p><ol start=0><li>Deep learning on graphs is cool!</li></ol><p>Now, seriously,</p><ol><li>There is information contained in the connections between data points, which is not being considered by a classical machine learning approach, unless carefully crafted features are available, which requires time and specific knowledge</li><li>The features available to a different dataset may be less informative than those in the Elliptic dataset, and therefore insufficient to produce a good-enough Random Forest model</li><li>There may even be situations when no intrinsic node features are available and we still want to be able to classify transactions. This would still be possible using a GCN but not with a Random Forest</li><li>Progress in unleashing the potential of GCNs can only be obtained by researching these networks</li></ol><p>Now let&rsquo;s take a look at a different kind of graph-based model that, I figured, might be a good option for the bitcoin transaction classification task.</p><h2 id=long-range-propagation-of-label-predictions-using-appnp>Long-range propagation of label predictions using APPNP<a hidden class=anchor aria-hidden=true href=#long-range-propagation-of-label-predictions-using-appnp>#</a></h2><p>Let&rsquo;s recapitulate.</p><ul><li>We trained a complex GCN model to classify bitcoin transactions as licit or illicit and improved its performance by better parameters and training</li><li>We found that a Random Forest model performed better than our complex GCN, almost effortlessly</li></ul><p>Why is this? In order to look for the answer it pays to take a closer look at how the transaction graph is structured. Let&rsquo;s take for example the transactions of the last timestep.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>with</span> sns<span style=color:#f92672>.</span>axes_style(<span style=color:#e6db74>&#39;white&#39;</span>):
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>8</span>,<span style=color:#ae81ff>6</span>))
</span></span><span style=display:flex><span>    node_label <span style=color:#f92672>=</span> list(nx<span style=color:#f92672>.</span>get_node_attributes(g_nx_t_list[<span style=color:#ae81ff>49</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>], <span style=color:#e6db74>&#39;label&#39;</span>)<span style=color:#f92672>.</span>values())
</span></span><span style=display:flex><span>    mapping <span style=color:#f92672>=</span> {<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>:<span style=color:#e6db74>&#39;grey&#39;</span>, <span style=color:#ae81ff>0</span>:<span style=color:#e6db74>&#39;C0&#39;</span>, <span style=color:#ae81ff>1</span>:<span style=color:#e6db74>&#39;C3&#39;</span>}
</span></span><span style=display:flex><span>    node_color <span style=color:#f92672>=</span> [mapping[l] <span style=color:#66d9ef>for</span> l <span style=color:#f92672>in</span> node_label]
</span></span><span style=display:flex><span>    nx<span style=color:#f92672>.</span>draw_networkx(g_nx_t_list[<span style=color:#ae81ff>49</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>], node_size<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>, node_color<span style=color:#f92672>=</span>node_color, with_labels<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, width<span style=color:#f92672>=</span><span style=color:#ae81ff>0.2</span>, alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.8</span>, arrowsize<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span>)
</span></span><span style=display:flex><span>    leg <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>legend([<span style=color:#e6db74>&#39;unlabeled&#39;</span>, <span style=color:#e6db74>&#39;licit&#39;</span>, <span style=color:#e6db74>&#39;illicit&#39;</span>])
</span></span><span style=display:flex><span>    leg<span style=color:#f92672>.</span>legendHandles[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>set_color(<span style=color:#e6db74>&#39;grey&#39;</span>)
</span></span><span style=display:flex><span>    leg<span style=color:#f92672>.</span>legendHandles[<span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>set_color(<span style=color:#e6db74>&#39;C0&#39;</span>)
</span></span><span style=display:flex><span>    leg<span style=color:#f92672>.</span>legendHandles[<span style=color:#ae81ff>2</span>]<span style=color:#f92672>.</span>set_color(<span style=color:#e6db74>&#39;C3&#39;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><pre><code>/anaconda3/envs/dgl/lib/python3.7/site-packages/networkx/drawing/nx_pylab.py:579: MatplotlibDeprecationWarning: 
The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead.
  if not cb.iterable(width):
/anaconda3/envs/dgl/lib/python3.7/site-packages/networkx/drawing/nx_pylab.py:676: MatplotlibDeprecationWarning: 
The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead.
  if cb.iterable(node_size):  # many node sizes
</code></pre><p><img alt=png loading=lazy src=/images/2019-12-15-btc-fraud-detection_files/2019-12-15-btc-fraud-detection_59_1.png></p><p>The original paper describing the GCN applied it to the node classification of papers in the Cora dataset. The bitcoin transaction graph is much larger and less dense than the Cora graph. One can hipothesize that for a denser graph with a higher average degree, each node could receive more information from its neighbors that would help make a GCN make a better prediction.</p><table><thead><tr><th>Graph</th><th style=text-align:right>Nodes</th><th style=text-align:right>Density</th><th style=text-align:right>Average degree</th></tr></thead><tbody><tr><td>Elliptic (bitcoin transactions)</td><td style=text-align:right>203769</td><td style=text-align:right>0.0003177</td><td style=text-align:right>2.30</td></tr><tr><td>Cora (citations)</td><td style=text-align:right>2708</td><td style=text-align:right>0.0014812</td><td style=text-align:right>3.90</td></tr></tbody></table><p>Furthermore, if we consider a simple chain of transactions (like the ones seen in the graph visualization), a node would receive information only from the previous node and pass it on only to the following node in the chain. In such a situation, it could be that the range of neighbors that feed the classification of any given node is too short and often not sufficient for a correct prediction. If this is the case, then considering a longer-ranging neighborhood could help train a better classification model.</p><h2 id=approximated-personalized-propagation-of-neural-predictions-appnp>Approximated Personalized Propagation of Neural Predictions (APPNP)<a hidden class=anchor aria-hidden=true href=#approximated-personalized-propagation-of-neural-predictions-appnp>#</a></h2><p>Enter APPNP. <a href=https://arxiv.org/abs/1810.05997>This model was recently proposed</a> as a way to reconcile the best of two worlds: neural message passing algorithms (in principle like the GCN), and personalized pagerank.</p><p>In <strong>PageRank</strong>, a measure of how central or important a node is calculated as a function of its connections and the importance of its neighbors. The pagerank $PR$ of a node $u$ is:</p>$$PR(u) = (1-d){1 \over {N}} + d \sum_{v \in \mathcal{N} (u)} {PR(v) \over D_{out}(v)}$$<p>where $N$ is the total number of nodes, $D_{out}$ is the outdegree, and $v$ is a node of the set of neighbors $\mathcal{N}$ of $u$. Pagerank ultimately requires each node to iteratively receive information from its neighbors until the pagerank stops changing and converges. The algorithm assigns a damping factor $d$, which can be understood by imagining a random surfer visiting nodes in the graph. The surfer would visit any given node moving along the edges of the graph until it reaches it with probability $d$. However, that surfer could also visit a node by randomly teleporting to it from elsewhere in the graph with probability $(1-d)$.</p><p>In <strong>APPNP</strong>, a multilayer perceptron takes the node features as input and outputs prediction probabilities $H^{0} = f_{MLP}(X)$. These are then propagated through graph for a $K$ number of iterations.</p>$$H^{t+1} = (1-\alpha)\left(\hat{D}^{-1/2} \hat{A} \hat{D}^{-1/2} H^{t}\right) + \alpha H^{0}$$<p>If you think the APPNP and the pagerank equations look similar is because they are. The teleportation probability $\alpha$ corresponds to the damping factor in pagerank. It tells us that for each iteration, the predictions for a node will depend on the normalized graph laplacian with probability $(1-\alpha)$ or on the output of the MLP with probability $\alpha$.</p><p>So how does do we train the APPNP model? I modified an implementation of an APPNP layer from DGL. In the <a href=https://arxiv.org/abs/1810.05997>original paper by Klicpera et al. (2019)</a> they use an architecture consisting of a 2-layer MLP with 64 hidden units each, followed by the propagation component given by the APPNP equation. In order to use a comparable architecture to the GCN model, I decided to set the number of hidden units to 100 (same as previous section).</p><p>I tested several combinations of values for $K$ and $\alpha$. However, a more exhaustive hyperparameter search is needed to find the best possible configuration. Moreover, I used two versions of the feature set: all features vs. only local features.</p><p>The best F1 score was obtained for $K = 20$ propagation iterations and a teleportation probability $\alpha = 0.2$. Interestingly, the model trained with only local features performed better than using all features. Furthermore, the addition of dropout to the network had a positive effect when using all features, but a negative effect if only local features were considered.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>APPNP</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(
</span></span><span style=display:flex><span>        self,
</span></span><span style=display:flex><span>        g,
</span></span><span style=display:flex><span>        in_feats,
</span></span><span style=display:flex><span>        n_hidden,
</span></span><span style=display:flex><span>        n_classes,
</span></span><span style=display:flex><span>        n_layers,
</span></span><span style=display:flex><span>        activation,
</span></span><span style=display:flex><span>        feat_drop,
</span></span><span style=display:flex><span>        edge_drop,
</span></span><span style=display:flex><span>        alpha,
</span></span><span style=display:flex><span>        k,
</span></span><span style=display:flex><span>    ):
</span></span><span style=display:flex><span>        super(APPNP, self)<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>g <span style=color:#f92672>=</span> g
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>layers <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ModuleList()
</span></span><span style=display:flex><span>        <span style=color:#75715e># input layer</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>append(nn<span style=color:#f92672>.</span>Linear(in_feats, n_hidden))
</span></span><span style=display:flex><span>        <span style=color:#75715e># hidden layers</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(n_layers <span style=color:#f92672>-</span> <span style=color:#ae81ff>2</span>):
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>append(nn<span style=color:#f92672>.</span>Linear(n_hidden, n_hidden))
</span></span><span style=display:flex><span>        <span style=color:#75715e># output layer</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>append(nn<span style=color:#f92672>.</span>Linear(n_hidden, n_classes))
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>activation <span style=color:#f92672>=</span> activation
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> feat_drop:
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>feat_drop <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dropout(feat_drop)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>feat_drop <span style=color:#f92672>=</span> <span style=color:#66d9ef>lambda</span> x: x
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>propagate <span style=color:#f92672>=</span> APPNPConv(k, alpha, edge_drop)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>reset_parameters()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>reset_parameters</span>(self):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> layer <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>layers:
</span></span><span style=display:flex><span>            layer<span style=color:#f92672>.</span>reset_parameters()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, features):
</span></span><span style=display:flex><span>        <span style=color:#75715e># prediction step</span>
</span></span><span style=display:flex><span>        h <span style=color:#f92672>=</span> features
</span></span><span style=display:flex><span>        h <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>feat_drop(h)
</span></span><span style=display:flex><span>        h <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>activation(self<span style=color:#f92672>.</span>layers[<span style=color:#ae81ff>0</span>](h))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> layer <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>layers[<span style=color:#ae81ff>1</span>:<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]:
</span></span><span style=display:flex><span>            h <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>activation(layer(h))
</span></span><span style=display:flex><span>        h <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>layers[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>](self<span style=color:#f92672>.</span>feat_drop(h))
</span></span><span style=display:flex><span>        <span style=color:#75715e># propagation step</span>
</span></span><span style=display:flex><span>        h <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>propagate(self<span style=color:#f92672>.</span>g, h)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> h
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span>query <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;model==&#34;appnp&#34; and nhidden==100 and bidirectional==True and weight_decay==0.0005 and nobias==&#34;False&#34; and (dropout==0 or dropout==0.2 or dropout==0.25)&#39;</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Plotting </span><span style=color:#e6db74>{</span>runs<span style=color:#f92672>.</span>query(query)[<span style=color:#e6db74>&#39;name&#39;</span>]<span style=color:#f92672>.</span>nunique()<span style=color:#e6db74>}</span><span style=color:#e6db74> runs: </span><span style=color:#e6db74>{</span>runs<span style=color:#f92672>.</span>query(query)[<span style=color:#e6db74>&#39;name&#39;</span>]<span style=color:#f92672>.</span>unique()<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>sns<span style=color:#f92672>.</span>relplot(<span style=color:#e6db74>&#39;epoch&#39;</span>, <span style=color:#e6db74>&#39;val_f1_score&#39;</span>, row<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;onlylocal&#39;</span>, col<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;alpha&#39;</span>, hue<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;k&#39;</span>, style<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;dropout&#39;</span>, palette<span style=color:#f92672>=</span>sns<span style=color:#f92672>.</span>color_palette(<span style=color:#e6db74>&#34;Set1&#34;</span>, <span style=color:#ae81ff>2</span>), kind<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;line&#39;</span>, data<span style=color:#f92672>=</span>runs<span style=color:#f92672>.</span>query(query));
</span></span></code></pre></div><pre><code>Plotting 12 runs: ['tough-sweep-4' 'fluent-sweep-3' 'happy-sweep-2' 'eager-sweep-1'
 'vivid-sweep-2' 'copper-sweep-1' 'classic-sweep-10' 'confused-sweep-9'
 'generous-sweep-6' 'dulcet-sweep-5' 'avid-sweep-2' 'giddy-sweep-1']
</code></pre><p><img alt=png loading=lazy src=/images/2019-12-15-btc-fraud-detection_files/2019-12-15-btc-fraud-detection_64_1.png></p><h3 id=mlp-benchmark>MLP benchmark<a hidden class=anchor aria-hidden=true href=#mlp-benchmark>#</a></h3><p>But just how much of an effect does the incorporation of the graph structure have on the performance of the model? How much of it is simply due to the MLP that is trained on top of the propagation phase of the APPNP? In order to assess this, I trained an MLP model with the same architecture and for the two different versions of the features set.</p><p>The results show that the APPNP model has a better F1 score than the MLP after 1000 epochs, both for the only local feature set and the full feature set. With respect to precision and F1 score, the addition of dropout was beneficial when using all features and prejudicial when using the local features only. In contrast, recall was slightly improved by adding dropout to both feature variants.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MLP</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(
</span></span><span style=display:flex><span>        self, in_feats, n_hidden, n_classes, n_layers, activation, feat_drop,
</span></span><span style=display:flex><span>    ):
</span></span><span style=display:flex><span>        super(MLP, self)<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>layers <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ModuleList()
</span></span><span style=display:flex><span>        <span style=color:#75715e># input layer</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>append(nn<span style=color:#f92672>.</span>Linear(in_feats, n_hidden))
</span></span><span style=display:flex><span>        <span style=color:#75715e># hidden layers</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(n_layers <span style=color:#f92672>-</span> <span style=color:#ae81ff>2</span>):
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>append(nn<span style=color:#f92672>.</span>Linear(n_hidden, n_hidden))
</span></span><span style=display:flex><span>        <span style=color:#75715e># output layer</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>append(nn<span style=color:#f92672>.</span>Linear(n_hidden, n_classes))
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>activation <span style=color:#f92672>=</span> activation
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> feat_drop:
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>feat_drop <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dropout(feat_drop)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>feat_drop <span style=color:#f92672>=</span> <span style=color:#66d9ef>lambda</span> x: x
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>reset_parameters()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>reset_parameters</span>(self):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> layer <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>layers:
</span></span><span style=display:flex><span>            layer<span style=color:#f92672>.</span>reset_parameters()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, features):
</span></span><span style=display:flex><span>        <span style=color:#75715e># prediction step</span>
</span></span><span style=display:flex><span>        h <span style=color:#f92672>=</span> features
</span></span><span style=display:flex><span>        h <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>feat_drop(h)
</span></span><span style=display:flex><span>        h <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>activation(self<span style=color:#f92672>.</span>layers[<span style=color:#ae81ff>0</span>](h))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> layer <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>layers[<span style=color:#ae81ff>1</span>:<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]:
</span></span><span style=display:flex><span>            h <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>activation(layer(h))
</span></span><span style=display:flex><span>        h <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>layers[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>](self<span style=color:#f92672>.</span>feat_drop(h))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> h
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#hide_input</span>
</span></span><span style=display:flex><span>query <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;((model==&#34;appnp&#34; and k==&#34;20.0&#34; and alpha==&#34;0.2&#34;) or model==&#34;mlp&#34;) and bidirectional==True and weight_decay==0.0005 and nobias==&#34;False&#34; and (dropout==0 or dropout==0.2)&#39;</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Plotting </span><span style=color:#e6db74>{</span>runs<span style=color:#f92672>.</span>query(query)[<span style=color:#e6db74>&#39;name&#39;</span>]<span style=color:#f92672>.</span>nunique()<span style=color:#e6db74>}</span><span style=color:#e6db74> runs: </span><span style=color:#e6db74>{</span>runs<span style=color:#f92672>.</span>query(query)[<span style=color:#e6db74>&#39;name&#39;</span>]<span style=color:#f92672>.</span>unique()<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>sns<span style=color:#f92672>.</span>relplot(<span style=color:#e6db74>&#39;epoch&#39;</span>, <span style=color:#e6db74>&#39;val_f1_score&#39;</span>, col<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;onlylocal&#39;</span>, hue<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;model&#39;</span>, style<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;dropout&#39;</span>, palette<span style=color:#f92672>=</span>sns<span style=color:#f92672>.</span>color_palette(<span style=color:#e6db74>&#34;Set1&#34;</span>, <span style=color:#ae81ff>2</span>), kind<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;line&#39;</span>, data<span style=color:#f92672>=</span>runs<span style=color:#f92672>.</span>query(query));
</span></span></code></pre></div><pre><code>Plotting 8 runs: ['tough-sweep-4' 'fluent-sweep-3' 'happy-sweep-2' 'crisp-sweep-4'
 'lucky-sweep-3' 'summer-sweep-2' 'eager-sweep-1' 'sandy-sweep-1']
</code></pre><p><img alt=png loading=lazy src=/images/2019-12-15-btc-fraud-detection_files/2019-12-15-btc-fraud-detection_67_1.png></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#hide_input</span>
</span></span><span style=display:flex><span>query <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;((model==&#34;appnp&#34; and k==&#34;20.0&#34; and alpha==&#34;0.2&#34;) or model==&#34;mlp&#34;) and bidirectional==True and weight_decay==0.0005 and nobias==&#34;False&#34; and (dropout==0 or dropout==0.2)&#39;</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Plotting </span><span style=color:#e6db74>{</span>runs<span style=color:#f92672>.</span>query(query)[<span style=color:#e6db74>&#39;name&#39;</span>]<span style=color:#f92672>.</span>nunique()<span style=color:#e6db74>}</span><span style=color:#e6db74> runs: </span><span style=color:#e6db74>{</span>runs<span style=color:#f92672>.</span>query(query)[<span style=color:#e6db74>&#39;name&#39;</span>]<span style=color:#f92672>.</span>unique()<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>sns<span style=color:#f92672>.</span>relplot(<span style=color:#e6db74>&#39;epoch&#39;</span>, <span style=color:#e6db74>&#39;val_precision&#39;</span>, col<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;onlylocal&#39;</span>, hue<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;model&#39;</span>, style<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;dropout&#39;</span>, palette<span style=color:#f92672>=</span>sns<span style=color:#f92672>.</span>color_palette(<span style=color:#e6db74>&#34;Set1&#34;</span>, <span style=color:#ae81ff>2</span>), kind<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;line&#39;</span>, data<span style=color:#f92672>=</span>runs<span style=color:#f92672>.</span>query(query));
</span></span></code></pre></div><pre><code>Plotting 8 runs: ['tough-sweep-4' 'fluent-sweep-3' 'happy-sweep-2' 'crisp-sweep-4'
 'lucky-sweep-3' 'summer-sweep-2' 'eager-sweep-1' 'sandy-sweep-1']
</code></pre><p><img alt=png loading=lazy src=/images/2019-12-15-btc-fraud-detection_files/2019-12-15-btc-fraud-detection_68_1.png></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#hide_input</span>
</span></span><span style=display:flex><span>query <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;((model==&#34;appnp&#34; and k==&#34;20.0&#34; and alpha==&#34;0.2&#34;) or model==&#34;mlp&#34;) and bidirectional==True and weight_decay==0.0005 and nobias==&#34;False&#34; and (dropout==0 or dropout==0.2)&#39;</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Plotting </span><span style=color:#e6db74>{</span>runs<span style=color:#f92672>.</span>query(query)[<span style=color:#e6db74>&#39;name&#39;</span>]<span style=color:#f92672>.</span>nunique()<span style=color:#e6db74>}</span><span style=color:#e6db74> runs: </span><span style=color:#e6db74>{</span>runs<span style=color:#f92672>.</span>query(query)[<span style=color:#e6db74>&#39;name&#39;</span>]<span style=color:#f92672>.</span>unique()<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>sns<span style=color:#f92672>.</span>relplot(<span style=color:#e6db74>&#39;epoch&#39;</span>, <span style=color:#e6db74>&#39;val_recall&#39;</span>, col<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;onlylocal&#39;</span>, hue<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;model&#39;</span>, style<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;dropout&#39;</span>, palette<span style=color:#f92672>=</span>sns<span style=color:#f92672>.</span>color_palette(<span style=color:#e6db74>&#34;Set1&#34;</span>, <span style=color:#ae81ff>2</span>), kind<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;line&#39;</span>, data<span style=color:#f92672>=</span>runs<span style=color:#f92672>.</span>query(query));
</span></span></code></pre></div><pre><code>Plotting 8 runs: ['tough-sweep-4' 'fluent-sweep-3' 'happy-sweep-2' 'crisp-sweep-4'
 'lucky-sweep-3' 'summer-sweep-2' 'eager-sweep-1' 'sandy-sweep-1']
</code></pre><p><img alt=png loading=lazy src=/images/2019-12-15-btc-fraud-detection_files/2019-12-15-btc-fraud-detection_69_1.png></p><h3 id=putting-it-all-together>Putting it all together<a hidden class=anchor aria-hidden=true href=#putting-it-all-together>#</a></h3><p>Please refer to my <a href=#tl;dr>tl;dr</a> :)</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#hide_input</span>
</span></span><span style=display:flex><span>query <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;&#39;&#39;((onlylocal==True and dropout==0) or (onlylocal==False  and (dropout==0.2 or dropout==0.5))) 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>and ((model==&#34;appnp&#34; and k==&#34;20.0&#34; and alpha==&#34;0.2&#34; and (dropout==0. or dropout==0.2)) 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>or (model==&#34;mlp&#34; and (dropout==0. or dropout==0.2)) 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>or (model==&#34;gcn&#34;  and (dropout==0. or dropout==0.5))) 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>and bidirectional==True and weight_decay==0.0005 and nobias==&#34;False&#34;&#39;&#39;&#39;</span><span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#39;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#39;</span>,<span style=color:#e6db74>&#39; &#39;</span>)
</span></span><span style=display:flex><span>g1 <span style=color:#f92672>=</span> sns<span style=color:#f92672>.</span>relplot(<span style=color:#e6db74>&#39;epoch&#39;</span>, <span style=color:#e6db74>&#39;val_f1_score&#39;</span>, col<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;onlylocal&#39;</span>, hue<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;model&#39;</span>, style<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;dropout&#39;</span>, palette<span style=color:#f92672>=</span>sns<span style=color:#f92672>.</span>color_palette(<span style=color:#e6db74>&#34;Set1&#34;</span>, <span style=color:#ae81ff>3</span>), kind<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;line&#39;</span>, data<span style=color:#f92672>=</span>runs<span style=color:#f92672>.</span>query(query));
</span></span><span style=display:flex><span>g1<span style=color:#f92672>.</span>axes<span style=color:#f92672>.</span>flat[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>axhline(<span style=color:#ae81ff>0.8072</span>, c<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;k&#39;</span>, alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.8</span>, ls<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;-.&#39;</span>, lw<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>g1<span style=color:#f92672>.</span>axes<span style=color:#f92672>.</span>flat[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>text(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>0.815</span>,<span style=color:#e6db74>&#39;RandomForest&#39;</span>)
</span></span><span style=display:flex><span>g1<span style=color:#f92672>.</span>axes<span style=color:#f92672>.</span>flat[<span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>axhline(<span style=color:#ae81ff>0.7799</span>, c<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;k&#39;</span>, alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.8</span>, ls<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;-.&#39;</span>, lw<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>g1<span style=color:#f92672>.</span>axes<span style=color:#f92672>.</span>flat[<span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>text(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>0.785</span>,<span style=color:#e6db74>&#39;RandomForest&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>suptitle(<span style=color:#e6db74>&#39;Performance of the best models of each class using all features vs. only local features&#39;</span>, y<span style=color:#f92672>=</span><span style=color:#ae81ff>1.02</span>);
</span></span></code></pre></div><p><img alt=png loading=lazy src=/images/2019-12-15-btc-fraud-detection_files/2019-12-15-btc-fraud-detection_71_0.png></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://www.arcosdiaz.com/blog/tags/graph-neural-networks/>Graph Neural Networks</a></li><li><a href=https://www.arcosdiaz.com/blog/tags/fraud-detection/>Fraud Detection</a></li></ul><nav class=paginav><a class=prev href=https://www.arcosdiaz.com/blog/posts/2021-01-01-covid19-germany-dashboard/><span class=title>« Prev</span><br><span>COVID-19 Germany local incidence and ICU occupancy (in German)</span>
</a><a class=next href=https://www.arcosdiaz.com/blog/posts/2018-04-01-fitbit_prophet/><span class=title>Next »</span><br><span>Fitbit activity and sleep data: a time-series analysis with Generalized Additive Models</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://www.arcosdiaz.com/blog/>Dario Arcos-Díaz, PhD</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>